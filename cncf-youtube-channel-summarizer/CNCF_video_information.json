{"BoNrIx93vZE": {"video_title": "Create a First AI Program with TypeScript and Serverless WebAssembly", "video_description": "Talk by Caleb Schoepp, Justin Pflueger\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/create-first-ai-program-typescript-and-serverless-webassembly\n\nThis workshop starts with the open source Spin tool for building WebAssembly-based serverless apps. We'll build a simple Hello World app in TypeScript (advanced participants can opt for Rust or Python instead). Then we'll turn our simple Hello World into a first foray into AI inferencing using a large language model (LLM). This is an excellent opportunity to try out AI with only a dozen or so lines of code.", "transcript": "just I guess before we jump in we should maybe say who we are my name is Caleb Shep I'm a software engineer at firon I've been there for about a year work on the firon cloud and some of the Upstream open source stuff like spin which we'll be playing around with today and and spin Cube too uh I'm Justin fluger I also work at Fon I'm a Solutions engineer um so I Engineer Solutions um and yeah work with firon customers um to kind of build out applications and kind of take advantage of of what web assembly on the server awesome so I want to start by talking about what you guys are going to be walking away with today and really kind of the meat of it is some hands-on experience with web assembly um so we're going to be you know getting our hands dirty there and actually playing around with web assembly on the server side and specifically we'll be using the open source tool spin which you'll learn more about in a second um and we'll be learning things about like how to route uh requests into your spin application kind of all the features you can use in a spin app like KV stores or generative AI models those large language models um and then we're also going to be like running these locally and and deploying to deploying to fir on cloud and kind of experiencing that full gamut there uh just kind of a quick agenda for today we're going to spend the first kind of like 15 to 20 minutes just talking about a few things here we'll do kind of a one1 level introduction to web assembly and kind of a one1 level introduction to spin give everybody here some context on that then we'll do a brief overview of what the workshop's going to look like and then the majority of the time it's just going to be kind of a live coded um walk through of the workshop and we'll go through a step in quick show of hands who's heard of or used web assembly before got a couple anybody firon fans spin fans out there all right well we'll make some new ones he we got back cool new new crowd to evangelize to awesome so um we'll jump right in here now I I I just want to give an opportunity here at the start we've got three ways we can kind of work through this Workshop the first way is you can install the dependencies locally and and do it that way second way is you can go to our Workshop our github.com fir on workshops download that and use a Dev container or kind of the way we recommend that people work through this is using a vs code code space so if you go to this URL using the QR code there you can click on code clone and then go to the codes spaces Tab and and generate a codes space it's a full editor online it will have all the dependencies for you know kind of work on the bo out of the box um so I'll give maybe a couple more seconds in case anybody wants to scan that and get that going but we'll walk through that later and if anybody gets stuck or has questions uh just raise your hand I'll come around and help you um there was a little mixup with the with the venue today there were supposed to be tables and stuff for you to get out your laptops and set them set them out but um yeah we just have chairs today so we'll we'll make two we'll make two okay I don't see any more scanning of the QR code there so we already did this poll awesome okay so starting with kind of like a 101 on web assembly what what are the three most important things we should like know about web assembly well first of all web assembly is a specification and more specifically web assembly is a specification of a binary instruction format so really what you you know you should think of web assembly as is it's a compile Target you can take your source code in rust or typescript or go or whatever language that supports web assembly and compile that into this web assembly instruction format and then execute that later and also we should know that WM is an abbreviation of web assembly you're going to hear me say WM and web assembly interchangeably you're going to see it interchangeably on the slides and everywhere so that's a good thing to keep in mind here's kind of just like a a visual of what this means right you've got your code you for example maybe use your rust compiler to compile it to a web assembly module and then that module can be executed by web assembly runtime which could be you know running in a container it could be running locally it could be running in the cloud on an iot device now whenever I talk publicly about web assembly I always like to kind of come back to four reasons why you should care about web assembly because it's like sure it's this cool new tech and there's a bunch of hype about it but like why does it matter and I think there's four reasons first reason is security and so web assembly offers us a sandboxed execution environment and in addition to that it also offers offers us something called capabilities based security so what do I mean by uh sandbox execution environment what I mean by that is that when a runtime is executing web assembly it's called a guest module it's it's sandboxed it's secure and there's guarantees in the web assembly runtime that the codee's never going to be able to break out of that sandbox it's not going to be able to do a buffer overflow or any sort of attack like that and attack the underlying device that's executing that web assembly we're also going to hear a bit more later about capabilities based security which is basically saying you have to explicitly require access to things on the system and then you can audit those those capabilities you're requesting the the second reason I think we should care about web assembly is performance um and you know really performance is just kind of table Stakes I think we're not going to see adoption of web assembly without performance third web assembly is polyglot and what does that mean poly many glot languages means that web assembly supports a wide array of languages so you can you know take a Python program or a rust program or any number of languages and I'll compile them to web assembly and have them cooperate together nicely and then four web assembly offers like a great portability story uh similar to you know Java where you compile it to that b code and then anywhere you have a runtime for that Java B code you can run it that sort of right once Run Anywhere story similar thing in um web assembly you have a runtime for all the different systems and architectures you want to run on and you get great portability out of that web assembly code when when we talk about web assembly you're also going to hear something called wazzy come up a lot and wazzy stands for web assembly um system interface and to to understand what Wazi is and why it matters I think it's important to think about the history of web assembly and so web assembly originated as a technology for the browser to bring non-javascript code into the browser um but that me it was really tied to JavaScript in web apis things like the Dom and such and so as there's kind of this movement to bring web assembly to the server side there there arose this need to have access to things on your system like the file system or environment variables or or other things like that and so Wazi is really a specification around how do we standardize you know these these Imports into a web assembly module so they can access these things on the system and behave more like a posx program and really you know wzi is continuing that story of portability standardizing what it means to interact with a file system whether you're on a posic system or a Windows system or whatever and also that security through capabilities based security you have to explicitly request access to the file system and then you can audit whether you want the code to have access to that again coming back to this polyglot story you know web assembly has great support across a wide array of languages on our developer site we have a table tracking that and this is really just growing uh it's it's amazing how fast support is landing for for web assembly across languages now we talked a bit about performance um originally web assembly you know was meant to be executed in in these JavaScript runtime engines V8 spider monkey those kind of things um and that's of course because web assembly originated in browser that was the kind of original context but again as this kind of like push to the server side for web assembly um we found a need for new run times and so that's kind of where we get these wazzy wazzy run times such as WM time or WM 3 things like that um and these are really kind of meant to be used in more like functions as a service use cases or like iot and things like that um not for this talk but I think it's really cool to point out there's like some crazy engineering here to get this kind of like near native execution speed um so it's a really cool space so you know we've talked about those four things that make web assembly great what what are some good use cases for web assembly obviously outside of the browser three kind of come to mind first is cloud and functions as a service so why you know why is that a good fit for web assembly well you think about what you're doing in in functions as a service you're executing arbitrary code from some customer so you want it to be secure you want it to be sandbox so you know you can safely execute their code you want it to be performant so you need near native execution speed and you need it to have really low cold start times and be cheap to ship around so you want small binary sizes which web assembly gives you so that's a great fit for web assembly and you know we'll be getting a lot more into that because that's really kind of what spin offers is that sort of functions as a service Paradigm the second one here is plugins um again coming coming back to this security being able to trust arbitrary code in those small sizes makes it a great fit for plugin architectures um I forget the exact quote but our CEO always likes to quote basically that web assembly is the last plug-in architecture you'll ever need because we'll support all the languages right that kind of polyglot nature and be able to compile anything down to web assembly for a plugin then third iot is of course like a great use case we don't really focus on it much at firon but uh again the security the performance the small binary sizes all are great re to use web assembly in iot space sweet so that's kind of like the Whirlwind tour of web assembly that should be like a 4-Hour talk but we compressed it into five um now we're going to talk about spin and so what is spin well web assembly is great we talked about all these things reasons we like it but it's like super new and the developer experience is like really rough around the edge and so spin is really the developer tool for kind of making building these serverless WM applications super easy so so we started this about two years is that right Justin at firon two years ago yeah two years in change two years in change um and so here's kind of like an overview of what using spin looks like and there's kind of really three Marquee commands in spin so the first one is spin new and spin new is basically how you go from like a blinking cursor to having an application in front of you and so you know today we will be going through and types script and you want to build your typescript serverless web assembly application and you go spin new and that will scaffold out a typescript application for you that uh you can then write your code into and it will also build a manifest which we'll be looking at later which kind of describes how do I compile my application to web assembly what capabilities do I want to have access to all those kinds of things once you've you know hit spin new and you've spent some time developing your code then you need to compile it down to web assembly and that's where spin build comes in and spin build the command basically just abstracts away all the complexity of compiling your code down to web assembly then once you've spin built you have your compiled you know web assembly application then you can run spinup for a local testing experience which will serve your application you can test it locally you could deploy it to you know firon Cloud which we'll talk about in a second you could deploy it to a kubernetes cluster you could really anywhere you you can run spin you could run that spin application spin has many features you know we'll be seeing a lot of them today uh it has support for serus AI which is going to play largely into our demo today things support for things like key value stores and databases uh relational databases config and you know we even support multiple types of triggers so you can trigger your spin application like a web server with an HTTP trigger so you know you hit a route and and your application is triggered or you can set it up for triggers like reddis so a message is published to a channel on reddis and you trigger application to process some of that data or mqtt I mean the the the possibility for triggers is really kind of endless in spin um just as an a brief aside I kind of want to point out on the developer. fer on.com site we have a spin up Hub which after this if you want to check out more about spin and kind of like see examples of things people have built that's a great place to go okay so I alluded to this earlier um firon cloud is basically our opinionated hosting solution for spin apps so if you want to run your spin app in production you can of course just spin up a VM or a container and run spin app or use our new project we just released spin Cube um but fir on cloud we we view is kind of the easiest way to get going it's literally just spin Cloud deploy and you've got your web assembly application running in the cloud um and it also offers some really cool opportunities with serverless AI that I want to talk about um so we'll get into this more once we start working through the workshop here real soon um but we're going to have an application and it's going to be accessing a llama 2 chat you know generative AI model and that's pretty straightforward that's great makes it easy to to hook your your function up to that and use that um but let's think about what that means when you deploy this web assembly function to the cloud and and you want to run that and you could also kind of compare this to to using AI in Lambda it's great you've got a request coming into your Lambda or your your spin function but that starts up really quick because all the benefits of web assembly but what if you have to spin up a GPU every single time you want to access that generative AI model that's obviously not going to be very fast and so with INF firm on cloud we've we've built this system where we're able to Multiplex all the GPU usage from all these different spin apps in the cloud to our own pool of gpus in the back end and so what that gives us is really kind of two key things one it gives you really low cold start times because no longer is a request coming into your app and then you need to spin up a GPU and and wait for that you just have a already warmed up GPU in the back end running but two it also keeps your cost down because the only GPU time you're really paying for is the actual inferencing of that so you know here's kind of an example of of what an application doing that might look like and this is kind of similar to the application we'll have by the end of the day where that blue line you have a inference request coming into your spin application and it's using these large language models which are like pulled in the back background for really low cold start times then you can use key value storage to cach the responses or put context embeddings into a sqlite database and do all those kinds of things um I'll I'll just Breeze buy by this for now we I think that will all kind of add up a bit more at the end once we've done it fun fact here uh fir on cloud uh is so our GPU pool is built on sio and greencloud so every time you inference today you'll be warming a pool somewhere in London yeah fun fact I like that one okay so it's time for the workshop let's get hands on enough talking today we're going to be doing a magic eightball workshop and uh I don't know if any of you ever played with magic eight balls growing up I certainly did not I had to Google what they are but for for the uninitiated a magic eightball is a ball that you come to and you can shake it and then it will come up with an answer to your question so you might come up to the magic eightball and say am I going to have a good day today and you would shake the magic eightball and it would magically come up with an answer um and say h the answer is unclear not very helpful but maybe fun it fun fact I I learned this while I was Wikipedia magic eight balls um it's actually just a D20 that's floating in some fluid and when you shake it it just floats up to the top so cool pretty cool technology um okay so we're going to be building a spin app that's replicating this kind of magic eightball experience and we're going to be not only generating these answers but we're going to be generating them with AI so that that's how we're using the generative model and then we're also going to be using the key K uh key value store to cach those responses um here's kind of the steps of the workshop so we're going to start with setup which is kind of just installing spin or setting up your codes space to get going there and then we're going to do getting started kind of play around with the Hello World Experience then we'll roll that into something a bit more useful build a Json API that will respond with that um um Magic 8 response just kind of with random answers we'll augment that then and use the llm model to actually generate those answers and then we'll put a pretty front end on it so we have kind of a nice user experience around that and then we'll cash those uh values with the key value store I think that'll probably take most of our time but depending on how things go we have a bit of an extra exercise yeah the the extra exercise is going to be uh kind of a Choose Your Own Adventure uh we haven't finished all the code for it so um yeah okay so here's a QR code and that should take you to the workshops so yeah like we said earlier unfortunately there was a bit of a mixup and we weren't set up with tables today um but you know I encourage you to pull out the laptop on your lap and and kind of work through it here um there's I'll I'll switch my screen in a second here once once people have a chance to go for the QR code there's three ways we're kind of supporting this Workshop again which is locally um basically installing the dependencies directly on your machine you can clone the workshop's directory and use the dev container through like vs code or my personal recommendation is just go to V uh GitHub and set up a code space where it'll spin up that Dev container remote for you and then all the dependencies will just work and you can do it out of your browser and if you get stuck at anytime just raise your qu raise your hand and I'll come around help oh uh password for Wi-Fi penguin yes fast or slow I'm not sure which network to choose I was thinking it was maybe the game okay one's five gigz I've learned and one's two and a half okay so seems like everybody's got that QR code scanned I will head over to the workshop workshops okay and we'll just give a bit of time at the start here because the um installation process might be a little bit slow on the Wi-Fi perhaps seems like my Wi-Fi is a little bit slow very slow well I might have to change to do it locally if it doesn't load so I guess I'll just do that to show people for a second come on okay so we're going to start with setup and there's really as I said three paths the the code spaces should be pretty straightforward just raise your hand if you can't figure out how to start that and then maybe the least straightforward one is just configuring your local environment So for anybody using Linux wsl2 on on Windows or Mac OS you can go ahead and install spin by using our Brew tap um and then we can [Music] go ahead and install some templates those aren't directly necessary for this tutorial but they'll be handy for later um and we'll just give a couple more minutes here oh yeah whoa size my terminal does that work there we go can people see that fine in the back yes go ahead so if you click like clone the the green one and then there should be a codes spaces Tab and it will take a second to generate the the code space because it builds a container but just justtin just fine okay perfect uh let's do awesome so by a show of hands can I see who's still working on getting a code space running or installing things okay still going I know while other folks are getting set up did we have any questions about web assembly spin or anything um from the first P portion of the talk cool perfect comprehension great job yes go ahead uh the question was what languages dispense support so uh it's a good question um so we have kind of tiers of languages that's been supports right now uh tier one languages um is uh what we call like languages that have support for generated bindings for our SDK um so going back to what Caleb was talking about before the Wy spec um is built on top of this um additional spec called wit which is web assembly interface types um and that's how we describe um external uh kind of interactions from web assembly so that's kind of how we express the spin SDK and there's five fiveish languages depends if you uh include typescript as an additional language on top of JavaScript but um four or five languages right now that we would call tier one support um and then probably 10ish languages that we would call tier two that compil the web assembly um but you don't have access to the spin SDK because they're still building out support in wit to kind of generate those bindings for the SDK um but you can still use um those languages kind of uh like a CGI uh script essentially so um standard in would be your request your like kind of raw HTTP request and then standard out would be your response yeah just for people who walked in we're just starting to set up and install spin there's three ways we can do it there's a Code spaces on the workshops where you can spin that up online um people might be having trouble with that I can't get my code space to load so I'm not quite sure how the internet's working for other people and so maybe locally might be a better solution too and we have steps in the workshop setup page for doing it locally just need to install spin from our brw tap for awesome and I'll just do another show of hands who's still working on setting up the spin environment couple people okay we'll give it like a few more minutes and then we'll just jump in here and do the demo and we'll just make sure you guys are tracking through we don't have sample for python prepared you can take a crack at it it should all all the sdks are there and work um but we'll be going through typescript here and any sample apps are all in typescript or rest for okay I think we probably have close enough and and if people fall behind just raise your hand and we'll we'll make sure to come around and get you caught up and get spin running um but I want to dive in and and start talking about this here okay so we're all set up let's go to getting started awesome so you know we can all start by running spin version make sure we''ve got spin running on our machine and as I said there's kind of this flow of spin new spin build and then spin up so we're just going to see what that looks like um just kind of locally here so I'm going to be doing typescript up here at the front so I want to build a new application and I'm just going to do this in this directory here so I'll do spin new and then I'm going to pass the application of the name I'm going to say hello typescript as the name of my application and then I'm going to use a template called HTTP typescript so this is going to start kind of a scaffolding command it's going to ask for a description of my application uh so I'll give it a description and I'll say something along the lines of Hello World app and then it's going to ask for a path and so by default we're this will make more sense later when we see the Manifest but by default we're going to be using an HTTP trigger which means it's HTTP requests coming in that are going to trigger your component and so path is saying what path uh you know should I allow to come into the component what what mapping of paths and so we're just going to stick with the default here which is just saying any path can come into the component um and and we support that awesome so I should see a hello typescript folder now and I can navigate into that yes is that big enough for everybody in the back I see some leaning forward so I take that as a no how's that I see a thumbs up cool awesome okay so we're in the directory here and we'll just do a tree to see what we've got scaffolded out for us we got a read me um and then we've got kind of all the acutron you would expect of a typical typ strip project so you've got your package.json for your dependencies and you've got um you know your Source directory with an index. typescript file and and all that kind of stuff so let's maybe start by looking at the um typescript file here and see what that looks like okay so it's really simple we're just exporting a handle request function and that handle request function takes in a requests and returns a you know a sync response and we're not doing anything fancy in here we're just returning a 200 status code you know text playing content type and hello from tssd as the type now we've got the package.json just like you would expect with a JavaScript project here and you can see that we're importing the firon spin SDK and this is what allows us to use these kind of custom HTTP request and response types now kind of where the real meat on the bone comes from is the Manifest I keep talking about and that's the spin.ml so here we got a spin. toml so this is configuring a number of things about your spin application uh you know in that application section it's just kind of metadata who's the author what's the description of this app what version all that kind of stuff the name of your app but the really important part is the trigger. HTTP section and then that component section so the trigger part is saying uh you know I have an HTTP trigger and I want to take anything coming in on the route SL do dot dot which is meaning any possible route and I want to map that to my hello typescript component which I have down defined below so then I have my hello typescript component and this is what's taking that source code we have um in that uh source. TS file and saying okay you need to run this command to build um this JavaScript into web assembly and then this is the underlying web assembly that should be used to run that component now there's a bunch of magic in the package.json build scripts that actually does the work of compiling to web assembly but that's the beauty of spin that it's scaffolded for you and then you just don't have to worry about all that compilation process great so we've got the application here is there anything else I want to show you no so that's it so now we want to build this application so we can go spin build and this is actually going to fail um because I forgot the first step in in a um here I'll just kill it in a typescript application we first need to install the dependencies so I'll mpm install those dependencies if I had thought about it I probably would have done that earlier pending on conference Wi-Fi we'll see how that goes um and so this is going to install the dependencies for the app and then we'll be able to run spin build which we compile it to web assembly any questions on that so far wow that's some slow internet I think I chose fast slow was pretty fast okay I'll have to try that one out okay we're switching to the backup plan because that's being slow and I'm going to use rust because that does not need to install a bunch of dependencies slowly so I'll just do these steps quick hello rust template HTTP rust uh we'll skip the description for now and this is going to have all the same things uh the the spin. Tomo is going to look slightly different with a different build command but same kind of underlying thing so we run spin build and that's going to build our app here H that needs to update create indexes too awesome so now we've built the spin app and for rust that's going to live under Target uh WM 32 be release actually no so you can see we've got that hell russ. WM file there now if we go spin up it's going to take our built application it knows from the Manifest where to look for that web assembly file let me just make this a bit bigger and we can see that we've now got our app running and so if I just go ahead and copy paste this and then in a new terminal if I curl that endpoint we see Hello firion responding back to us awesome now just as kind of a demonstration we can hop in here and edit the source and say hello web assembly and rebuild that application because we need to recompile that web assembly to use that new hello web assembly string run spin up again and recur that endpoint we should see Hello web assembly awesome socket Timeout on the mpmi okay well maybe we'll see how going doing typescript up here at the front is awesome so that's getting started that's what scaffolding an app and spin looks like how are we doing any questions anybody stuck not making it through that part go ahead so this is kind of off a little bit of the topic but if you've got an application python one of those and com together yes yeah absolutely and then you got unfortunately lowle C function you're talking to an iot how do you you have to obviously build a web assembly API for that that kind of how you do the iot deployment you're specific system yeah go ahead Justin yeah so um you would need to kind of use wit and the Wy spec to kind of poke a hole from the web assembly guest into the host to be able to access that Hardware device okay um and that can kind of be your own um kind of interface whatever you want to create that um and then you can use like Zig uh compiles to web assembly um so I just kind of want to know the I don't want to like freak out yeah yeah no worries um that's that's part of the component model that's something that um just got uh Wy 02 uh just got finalized let's see January I think that sounds right yeah yeah and I mean to put it another way spin does as much as possible to hide web assembly in a lot of ways like you should not even to think about it but all the configurations and options are there to do all the plumbing you want and run your C with your special apis and all that stuff under the hood work oil ref and they of don't support all seral connection have big we right right right totally um another way you could could do it is uh since spin is like all open source you can create your custom triggers um so we have a lot of like Community contributed triggers um so say you wanted to run some piece of web assembly based off of some Hardware event you can write your own uh thing in like see your rust system yep yeah awesome So for anybody who just came in we're just going through the workshop flow here it's just at github.com for on workshops uh you can set it up locally or in a code space and just raise your hand if if you need to get caught up um great so we've demonstrated that kind of getting started flow let's move on to the next step here and build ourselves a Json API so what we want to do is we just want to build something really simple that takes you know in a question and returns turns just a random answer from these four examples and then we'll spice that up later so again we'll kind of use the process of spin new scaffolding out an application and then we're going to um build it uh with some of this fancier code so let's just get going on that so first I'll kill this hello world typescript I had then I want to do spin new magic 8Ball TS and we're just going to use the HTTP typescript template again a magic8ball application and this time we're going to use a specific path and we're going to say only take in requests to the magic8 path oh yes thank you slashmagic 8 awesome so now we've got our magic8ball application here similar thing this time I'm going to learn my lesson in npm install while I'm coding so hopefully done awesome so we've got a similar handle request function this time except instead of just like directly returning a response we actually want to use a question that's coming in from the request um and so how we're going to do that or sorry I'm getting ahead of myself we're not even going to use the question yet we're just directly returning a random answer um and so you know pretty straightforward stuff here just a normal JavaScript function that is taking in uh an answer and I can dump that directly into my application oops and then we can also see we are whoops sorry folks we're directly using that response instead of returning this 200 awesome we're getting the red error here because we haven't npm installed yet that's okay that will go away and we will wait again looks like it's making progress though and thankfully this will be the last time we'll have to npm install for this app awesome okay now I can spin build the app that's compiling the web assembly using webpack to to do all the bindings and such and then I can run spin up awesome we've got it going if I curl it we get no response as expected because we're not hitting the Magic 8 endpoint so it's not routing through to our component but if we add the appropriate Magic 8 we'll see some Json response pipe that through to JQ awesome and we'll see that it's randomly returning a response so you know this is obviously like really really simplistic but it kind of gives you an idea of how you might structure sort of a simple backend API out of a spin application and have different routes running to different components for your different services or whatever um you can obviously crank the complexity up from there anything else we want to hit here Justin I don't think so any questions yes yeah so we're we're hearing a question about some codes space issues we help out with that it's likely so Cod spaces does this kind of fancy port forwarding stuff which can sometimes cause issues so we'll help out there awesome okay so we got a Json API now it's time to kind of make this a little bit more fancy and and use the AI go ahead that's a great question let's go see uh so I need to see where the WM file is living and the WM file is living under Target Target get some human sizes so two Megs and go ahead yeah yeah yeah and you're saying specifically using different languages within spin sure the the short answer is no they're typically going to be similar but the way that different languages support web assembly is going to result in pretty different web assembly binaries so just as a brief example python the way we support that Dynamic language is we actually bundle in the python interpreter compiled to web assembly um which is going to increase the size that's a whole other conversation of in the future we'll be able to link that out and things like that but it will be different I saw a question there no I was I was going to ask a question about if that binary comp correct correct yeah just to repeat the question he's saying that all the dependencies you've you've compiled into that is going to add to the size of that yeah and actually your your node modules won't be included in the WM file the what takes up a decent chunk of this is the uh JavaScript interpreter so interet yeah yeah so our current the SD the JavaScript SDK use today is actually built on top of our rust SDK and it uses um a project called Javy um which I think is built on top of the quickjs um interpreter um so we take all that compile it into um rust and then we use that um our rust SDK to actually invoke The Interpreter and load up your JavaScript um but then web pack should strip out most of the node modules that aren't used um yeah yeah totally yeah so if you're looking for a smallo what language uh smallest smallest probably uh smallest with the best support would be rust um uh there's also quite a few uh like Community contributed things around Zig C++ as well that that should all work awesome okay so now we're GNA really kind of dive into the meat of this and and start using the generative AI model specifically llama 2 chat model uh to to generate these responses so the the way we make this easy is we have an SDK written around this that you can you know use in your rust or JavaScript application to to access that llm model and so what we're going to do here is we need to modify our request Handler to take the question like I was talking about earlier from the request so we want to take in that request body decode it use that that value as our data then we want to pass that data into a prompt that will pass off to the chat model and then that will generate the answer to a question I'll hand it off to Justin for a second to kind of talk about the prompt the Llama 2 chat prompt and how that all works yeah so as you can see there's some like specific formatting in here if you've never used us the Llama before um these are just instructions that llama understands to kind of uh differentiate between what like you want the uh text generation to use as context so that's this kind of CIS uh with the with the brackets around it or the arrows around it um so this portion of the prompt will be interpreted by llama as uh contextual information for um how to answer the question and then everything after CIS um up until uh the end of this instruction uh will be interpreted as uh this is the question or thing that I need to actually generate text for any questions about that before I throw that into the application there okay okay so let's start by pulling this question out of the response body here uh we will pull up our source code so we want to pull the question out here and we need to get some decoders [Music] um let's go like this so these are just kind of helpers basically to take do the bites to to string mapping that we need to do in JavaScript this isn't a requirement in other things like rust sdks great so we've got this encoder and decoder now and we're going to um decode the question out of there return you know an error if no question was asked now we need to change our answer function to not just be randomly returning something so let's just remove that for now but instead we want to do this more exciting thing and use the llm here great okay so we're going to see some errors that's expected that's because we're not importing these types from the SDK no need to to import anything extra from the package Json it's all coming out of the spin SDK package and we'll fix the one last error and we'll pass in the question that we've parsed out of the body into the answer generator here great looks like it's going to compile I'll come over here we'll run spin build again did you want to show him spin watch since he built it funny I was yeah so it it would be amazing to to think about how many times I forget to run spin build and so I got tired of that and I built a spin watch command and spin watch is just basically going to watch for any changes to your source code just kind of like any other npm watch command and automatically run build for you and and then directly run up great so we've already got up running now we're going to run this and there's going to be an error and let's see why that is well the first obvious error that I forgot about is we didn't pass any data to it so that's doing its job so let's pass a question to it uh is this demo going to work will be my question okay we got the error like expected now remember back in my presentation when I talked about the security of web assembly and the capabilities based security this is where this comes in so in order to be able to use this AI model that we want to use we need to explicitly request the capability to access that model so we can see this error here the component does not have access to the L 2 chat model so let's give it access to that and the way you do that is by adding that to the Manifest so we go back to our application and we go to our spin. toml our application manifest here in the component section we need to say hey this specific component has access to a model called Lama 2 chat so we should have seen this rebuild there and then we can rerun it and we're going to see another error perfect oh the error is because I didn't run it there we go okay another error is expected so now it's failing because for local development we expect that it has a llama 2 chat model locally in yours spin a models directory now um I have it downloaded locally and so I'll be able to show that to you unfortunately it's a it's pretty massive it's like eight gigs or something like that and so it's kind of untenable for us to all download an 8 gig model um on conference Wi-Fi here but I can demonstrate this and then I'll be able to show you in a second how you guys will be able to run it the two different ways so if I just make make a directory do spin here and uh file exists oh okay and then if I just uh link I have it just stored in one location on my uh laptop so I don't have to copy the model around everywhere and I link and AI models directory here perfect then I should just be able to run spin up again because it will be looking at runtime okay it's running now the one other reason we might not want to to use models here locally is because I'm grinding away this giant llama 2 chat model on my MacBook and so this is going to take about a minute to run uh which is obviously not great for demo purposes but it kind of paints to this story of uh being able to at least use it locally and you can have do work around like running smaller models with smaller parameter sizes Yeah by by default it will use a quantized model to execute on on CPU locally um if you wanted to take of GPU execution while running locally you would have to clone Spin and then do a cargo build with the feature uh enabled um but it is possible it's just not enabled by default because it massively increases the size of of spin CLI so so while that's chugging away oh yes question uh just will come take a look yeah thanks thanks for asking we totally want to help make sure people are sticking along with us so while this is chugg along I'm going to show an easier way we can run these models and and something a bit more practical for you guys um so we're going to just go into this other directory and I'll go into our magic eightball and we're going to deploy to the cloud and so that's free to do um and you know it's going to give us access to that firan service serverless AI function so the command to do that is spin Cloud deploy so it's going to once you've built your application bundle all that up and shoot that off to the API uh sorry the cloud um for you guys to do that Justin there's a question over there uh for for you guys to do that you'll just have to log into firon Cloud it's really easy just use as a GitHub account to log in and then there's like a little off Loop where you do spin Cloud deploy and it'll ask you to go enter a code into firon Cloud Okay so we've got it uploaded now we're waiting for the application to become ready on cloud that'll take a couple seconds awesome so we've got it running in cloud and we'll curl it with the data is my cloud demo going to work maybe maybe not all right to be determined if this is internet issues or app issues wouldn't be a live demo without this though awesome so this is just the cloud dashboard and we can use this kind of for debugging here we'll go into our application and eventually ah perfect so we can see that that worked and we got our response back um not sure why that was so slow I'm guessing just the conference internet here and we can see we've got some requests that were made into the app and eventually some of the logs will propagate through onto here um although actually they won't because we haven't added any loging to our application so that's how we can deploy the application to cloud and it will run that and again you know I I talked about in the slides but I want to talk about it a little bit more here of like what's actually happening when this spin application is running in the cloud and it's making a request to uh GPU to do an inference what's happening locally is our sdk's directly look for that Lama 2 chat model you know in your spin a models folder and directly execute that but what's happening in the cloud is it's basically taking this request to do some inference putting it on a que along with like multiplexing that in with all the other applications running in the cloud and then we have our pool of gpus that are serving this queue of inferencing requests and so that's how you're able to have all these warmed up hot gpus serving your inference requests um but not need to pay for that cold start time of starting up the gpus and only being paying for that when you're using it okay so that's um one way you can do it another way you can do it is we have this thing called spin Cloud GPU which is a little plugin we built that basically uh eases local development so you don't have to run that AI model locally you can sort of proxy it to a GPU that we're running in our cloud and so you're still spin up locally running it locally on your machine but instead of the SDK saying hey I'm going to look for this model locally it's going to proxy through an application you've put in the cloud and and make a request to that so you're going to say uh spin Cloud andit it's going to create this application in the firon cloud and then any you know inference you do locally is going to go through that kind of proxy application so this is another way uh you can do it and I'll just quickly demonstrate this I've already installed the plugin but you would just need to run this spin plugins install command then you need to to go to your application here I'll turn it off oh and just while we're here um I just want to point out that the The Local Host request we did much earlier where I said it's going to take forever did did eventually finish and this is the answer we got from running it locally this is the answer we got from running in cloud and then and then I'll show the third option here so we'll kill the spin up then we're going to do spin Cloud GPU which is just triggering our plugin if you just do spin it will show you all the commands and you can see Cloud GPU is a plugin we have spin Cloud GPU uh nit that's going to go through the process of initializing this sort of proxy application in the cloud fail to deploy that's interesting I wonder what's going on whoops perhaps I corrupted my spin Cloud GPU plugin we we'll give it the old College debugging try if you meant to load from reg spin interesting ah yes question for the group or just need some help awesome any questions for the group while I try and debug this spin Cloud GPU plugin so let's uninstall it and reinstall it and see if that fixes it uninstall Cloud GPU awesome now let's reinstall it awesome there we go well looks like I will have to open an issue in that open source project to see what's going wrong there go look at my applications and we should expect to see come on bad G looks like I'm having some internet issues here anybody have any questions about deploying to the cloud and getting that to work do we have anybody yet who successfully run an inference on the cloud not yet I see people hard and trying great question um to really briefly describe why we think web assembly makes sense for this kind of functions the service context security you know you get a sandboxed execution environment so you don't need to worry about the code people are running or at least worry as much performance you get like near native execution speed with web assembly while still getting this kind of like polyglot experience where you support all sorts of different languages and then kind of a portability story where that's much easier with web assembly a lot more depth to it than that but that's I'd say the tldr pitch okay well I'm going to call it here because I'm thinking I'm having internet issues deploying this and maybe we'll try that in a bit here I'll move on to the KV thing here and raise your hand if you're getting stuck on the GPU stuff and and Justin will come around and help and make sure you're following along great so we've got the ability to take in a question generate a response using uh you know llm inference and that's super easy but that inference is still expensive no matter what way you cut it you're still using a GPU so it'd be nice if we could cash the responses and you know every time somebody asks is my demo going to work we don't have to say maybe the internet's not so good we can cach that response so how do we do that you have something done no uh how do we do that KV has uh spin has a KV function that allows you to you know write and read from the sort of key value store that's baked into spin so first thing we need to do that is again this kind of capabilities based security we need to go into the definition of our component and that manifest and say yes I give access to a key Value Store a default one so we'll go here we'll give access to that perfect just giving access to a default store then we want to kind of add this wrapping layer around our answer function which instead of just directly calling that llm it's going to use our spin SDK to open up a KV store and then grab based off the key which is the question you know and see well let me show a typescript example we're going to see does this question exist in the key value store already yes it does okay great go get that value and decode it and then respond with that instead with the small Cav yet that if the response was ask again later well we want to try it again to recalculate that answer otherwise okay we haven't found this key in the key value store let's go ahead and generate that answer store it and return it so we can add this oops to our app get our set answer again we don't have KV imported we'll import that here update the import now let's use it let's get or set answer awesome that should work so if I spin build here great successfully built again um could test locally but we'll we'll get the really slow calculation uh you know inference process so we're going to try and deploy internet pending then if that doesn't work we can try running it locally here spin Cloud deploy if your app already exists is basically just like redeploying and like updating the version of the application we cheat and do both at the same time we're going to run spin up here locally the key value store is just backed by SQL light on your machine that's what it actually is under the hood and we'll curl to the Local Host is this demo going to work and let's slap a Time on there whoops so that we can compare a cach dancer versus not bad gayway you want to try deploying on yours I'm curious if I just don't have internet or if cloud is broken uh no so that wasn't working because of the internet so we're just still using the local one that hi in the do spin AI models yeah awesome signs point to yes but no guarantees promising okay and we can see that took 40 seconds there switch slow okay I am just gonna switch internet networks maybe I can just shop hop on sheena's Hotspot real quick uh Penguins okay now we're going to this is again running it locally with the model again and we see that it took uh 200s of a second because instead of running this really slow 40c inference on our machine we're just pulling that answer out of the key value store really quick let's just try one more time on the slow Network looks like that deployed successfully we'll wait for that to be ready here when when this waiting for application to become ready is is running what that actually means is so we've taken your wees in binary and we've uploaded into our air quotes cloud and we're then Distributing it across our cluster so it's on on multiple different machines and so then when a request comes in it can be roted to any number of machines and we can spin up your app and so it's you know this kind of unique property of web assembly as a function as service that it's not like a Lambda you don't need to keep a container hot and running to to get these low cold start times you literally just have some web assembly on disk and when a request comes in you can execute that web assembly and there's still no cold start time which is one of the cool advantages of web assembly great so oh I wish I did a time for that let's do a slightly different answer so we can compare uh strictly different so okay there we go about a second and then if I run it again we see about a fifth of the time with the caching great so that's in the cloud now that my internet's working again I just want to see if I can demo This Cloud GPU we'll deploy that application and while that's hopefully deploying I can just point out the kind of next step in this is already understand how to deploy so we'll skip over that um external DB so you know we're using this kind of built-in key Value Store baked into the spin SDK but we also have support for a number of other external data stores so like redis or like SQL light or things like that so seven is kind of like an exercise we're leaving to all of you if you want to explore where okay what if instead of using the spin KV store I wanted to go to redis cloud and spin up a redus database there then use that what would that look like yeah the the nice thing about being able to switch out the KV store on the back end is you don't actually need to do any code changes to be able to do that um you can use the runtime config uh to just tell the spin host hey I want to use redus instead of the local sqlite database um which makes it really nice to use when you need to kind of deploy things uh to different environments so um say you want to develop locally in sqlite when you go to deploy it to like a kubernetes cluster or something like that you want to point it at a redus uh key Value Store um you can also write your own host implementation I think we have additional ones for um what's the what's the Azure one oh Cosmos TB yeah Cosmos TB um so yeah you you can kind of swap out the implementation without any code changes totally So for anybody you know curious or wants to pursue that we have all these sample apps here and under seven there's some example source code both in typescript of Ruff how you might do that great back to this Cloud GPU sorry for all the bouncing around with the internet this is the the option that you're running it locally but you're using this proxy application in the cloud to access these fir on cloud GPU resources so it's asking us to set up some runtime config so we'll just do runtime config dotl paste that in there and so this is the you know the runtime config that Justin was talking about where you can switch up the backends and one of the configurations you can set in this runtime config is hey what do I want to use for my underlying llm compute and here we're saying we want to use this remote compute here's the URL for the spin app that's in the cloud and and we'll kind of proxy those GPU requests for you and then an off token that was automatically generated so now if I go spin up and I go runtime config file runtime config dotl unable to listen I have it running somewhere else we're running locally but now instead of pointing at that model underpin AI models we're pointing at this kind of proxy app which I will just show quickly here firon Cloud GPU that's the app that was generated by spin Cloud GPU and nit we'll make a request to it so this is locally um let's just do a new question how cool is spin Cloud GPU and we can see you know like 1.2 seconds so similar times to directly using it in in firon cloud and again we would expect the caching it's like one1 the time there great so we got that working let's um what's the next step here so that's the meat of the KV stuff there I'll stop here for a second does anybody have any questions about what we've done so far yeah great question yeah that so that was step three where we take this you know kind of simple Json API and then we're now stripping the question out of the the request body and then passing that question off to this function where we generate a prompt for for the llm hand that to the llm with the SDK and generate response correct and then just the one other notable thing there of making sure you're um defining the capabilities the component needs sweet I'm realizing I skipped the front end step I I knew I was missing something so let's go back and do that um so little out of order that's totally fine though let's go at a front end on this so what what we're going to learn kind of with adding this front end is that you know alluding to an earlier question can I have multiple components in an application of different languages and such yes absolutely so we're going to show how that's done um and so what we're going to be using for the front end is just a static file server component comp so it's this pre-built component that we've already compiled to web assembly and it basically knows how to take in requests to what other path it's serving under and then handle those requests and map those to the files that you have in some sort of like local assets directory so uh if I go ahead and use the spin ad command which is basically similar to like spin new except it's taking an existing application and adding a component to that so I go into my app here and I say spin add something called file server that's the name of the component using the static file server template we want it to map to uh any path so by default it looks for any request coming into static but in our case we want to do it under any path and what you know directory of of assets do we want it to serve up from we'll use the default of assets and then go ahead Justin yeah just a just a note here um could you open up the spin toml file yeah um so now we've got two paths in here and one is kind of like a wild card path so the way that spin um interprets that is it will look for the longest match for your url so say you gave it slashmagic 8 it'll it'll route to that wasm file first because that's the longest route uh with the most like URL components in it um and then any anything else that it can find for it'll default to wild cards so that's kind of how that how that works without returning 404s for other paths yeah that's a great point to bring up the spin. Tomo here right so Justin talked about how there's this new trigger route added to the configuration that's pointing with that wild card to the file server component and then you're going to notice this file server component looks different than this magic eightball component the the kind of biggest difference is with the magic eightball component we're compiling and building it locally so the source we're pointing at some local directory and then we have a build directive which is saying hey when you run spin build run this command to compile and produce that web assembly at that Source location in the case of the file server we've already built this web assembly component for you and are storing it in GitHub and so you can just point the source directly to the URL that this kind of like pre-compiled release asset um and then of course this is you know another cap capabilities based security option where we're saying Hey I want to give this component explicit access to files with this kind of mapping from source to destination so that's the spin. toml um if we go and spin up here and we use the runtime config again so it's uh quick fail to load component invalid Mount Source assets okay so it's failing and that's because I have not yet copied over the asset assets so if I just go to the sample app here copy the assets directory and then go to my app oops and paste in an assets directory we should see assets great and we've got it running now so let's curl it again hitting the local Point let's use a new question uh how is the weather today great pretty quick still working there now let's see what this front end looks like so if I navigate to the browser just on the the bare path it's going to serve up this index.html which we've gotten the assets and and'll show in a second and we can ask a question today um uh is my lunch going to be tasty let's see what it says well try a different question I've been having trouble with uh quote escaping in in the answers by the llm so that might be a little debugging Adventure we can go on to together work yes I did okay well we'll go on a little debugging Adventure yeah maybe um so let's start by adding some logging to our app so this just kind of works like you would expect it to work in a normal application let's just see you know yeah one thing to note here um something that is being actively worked on in the web assembly SPAC is obviously debuggers um so uh we you're able to generate debugging symbols but um currently unable to set break points and and stuff like that so um something that we're actively working on um but uh yeah back to back to print lines for for debugging kind of takes me back yeah I just actually saw for WM time which is the underlying runtime web assembly runtime that spin uses that they're they're making movements in the debugging space which is really exciting yeah and you know this is pointing to the future but web assembly gives you so many cool opportunities to innovate in the space because you have this web assembly module that has these like really clearly defined imports and exports uh it makes it really easy to implement things like time travel debugging because you can at the interface of this module uh what's the word I'm looking for know anything that's um non-deterministic and be able to track that and kind of be able to do this time travel debugging through there all right so we've got some logging now we're using a runtime config this is on the Local Host how's it going okay well we're getting our answer oh it's turning some kind of [Music] weird bracket on the end well let's see how this works in the cloud and while we're doing that I'm going to just show you the assets of what we're actually serving and so so the kind of meat of it is just this uh HTML file and we've just got this infer function that whenever you hit enter or or HD infer it's basically pulling that question out of the input box there making a post request to our Magic 8 end point and then waiting for that response and then inserting that reply great so we've got it deployed let's go to the cloud version uh how is the weather developer error yeah source of the errors is between the keyboard and developer Network I think it's probably in your code yeah I'm not sure what's up with that okay but so besides the the JavaScript not working there in the front end I think we've demonstrated you know kind of the key point of this which is you're able to compose multiple different components they can be pre-built or you can be building them locally to kind of compose this experience together that's the meat of what we want to present now Justin has um kind of a further steps of using SQL light and embeding that maybe he can step up here and yeah uh before we go onto that is anybody have having issues getting things running either locally or Cloud obious question yeah do we have the tutorial on the web yeah uh so github.com onw workshops um that's where you can find all of this um all this information all the markdown files um code space um and then if you scroll up um this apps directory here is where all of the answers will be so um yeah there should be answers in there for all of the sections except for the experiment that we're about to go on cool do you want to use mine or do you want to plug yours in I'll use mine okay I think I have some stuff already yeah looks good people can see that in the back or do we need to zoom in a bit no complaints okay cool okay so this is kind of like a a bonus exercise we weren't sure exactly how much would kind of take up with the first portion and it looks like we still have an hour and a half so yeah another portion of of the spin SDK yeah maybe I got shut off Mike was off um yeah another portion of the spin SDK um there's another function called generate embeddings so um let's talk about embeddings for a minute uh so so far we've we've done some basic kind of text generation right um where we've given llau a prompt to say here's our question uh contextually this is what we the function that we want you to serve and here's our question and that's that's cool uh and one of the kind of tradeoffs that we have to make um to be able to kind of run all of these inferencing requests across a bunch of different tenants um is in order to have that kind of low latency inferencing request for everybody we have to use the same model so um there's no like custom models built into firon Cloud um it's all built on top of just the the open source like llama 2 model um and if you wanted to you know run custom models that's something that we're working with a couple of customers on um so yeah feel free to feel free to find my contact information if you wanted to run custom models but um yeah that's kind of one of the one of the trade-offs that we have to make so um one of the Avenues we can take to kind of give llama to more information um is in we can insert more uh context into the prompt that we give llama 2 so uh how do we how do we kind of do that we could just dump a bunch of text in there um but then it has to kind of go through and figure out okay what piece is relevant um so another thing that we can do is generate an embedding based on a a piece of text so um I think the the kind of overarching term for this is called retrieval augmented generation uh rag for short um and this is kind of what open AI does when you go to chat with chat GPT uh you can nowadays you can ask it about you know current events or uh websites and what it does when it when it when you ask about that is it will um in its database it will generate an embedding based on your question and find other pieces of text um that are relevant to your question and include it in its prompt um so the way that this works is um that for a given piece of text uh you can generate a vector for that piece of text um and it's a 384 Dimension Vector so essentially just an an array of floats with 384 floats in it um so for like one example if we were to ask the magic eight ball will it rain today it's going to be a random answer because it has no idea what the forecast is um however if we were able to kind of give it today's forecast you know in in like some sort of Kon job just go grab the forecast for the day um generate an embedding for that um piece of text so we can start to give Lama the Lama to prompt more information um by using embeddings so the way that we're going to try to do this um and I've built this demo before but I don't have the Code 100% ready right now so it's going to be a little bit of an adventure um is we'll go through like a list of facts um so and then we will generate an embedding Vector for each of these facts and then when we go to post our question to the magic apall we'll gen generate another embedding vector and we'll do a vector similarity search um uh and find the uh closest Vector that is um to your question and then grab that kind of fact and include it into the into the Lama 2 prompt um it's kind of like a dense a dense piece of information so do we have any questions so far yeah sorry yeah the the um embedding Vector um that uh we use uh so we use the all mini LM L6 V2 uh embeddings model and that generates um a vector of 384 Dimensions or uh yeah just an array of size 384 so 384 elements and they're all like float 32s um yeah um yeah are those predefined Dimensions um uh to be honest I'm not entirely sure I just know at a high level that if I give it this piece of text it generates a vector and I can use that to compare similarity to other pieces of text um kind of doing like a like a search this also is in my area of expertise but I'm pretty sure the embedding Dimensions is just baked into the model so different models might have just different predefined embedding sizes yeah cool all right well let's uh jump into some code uh no you wouldn't be doing that in real time um so you like um you need to kind of preedee uh like all these vectors that you want to um um and possibly include in your search so what we're going to take a look at here is using kind of our built-in sqlite database uh which is backed by turo and that has the SQL like uh vsss extension Vector similarity search extension kind of built in um so we'll store these vectors in a sqlite database and then we'll use a sqlite query um to run a ve Vector similarity search and it'll kind of rank our most similar similar vectors Ju Just to add some color to that that we're talking about what we are building right now in in this little demo you totally could hypothetically you could have a spin application where you have your sqlite database and you have a Chron trigger that's triggering an application every hour to go fetch current information generate embeddings for that load that into the database have this separate you know endpoint that's serving up these inferences so not what we're doing right now but totally is something you could do yeah so if we have the code already maybe a good place to start would be showing them the prompt of how we're going to embed that additional context yeah so I'm just starting from I'm kind of working off the answer uh so building on what Caleb's done so far um and uh um yeah so this is kind of our our basic answer inside of this CIS section here maybe just zoom in a bit oh yep um we're just going to add in a a spot for well you know first we need to go through and and the process of setting up our kind of SQL light database so um the way that that I'm going to do this is uh using it on spin Cloud so I think it's spin Cloud sqlite so I've already created a a sqlite database and I can see it here yeah so this will this will be the the database that that I want to use here is Magic 8 um so the way that uh you would do this if you're follow trying to follow along um emphasis on trying uh is you would run this spin Cloud sqlite create and give it some name um yeah and then um we're just going to create kind of a basic table for this so um I think I've already got a well we'll just write it out to a file and just while Justin's doing that I want to remind if anybody's stuck on anything or as feel free to raise your hand and I'm happy to come around and help cool so this is just kind of the basic table that I'm going to create here um and I can do this by running spin Cloud SQL light execute just make sure I get it right yeah so give it the database name I just called it magic 8 uh and then it wants a statement um so you can either kind of copy paste the kind of text in here or you can use the at symbol to uh tell it what file to use and that completed successfully cool so kind of like Caleb did earlier uh where we gave our spin app access to lud chat we're going to give it access to two things here so we're going to give it access to our AI model called all mini LM L6 V2 and also the uh default SQL light database so just going to paste this one into the magic a ball component uh so um I have a bunch of different spin demo applications deployed um so I have a default database that's hooked up to other things you guys probably won't run into this um I'm just going to change this does this need to be a label Caleb in the spin toml to be honest I don't know that's all right um so I need to link this magic 8 uh sqlite database to my magic 8 ball application um you guys might not have to go through this um I do again if you have any questions on any of this go ahead and ask Caleb cool so I just had to link that in um like list cool yeah so now it shows that my Magic eightball app has a has the is linked to the uh Magic gate database that I created with a default label um there's a bunch of different reasons why we created uh labels around the databases so that you can kind of create databases and um instead of having to like back them up and delete them you can just Link in a new uh database um and then you still have your other one there in case you wanted to swap out okay so going back to kind of our sample code here um we've got our uh database set up and hooked up to our um magic 8Ball app um and we've given it access to the all mini LM model uh and here's some sample code that I wrote um to be able to kind of take in a piece of text and generate embeddings for each of the paragraphs so let's see what that looks like all right so we're just going to paste this code in here and fix all our Imports uh so this is a type that I created earlier um in order to kind of store our embeddings and then one other thing that I was doing here is just kind of generating an ID for it not technically necessary but going to do it anyways uh so we can still use quite a few uh mpm packages uh the only thing that definitely doesn't work is uh things that have native node dependencies um so it has to kind of compile see right now um that doesn't work um but we're working on an update for the SDK where you'll be able to bring in some of the kind of native dependencies there as well just make sure I haven't made too many mistakes yet oh there we go didn't include the types great so that should work um and kind of want to like a different end point for this so um I think what I'm going to do what is it request dot I'm just going to hack this in here if I was doing this a different way I would probably put it into a different spin component but for now this will kind of work for e uh so since I want to use kind of a different path to be able to kind of insert these uh embeddings into the database I'm going to add a wild card to this route um and it should still work um I'm just going to kind of intercept that path here again I'm just kind of hacking this in here uh for the sake of speed uh so ready to insert the data into the database we're going to see here I'm pretty sure this is going to give me an error but no didn't need to link in the for for just while we're waiting for the uh app to deploy to the cloud here any other questions for the group kind of big stuff about web assembly or anything we've done today be happy to kind of address that for the group yeah um it kind of depends um and this is where like most people kind of start using things like Lang chain um where you can uh say like I want to use this website as a source of information so a really typical one is um like a a knowledge base like an internal knowledge base for your company uh where you can kind of go through and index um your like internal uh blog articles or like uh documents um like Word documents or whatever um and then Lang chain will kind of go through there split out the text similar to like what we're doing here save it somewhere in in like a vector database and be able to look it up that way um yeah so there's a lot of like Lang chain examples out there around it um the I think the creator of olama was also on our on our Discord and he's also adding support for being able to use AMA and spin and being able to do kind of retrieval augmented generation using those apis as well great question cool uh yeah so this is this is again the the 384 Dimension Vector that gets generated um so here's the text weather forecast for today shows 80% of rain um this is the kind of embedding Vector that gets saved to our SQL light database um so now that we have a fact in there um let's go through and see if we can use it so the next thing that we need to do um is to create a virtual table inside of our SQL light database so we've we've inserted that Vector into our table uh and now we need to create this virtual table in order to uh perform the vector similarity search and this is just um you know just how this SQL light extension works so I'm just going to create another SQL file called it call it reindex um so every time I would go through and create a um or add in more context there I would need need to go reindex the reindex the database I'm sure there's a more clever way to do it um this is just the way that I found works right real just make sure I didn't fudge the uh database name h I think it's got to be something with the query I think you I think the plugins are all enabled by default in Cloud H well this was working earlier let's just point it at it's fairly anticlimactic that the um those statements aren't working I'm obviously doing something wrong not sure what it is maybe our backing database provider is having an outage who knows demos right uh so I think that's probably probably all we have since the can't get this portion of the database um query to work um I'll probably post like a fully working example later today to the workshop's repo if if you guys wanted to check that out um but at least we uh generated an embedding so that portion Works totally uh you can also do a cosign similarity search in memory um if you wanted to kind of query out all of the vectors load them into memory and then do the search obviously not quite as performant as performing it inside of a vector database um but that is possible too we've got about 55 minutes so we'll be around happy to answer questions questions talk with you guys about spin apps you've been building do any of that kind of stuff so thanks for taking the time to attend this Workshop yeah ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "3sgPJJTstoA": {"video_title": "Stop using kubectl, and use Git instead! - Hands-on GitOps workshop using Argo CD and Helm", "video_description": "Talk by Nicholas Morey, Justin Marquis, Christian Hernandez\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/stop-using-kubectl-and-use-git-instead-hands-gitops-workshop-using-argo-cd\n\nKubernetes is a declarative-first platform where manifests written in YAML describe what resources should exist in a cluster. Many new users will use imperative processes, running `kubectl apply`, to define and change the state of their cluster. This approach is known as the \u201cpush\u201d model, and while it works initially, it does not scale as more users adopt the platform. Actors outside of the cluster should not be able to make changes directly. GitOps practices provide a declarative approach to defining and managing the state of Kubernetes.", "transcript": "all right welcome everyone uh Hands-On get Ops Workshop using Argo CD and Helm um other also otherwise known as stop using qctl use git instead um so this Workshop is uh brought to you by CU um creators of the Argo project so if um uh in case you didn't know uh Q got was is a startup founded by the creators of the Argo project so um to get started here here um this um this will be you'll be certified introduction to uh get Ops and Argo CD um at the end of this so if you want to receive uh certification um go ahead and uh scan the QR code or else go there on your laptop I'm going to keep this up for a little while uh also uh we do have um marketing didn't actually send us enough uh swag so like uh the first what do you want to say 12 people sign up get something get something 20 people get uh first 20 people sign up uh we we're getting a notification here uh to get your certification at the end also we have uh something little special as well for the first um uh first 20 people so I'll leave this up for a few more seconds while I get some water also I'll give you this mic here so you can do your introduction it's hot so is that yeah I'll I'll hand off to you okay cool cool looks like everyone took a picture of it Andor scanned it um if you haven't um uh you can come up and get it from me later uh if you wish here so we're going to say who we are so all right introductions so my name is Christian Hernandez I am actually an Argo City maintainer I'm also an open get Ops maintainer I am head of community at aity here um so again a created the Argo project long long ago inside of into it um also native to Southern California Native Los Angeles I actually live in the west side but it's my actually first scale so interestingly enough I lived here all my whole life but this is my first time here so um um is I'm excited to check it out I'm also a guitar player so any musicians out there uh want to grab me in the hall um we can nerd out on uh guitar and uh you can find me on the socials uh GitHub x uh Twitter and I'll let um Justin here uh introduce himself hi I'm Justin and I am also working at Acuity here with Christian and today we're gonna show you an introduction into Argo CD um if you guys have any questions at the end feel free to ask we should have plenty of time I I am also Argo CD maintainer and I'm a technical support engineer at acity and I am also local here to Southern California this will be my Fifth Scale and my first time presenting and I've been an avid scuba diver here in Southern California for the last 20 plus years awesome awesome cool so um I'm going to be running through this Workshop present well first I'm going to go through a presentation kind of just get some background and running through the workshop with you if you have any questions or running behind or you know need any help during the workshop raise your hand Justin here uh will help you out um he's actually uh building out the workshop updating the workshop as we speak so um I'm going to go through this presentation and then maybe take any questions out of the presentation so um first I kind of want to gauge um you know the level of knowledge here U oh actually before we start the prerequisites the only prerequisite is you have a GitHub account um we trying to make it as easy as possible um if you don't have a GitHub account now is probably the time to start uh signing up for one um we use Dev containers right so um inside of GitHub so we try to keep this as self-contained as possible try not to stress the Wi-Fi too much um the people who did a presentation before us actually said uh there they had some issues with rate limiting and whatnot so um I actually jumped on the slow Wi-Fi actually is faster for me funny enough so little trick I don't know if uh if you're jumping on it'll make it slow but um so yeah so anyways prerequisite GitHub account um that's really in a browser right really trying to make it as easy as possible so uh show of hands right um uh kubernetes day I guess uh you guys are familiar with containers at the very least all right cool sounds good good group familiar with kubernetes show hands all right awesome awesome cool um familiar with get Ops okay a little less awesome Argo CD Argo CD okay cool cool we got a good uh good mix here um of folks so uh a lot of the times I it's it's hard for me to judge because I'm usually like at cubec con um or like a red hat Summit or something like that I uh uh people are more familiar than a conference like this where it's like a wide range so uh we got a good mix here so uh let's talk about kubernetes a little bit um this review for a lot of you since I saw a lot of hands up there um but um you know we trying to um build on why raing Argo City and I always like to start with kubernetes right so um from just from a really basic 10,000 foot view um kubernetes is a resource orchestrator um it started off as um orchestrating containers right you have containers a convenient way to package an application um but there was really no way at the time um um to orchestrate that across a large environment and so it started off with containers um it it moved on to really being like a a re um a resource orchestrator to now because of the kubernetes API and the introduction of something called a custom resource definition you can start orchestrating other things um even other things outside of kubernetes if you are interested in things outside of kubernetes outside of containers you can look at things like the uh cross uh crossplane project um where they're orchestrating things outside of kubernetes but using kubernetes apis um and kubernetes is based on something called a promise Theory right um give me your yaml and I'll make it so sort of thing but I think Kelsey high tower had a really good um comparison to kind of like the US Postal Service right I have you know a letter with uh destination right what where I want this letter to end up right um and I give it to The Postal Service right I don't have to worry about um you know uh planes being delayed I don't have to worry about trains breaking down I don't have to worry about like delivery trucks breaking down that is taken care of for me right I I don't have to tell them how to how to get my letter there it's just I just tell them what my end state is right and um really talking about the declarative nature of kubernetes um which is uh which can be done because you know containers are immutable um so we really kind of want to describe the end State I don't want to impera imperatively say um you know how to get there I just want my end state to look like this right and how that gets done I really don't necessarily care about that um and that's kubernetes um made up of different controllers right sort of the idea of do one thing but do it very well right there's a controller to um scale a pod right um you know the Pod controller that's all it does uh there's a controller for services there's a controller for deployments there's a controller for each individual thing it does it very well um and it's very Loosely coupled right think about things that are Loosely coupled um in kubernetes all based on labels right do one thing do it very very well you can build relationships between those things um based on labels and uh one of the things about kuber say cloud agnostic how I interact with um you know aw right and how I interact with uh gcp is basically the same I'm using the kubernetes API and the backend infrastructure doesn't necessarily matter to me um it matters depending on who you are right if you're either a developer you really don't care uh maybe you care a little bit more if you're an operations platform engineer type of thing but um it's Cloud agnostic meaning it's it's the same similar apis no matter where I'm running my workload so 10,000 foot View kubernetes I'm probably um you know saying things you you guys probably already know for those who are um already familiar with kubernetes right so um so there are some drawbacks to kubernetes um it's um you're you're take you're going from a monolith right an application it's like this is my application um this is you know especially for those you Java folks it's like you just kind of know that's like this is my application this is my monolith this is and I deploy it um and we're going from that to Loosely coupled objects right like I mentioned before look like it's it's you know do one thing but do very well but your application is actually complicated when it comes down to it you have a a like a you know frontend API in a container and I have a service that is basically my network uh coming in like well those things kind of need to come together right I need to be able to access you know if I have especially an API I need to be able to access that API via Network somehow so there's a service um attached to that there's an Ingress attached to that there's storage that's attached to that there's like all these little different components and now um you got uh essentially kubernetes is you got like rat droppings really everywhere right and you're trying to um um trying to like kind of like deploy them and aggregate them together and it's um um you know if you're running like one cluster you know or maybe you know half dozen cluster that's kind of okay um but when you start getting more and more into scale and different environments right like you don't just have a production environment um you have different environments and those environments can have you know dozens and dozens of clusters and trying to keep track of all that is a little difficult right and um also another thing is that oh actually changing the outline that logo got big but um you you took now we have declarative infrastructure but people are still managing managing that imperatively right it's like oh yeah I'm using terraform or I'm using you know um um you know I'm using you know my anible or I'm using this I'm using that like you're still kind of doing things imperatively with an with a declarative infrastructure right um before I think someone said like oh kubernetes is kind of like how a system is designed if I took away your SSH keys but we really traded SSH for cctl right like there there was really no no difference there um okay I can't SSH into the system but I'm you know managing it with Cy tail now um and it's it's really CC tail is an imperative way um an abstract comparative way of managing kubernetes and actually now cicd got um became harder right cicd actually became harder with kubernetes uh instead of making our lives easier uh it made it a little harder because now um we're trying to do synchronous tasks you we're trying to meld synchronous task and asynchronous tasks together right the asynchronous nature of kubernetes eventual consistency but like you have a CI CD process that like needs to wait for things and but now you're trying to meld the Two Worlds of you know you know especially if you're going from like VMS to um you know traditional VMS to uh to kubernetes like now it's like okay now I'm melding um synchronous processes with asynchronous processes that um and uh you know now you're like you're going from VMware to reality right um one of the things that early on in kuber um that people seem to not get is like you know we come from a VM World VM wear world I guess broadcom now um where like you have like the VMware cluster and it's like the central place and it's like well people were trying to treat kubernetes that way and it's like no actually you're running multiple kubernetes clusters and um there's really no Central management system for kubernetes um at that time and so you know there was a mind shift there as as well so um so there must be a better way right so there you know so now we have you know um all these little kind of rat dropping objects that you need to manage across multiple kubernetes clusters across multiple environments and um and so the idea came by uh of something called open get Ops so for those um for those who interested open get ups. Dev but um open getups was was a um kind of a work it started out as a giops working group and it started with members from actually the Argos the Argo project members from the flux project um members from different um you know all the big players that you'd expect Red Hat um AWS weave Works um codefresh uh into it um I'm probably gith Hub I'm I'm probably missing a company VMware even as as well just like all these companies came together it's like okay let's Define what get Ops is because now we have this little thing called get Ops which is basically managing everything in get and then everything else using git as a source of Truth essentially and it's like well let's define what it means to be get Ops because we don't want happen to get Ops uh well what happened to Cloud right like really like everything's Cloud now like Cloud just became a like first became like a buzz word and now it's like everyone calling everything Cloud um um you know people calling their local storage Cloud um their Plex servers whatever right oh that's my cloud um and so we want to Define what it means to be um what is get Ops right so get Ops there's four principles um if you visit open get ops. Dev there's more information about this but there's really four principles of what get offs is and it really stemmed from the fact that we want to Leverage The declarative nature of kubernetes um we want to leverage that and we want to manage our entire system that way um so instead of managing individual objects with cctl we manage it holistically right so now you kind of have you see the entire infrastructure holistically um and if you're running things declaratively right a system manag I don't usually read off the slides but I think this is um I usually do when open get Ops comes up the getups principles is a system managed by get Ops must have his desired State expressed declaratively right so no um um you we want to take advantage of a kubernetes system right of of a system that's declarative by saying okay well like we want to describe everything declaratively it's version and immutable um ironically we don't use a we don't use git in getop uh open gitops right because um as long as a system is version immutable which git is but other things are version immutable like an S3 or object storage oci um other things like that um SQL light technically um is version and immutable but the desired state is stored in a way that enforces immutability versioning and retains a complete version history um it's pulled automatically so um I I actually personally don't like the wording on this but you know open source you get outvoted um it says pull it has nothing to do with push or pull model has nothing to do with that uh what it means is that the Declarations software agents automatically pull the desire State declarations from The Source uh the meaning behind this is web hooks right um and um this always this always causes some animosity especially on Twitter um I'm not against web hooks you'll definitely use web hooks even in a get Ops environment but getop says don't rely solely on web hooks you're use web Hooks and you're going to use an agent to continuously monitor your desire state so that way it takes care of things like configuration drift and things like that you don't have to wait for the next run or you don't have to wait for some event to happen it's just continuously monitoring your system um and which then leads into the the final principle is this continuously reconciled so Contin reconciled meaning that okay um is if I see some sort of Divergence I'm actually going to attempt to reconcile that for you um software agents continue you see observe the actual System state and attempt to apply the desired state so anytime there's any drift U whether on purpose whether uh by accident whether by you know whatever means you know is you know we want your desire say that is your source of truth if it doesn't match that I'm going to I'm going to try that right and so um interested in attending a meeting please attend a meeting open get offside de there's a uh there's a link there to attend a meeting to find out more um I uh we are we at open get Ops uh um project we focus specifically on kubernetes but we understand that there's entire systems out there um that you need to manage in a way so we actually would want to to see okay like how can we expand this to include things that are kind of not declarative right and so um like VMS like you know even mainframes like you know whatever right like we we need to have some of these principles to expand so um so that's open get Ops and that's get Ops in general um I want to kind of just go over what happens without get Ops right uh talking about using Cube CTL um the uh the graphic here I I I can go I can go over it but the the main idea behind here is that any part in this scenario can change your cluster right whether it's a developer directly or a developer writing something in a repo a repo um you know using GitHub actions or whatever writing into uh you know changing stuff in the kubernetes a container registry um doing a a a web hook to to update something your CI system um you know me coming in um doing cctl running something um I can change the actual state of the system what we call an out of band change um to the system whether whether it's wanted or not wanted whether it's needed or not needed whether it's a break glass scenario doesn't matter changes occur in your um in your cluster from anywhere and you don't really know um necessarily know where that change came from whether it's it's um whether you do it or not right and so with gitops there's this kind of little barrier right there's this interface is what I like to call gitops is say interface into your system is G because there's a controller it doesn't matter what changes right I can go in cctl scale something right a deployment but if it doesn't match the desired State the get offs controller will revert it back um and so any changes to the system needs to be done through get workflows that's the idea um whether that workflow um is um whether that workflow is something like a PR right it's like okay hey like if I need to make a change to the system I need to make a PR to the to the repo and approvals need to go through through place or whether the CI system creates the pr or whether uh the CI Sy whether you make a PR and then then triggers a CI system that changes the idea is that your interface into the cluster is your source of truth right and get in in this case and so um yeah the gitops operator has a readon access to the git repo and and it what it does is essentially just does one thing does it very simply right is um it uh takes your desired State and just make sure that um the running State equals your desired State and so now uh whether it's a developer whether it's a CI system a robot whatever how I change a system happens through get right and you know there's other there's best practices and things like that that um you know we can go over as as we as we move along but things like protected branches and um uh things like that right approvals uh get blame whatever roll back roll forward all that's done through get workflows right um so uh some of the things uh that get Ops gives you control right like as I said before all interactions happen via git um especially if you're using like GitHub uh or gitlab git T right you you have this kind of visual of like who made what change when where why um collaboration right so like if everything's out in the open um how I change how the system changes how the system behaves is all there um and again like I said before you can set up approvals and things like that um from different people is like okay only you know only certain people can merge um at certain times that sort of thing um compliance right A lot of people say um gitops is secure I'm like well it's it helps in security right it's it's it it'll help you along way I don't want to necessarily say it's you know do get ups and it's secure I'm like well there's other things to take into consideration but um you have a clear audit log of you have this convenient audit log especially if you're using git of the state of changes of who made what change why and you have this kind of like log of like oh what happened on you know this date on this time it's like okay well you know you know Justin made a change and you know scaled it up that's why we're getting charged more right so that you know that sort of that sort of thing um and so uh you can do roll backs as well right since things in get and you have that that convenient history you can roll back to a certain time um those are um uh those are a little trickier I understand with with gitops especially with Git especially with State full applications but um you have that ability right to see where where it was and make the rollbacks a little easier and Disaster Recovery um that this is probably like my favorite thing things that I've that I've done things that Justin has done as a support engineer is like okay well you know build up a new cluster point it to the gate reu everything magically works like it there's little nuances there but it it actually is as easy as that right um you know you may have to do something with storage and permissions but like it really expedites the disaster recovery process uh using gitops and so um that's kind of the things that the idea what G UPS gives you and what um um the the um the whole theory behind uh get UPS so again I'm open get UPS maintainer so uh feel free to go to open get ops. deev come up to me afterwards um we going talk about get Ops and what we're we're trying to do and so um so now enter Argo City so um before I go into Argo City too much the there was kind of like two main players in the kubernetes um in the cncf landscape that came up um around the time trying to solve a similar problem right I mentioned before add into it um our our Founders at at Acuity created the Argo project because Inuit needed a way to onramp their developers and their um their Engineers onto kubernetes they made a strategic decision to move to kubernetes and Cloud native and they needed a way to onramp um the workloads without teaching kubernetes to developers right so they they needed a quick way to migrate and do it in a way that developers um um doesn't have to doesn't need to know the minutia of kubernetes that's where Argo City got developed around the same time um company called weave Works needed a way to manage their systems um uh effectively right at that time we Works was a a managed service provider um they had a lot of lot of products but internally they're like oh we need U we needed a way to manage uh our internal system and to recover cover and um if you ever read a Blog about this Alexis Richardson um former CEO of wave Works has a funny story how it came up where um someone fat figure to config took down an entire region right I'm pretty sure we've all been there um and uh they recovered like within a half hour and he asked the question how do we recover so fast right like usually CEOs get upset when something goes down but they're like wao that was like Fast recovery why and they were explaining kind of some of the process they were doing and kind of these tools building and that's where flux came from right so there was that kind of the two um uh Back stories about these tools being developed to trying to implement some of these um uh some of these principles right that I just got through with get uh with giops right and so so Argo CD so Argo CD um is made up of different components right um flux is kind of built as a tool set for like sres and things like that Argo C was really built as a platform so it's made up of like more components um and so there's something called the application uh resource config management tools applications uh application Health app projects those things you you're going to hear a lot uh from Argo City only because they're abstractions from trying to use kubernetes directly and so um this is actually uh Argo City components right oh that's that's where that ended up anyways um that Aro City component should be on the top there but anyways um it's made up of different um different um parts right and so um the first part is actually the API server so Argo C's um interface into Argo c as the API server so whether you're using the web UI or the CLI you are interfacing directly with the API server so you're not really using Cube C at all um you're interacting with Argo CD and Argo CD does these things on your behalf um and it's at the core it's made up of it's made up a few components but the main components is the um uh the application controller right so the application controller which I'll go deep in a in a second is basically um an application is an abstraction away from these Loosely coupled components so remember where I was talking about like you know KU you know you you know you have a service you know maybe you have an application simple application right you have a service a deployment and maybe an Ingress right but like you want to deploy them together well that's an Argo City application right Argo City application is basically a collection of these objects right it can be as simple as you know service deployment Ingress or crd um you know uh was it a Chron jobs uh stateful sets right you know like you can have but that's kind of it's a unit of measure but basically it's a collection of objects is what an application controller is uh what an application in Argo CD is um and then there's the repo server right and so repo server is essentially that component its only job and I'll put this as simply as possible is doing a get clone that's all that's all it does so it looks at get does a get clone says is there is there you know is there a change oh there is a change it notifies the uh application control say hey there's a change you need to apply these new manest um and that's all the repo server does so there's like kind of like a separation there of um because that way the application controller has no idea your about your credentials or anything like that all it does is like um it just looks for a change looks for a notification from the repo server like hey there's a change um so that's what the repo server does there's also connections to like Helm and stuff like that the repo server does things like it renders a lot of these manifests from like if you're using tools like customize or um or Helm but the idea of the repo server is kind of that separation it's like all it does very high level very simple get clone is there difference yes okay application controller do your thing so uh and then there's the application set controller someone uh I believe you walked up talked about application sets that's kind of a little bit more of a an advanced topic but from a high level application set all it does is it creates applications right so um you as you can imagine you can have um you know your application are made up by different components right simple front end backend database and U you need a way to kind of like deploy those um in a way without creating a manifest for each one of them an application set controller it's essentially a templating uh engine to create applications so this kind of like a an application Factory is like what I like to call it um you know you fit you in something you know buzzes and just spits out applications um so it's templating agent right and so all these core components uh interface with the cubec uh with the the cube API right so Argo CD interfaces directly with a QBE API to do the things it needs to do um it uses a red as cache uh for a lot of this um and um you know the repo server interfaces with Git it also does with Helm but um for the most part interfaces with Git so you don't have to uh connect everything with Git and then uh there's a uh it Argo City also includes decks so uh like I said before Aro city is a platform and during the workshop you'll see why but um Dex uh just in case you need to use SSO right so Argo CD is a platform Argo CD allows you to connect create users and abstracted users you know different from the uh kubernetes um you can uh assign policies and things like that and a lot of people connected to um you know something like GitHub SSO um Google um actor directory right anything that Dex uh connect to so it interfaces with Dex so these are kind of like the core components of Argo CD um uh and there's other things that that that come along with it but these are kind of like the the core functionality there's also like a notification engines there's uh there's rollouts there's you know other things to do like a little bit more complex um kind of deployments but this is like the core functionality in um from what we are going to uh do during this Workshop so um I mentioned Argo CD application Argo CD application like I said before has is a is a unit of work the Argo CD is basically collection of related manifests kubernetes manifest specifically and um the basic uh principle of it is like you define your Source you define your destination you define the sync policy for example the source is this git repo you uh look at this git repo you look at this path and you're you're looking at at the head right or you can you know put in whatever Branch or tag or whatever but um for the Target revision and the destination you say this name space um on this uh this kubernetes cluster right so that's where the the name comes into place that could be name or server URL right um saying all right so this is the source I want this is my source and this destination I want you to make this alike and then you have sync policies where it's like if the namespace um isn't created cre create it for me before I start trying to apply these things there's other sync policies like retries uh Auto heal things like that we're going to go over those in the um in the workshop right you're going to see some of these sync policies in action as well but it's essentially um Argo CD is a crd that you define your source and destination your sync policy um one really cool thing about Argo CD and you'll see this again in the uh in the workshop is that it has has a um a web UI right essentially when um when the folks over into it were creating Argo CD they heavily focus on the developer experience because again that's that was the problem that we're solving was uh developer um trying to get their developers onto kubernetes so they built um uh feature Rich web UI and it gives you things like a top level health application so like at a glance I want to be able to see the health status of all my resources that are managed by this application um I want to see information about individual um components like a pod right like if the Pod is misbehaving I want to be able I want to be able to see that um and be able to Define custom health checks and so like if you have some workload that needs some you know song and dance to be able to say if it's healthy or not um all that bubbles up in the webui be able to see it uh from the interface it really is I I I would argue the the first kubernetes um UI that was out there um well the first one that was this feature Rich and so um you know at that time you know now there's you know web UI right like um you know I I used to work for Red Hat you know open shift that was a fantastic UI as well um but this is was one of the um first uis that came up uh I I would say in in kubernetes and had all this information helped with things like triage um and things like that so so we're going to see it all in action um with the workshop here so um while you're getting your laptops out and getting ready and things like that I want to pause here for a second and to see if you have any questions I know I just dumped a bunch of information at you um it was kind of like a brain dub I introduced a lot of like Concepts and things like that so are there any questions so there's question here yeah I'm sorry can you say it again oh mic one second uh will the slide be available after the workshop oh yeah uh is there a place to upload them or we can make it available somewhere yeah we can we can make it available for sure yeah question quite a bit you're earlier uh so this might be too deep in the weeds but I've used ero CD quite a bit already MH and we've been playing around with operators and trying to do stateful stuff yeah and do you guys recommend it for stateful or we're like pulling out of that and I'm just wondering you commented on it and it feels pretty hard especially if you're going to move stuff around yeah it's It's Tricky with stateful uh workloads with Argo CD um just because it's just tricky with kubernetes in general at running stateful workloads but um there are tool sets in Argo CD that do things like um are using like things like syn waves pre- sync hooks um ARG roll outs doing that as well but um upgrades are like upgrades a little tricky as well like if you just want to like move things around it doesn't you can't like move it between name spaces I guess that was one issue we had yeah that's true yeah it's like it's like a destructive operation I I I know it's like the it's a limitation of kubernetes more than Argo CD but uh yeah just wondering how you folks handle that if at all um is that just like a full migration to a downtime that that is um do do you know Justin like I know you're and I'm aware this might not be the appropriate time for a deep level question like this but I could ask after yeah yeah know we can have a conversation conversation because like it's I was going to say it's kind of nuance I know we work with customers that that do a lot of these things and um there there is a lot of like you know hand wavy things that do so but for sure yeah thank you all right any other questions questions comments before we Dive Right In no cool all right we're feeling good feeling good all right um so I'm going to uh pop them in the next slide I do recommend taking a picture of it scan the QR code doing something with it because there's a lot of information about the workshop on there um and I'm going to be moving back and forth I'm going to be doing the workshop with you so uh whatever issues you run into I'll run into as well um so you know Keeps Us honest as well about the workshop um also Justin is walking around as well if you have so there are two tabs um technically three eventually um we're going to have the workshop instructions are on uh that link I'll work with my marketing to get a better slash because q02 P9 you know like it's gonna um I'm sorry any way access it on the laptop oh the the the link yeah uh we we tried um we tried doing something like that I don't know they didn't give us a place to upload the slides so um so yeah so this is here uh scan that QR code or uh take a picture and you know type it in url I'm going to leave this here for a little bit I'm going to be going back and forth you're going to need two tabs right one is the workshop instructions and two is the workshop platform right so AC has a platform that manages Argo City for you we kind of carved out a training instance so that way um uh you can have like all the you know all the features without uh um signing up for anything um this will take you Workshop platform should take you to training. ac. Cloud um and so uh either you know sign up uh with your uh GitHub SSO Google or um email you know whatever it'll drop you right in but I'm going to give it a second to let uh everyone kind of uh get that this information on the laptop we should have like printed this out it would have been cool yeah what would an unexpected eror uh oh oh that that is so that's yeah that's adjust an error let's see if it works for me I'm going do Google okay you did Google I did I did Google authentication um I don't know if if you're doing it with uh the email you may running a a different I want to make this a little bigger so you should see this and then let me open this in another tab so let me make this a little bigger as well so we have uh the lab instructions on one Tab and the other one after you log in um let me log out kind of goes go through this process again so yeah maybe reload the web page ironically I do run Edge as well on my Linux laptop anyway um I'm sorry yes yes so these are the links and you'll open two tabs connect scale I am connected to slow because it's faster [Laughter] ironically it's working I wonder if it's uh what's that all right I'm going to switch tabs really quick so you should see at least this comes up and then the other tab you should be like around here so that's kind of where we are I'm going to leave o so I'm gonna come back and leave this up for now all the internet yeah I'm downloading oh fun okay so you got it to load okay yeah yeah it might be slowish so we haven't gone too far if you if you just walked in we're here in the in the lab instructions and the lab platform should look like this um if you're here feel free to either continue with GitHub continue with Google or uh do a username password um for an email I'll drink some water because for what it's worth I'm using Chrome oh do you okay okay who's still having trouble those there zeros yes there's a couple people still having issues um looks like mostly someone did mention that it might have been the Wi-Fi CU it's you f just slow and yeah so if like half of you go on slow half of you stay on Fast maybe um slow oh really if you're get the white got tell Remington about that any way to make the JavaScript a little lower okay so have all of you at least gotten the uh thisinformation yes okay cool I believe oops Yeah okay so you should be here um there's some cool uh information if you want to watch the video later and you know that's uh that's totally fine I think this is Nick's video um but this um so what we're going to do is it's a little kind of little scenario right um uh we're going to we're using Dev containers on GitHub again I mentioned that before but uh worth reing on GitHub code spaces so um uh so um when we're on the platform anytime something's asking about your organization um I'm just going to use my GitHub username you can name it whatever you'd like I just call it my GitHub username uh we just kind of um to keep it kind of mindless um we're going to be using Dev for your environment so anytime uh we talk about the environment name or anything like that uh we're going to be using the word Dev uh for your environment um um another thing to note um if you already have code spaces running and you're like on a free tier you may have to like shut down or stop one of your instances um for uh for that I know I've ran into that um before and so um so yeah and then we're going to be um we're going to be using a a template from GitHub so make sure um when when we get to that step on GitHub uh when using the template make make sure you use this template don't Fork it cuz the lab won't work if you if you Fork it right so like or you can feel free to Fork it but then choose use this template right and so I'll I'll go over U how I do that as well so again prerequisites um uh typically you know we say hey you should be familiar with kubernetes get Ops Argo CD uh we're going to be using the acut platform but we're going to have a test instance of that um and so uh if you're gonna if you're G to you know again this is a public web website if you're going to go home and try it out um you know I I've done this on kind locally using Dr desktop it works just fine so um so uh first step right so let's uh set up your environment so um there's a link here it says click this link and then use this template um from this repo right and so um I'm going to do that so open this link it'll uh let me make this a little bigger so it'll uh it'll it'll use this template once you click that link it'll take you through this page um choose owner I'm going to choose myself um repository name is go back to the the instructions it tells you explicitly to use this name so I'm going to copy and paste this um and again as uh Nick who's not here couldn't make it but this is important because the tutorial assumes that you'll be using this name so it's going to be assumption right and so um enter that as a repository name and then uh click create repository this will create the repo basically it um um uh it it takes that template and creates this repo for you this will take a little bit um so I'll kind of pause here for a little bit so where we are in the instructions is we are in the uh set up your environment uh we click this link to use the template make sure to name it this um and then we create a that will create a repository for you so you should be here so it should see here um from your repo uh we we we'll we'll go into that a little later um and uh we're going to be using this repo and we're going to be using Code spaces from this repo so this part actually takes a little bit um it'll actually give an opportunity for everyone to catch up because it takes a lot a while to do but basically um you're going to uh click the green code button and you're going to use um create code space on Main all right so I kind of give you the screenshot and it'll start building the code space for you so if you click on code little down space it it might have you here just make sure uh to click the tab where it says code spaces that says create codes space on Main again if you're on a free tier you and you already have a code space you may have to shut down um uh to to be able to use another code space but click on create codes space with main this will open up a new tab and um it'll uh it'll take a little while to load I think it took uh about two minutes when I did this at home so just like on my on my own Wi-Fi so I don't know on conference Wi-Fi it may take a little bit longer um but it'll take a while so um you know kind of leave that up and r running this is kind of where we are I um this will give a chance for uh for everyone to catch up if they're still having issues so is it a real slow okay yeah so this this will this will take a minute so if you're having uh issues or if you need a a bio break this will take a while scratch it um it doesn't use a pre-build it basically we take the uh the Microsoft uh two image and basically run um essentially from scratch so it'll it'll build yeah yes okay okay oh really okay yeah yeah I'm also I'm actually doing a workshop at qon in Paris I fly out like in a couple days about Argo Argo rollout so I'm actually doing another Workshop our that's so um when you get to this page um it actually runs a post like install script so actually give it a minute even even when you reach this page there's going to be yet another script that runs so just give it a little bit more time I want to make this a little bigger because because this is trying to see so yeah the issue like um it says setting up remote connection I think it's the connection from here to there right there's stuff happening but it hasn't reached oh really oh interesting all Thea okay oh that's interesting yeah cuz you do like plus you know workflow yeah M now it makes sense why they give us three hours for this Workshop Because by the time things load or the Wii same yeah I never I always pre-record my demos because just cuz rate limiting yeah yeah yeah yeah exactly that yeah open with codes space yeah yeah so from there here uh a question for those who are catching up um it says um create code space on Main so when you click the little code thing make sure on the code spaces Tab and they click create and then you'll see this screen and then um which then should bring up this screen there's also post running Scripts yeah I didn't see any other hands raised I think everyone's oh there's someone there um has a question but I think everyone's experiencing the same was yeah yeah' we also oh really oh okay yeah yeah we actually so usually we run this like locally we tell people run kind and blah blah blah that just was a nightmare and we we recently switched it to Cod spaces so like we just like did this last like last minute literally like two weeks ago like let's use cod spaces and like it worked we like okay GCR whatever we EAS do that yeah yeah simple action I have to so for those of you who just walked in I recommend taking out a camera taking a picture of this workshops instructions Workshop platform on your laptop if you want to uh follow along we're not too far along so um we're just waiting for everyone's code space to come up and running so oh look at that yeah don't close this tab this tab is getting it back up is going to be a nightmare so it yeah so even I think even up here it actually just runs a little script I can I I can check okay cool yeah oh yeah yeah let me put put that back up um people are still catching up so it's cool a long time yeah yeah we have a good I haven't actually checked my email because you know I'm doing this thing so oh go for for yeah that's if you have time yeah I mean or towards the end or overrides and things like that yeah it's um I've used uh yeah I use I use application sets heavily just because it's just so so easy an advanced topic or just just one way of yeah one way of doing things I guess yeah it's templating it's you know so standardizing yeah standardization is always good here so I if any of you here at least has like a prompt a few cool all right um so there is a post um a post install script that runs and the most efficient way to check that if you do K space uh ge and hit tab if tab works that means the post tab completion Works um with K so usually I create an alias for K you can use Cube CTL as well um um but that is uh if you do a kind get clusters you should see Dev there this is very tiny let me make this a little bigger one second yeah so here um so should kind of look like this here um if you do K get clusters it should say Dev and then if you do uh cctl get pods uh- a you should see something like something like this right so I'll leave this up for a little bit um for those who are catching up so like if you have your Dev container up and running um you can run these commands if you can run these commands and you see something similar output that means you are um you are right on track and you're running um almost ready to start using Argo CD so it scale it up a little bit Yeah Yeah more scale okay so you see that oh you see them running okay cool this look like there's a few of you who are going ahead which is awesome that's fine um I'm a little uh slow to try to make sure everyone's kind of on on the same page so seen the terminal yeah that I I was there it this takes it takes a while there for there because there's some pre and post scripts that run that right so it it it'll take like a minute or two so oh it's scary I considered that too but we don't like refreshing the page yeah I mean the code space does stay up like I mean it you can connect to it later it's just um you know know you're deciding so at this point um we should be about here right you'll see this page uh you'll have your your code um the repo up and running um and you should have this up so uh again how many of you are here show hands at least here okay cool um I well I'll have another pause section for those who are trying to catch up um so once you're here here again um if you do K get clusters you should see Dev uh qctl get pods - capital A you should see um uh very very tiny kubernetes cluster uh cool all right again that's step 3.2 verify your environment make sure uh things are up and running right so now we now we're actually going to go to the platform uh QD platform um so here uh there's some info about actually doing it on the um uh on our actual platform we have a separate plat platform separate from our a production instance so you don't have to worry about that um but we're going to go through the similar steps right and so here I am logging in with Google uh you can do with uh with GitHub you can do it with email address um username password standard uh whatever you choose I'm going to do it with Google um and again I think a lot of you came here right again I struggle with you oh no yeah just trying to get there we go all right how many you have SE at least been here or are here cool a little bit all right um so what we're going to do is um we on the acut platform um first you have to create an organization um before you can you can do that um so there's a convenient link here please create or join our organization you click that um new organization I'm going to call mine again my GitHub username you call it Fu whatever um but I use my GitHub username just to um and then once that's created uh we're going to create an Argo City instance um here so's here organization yeah so now we're going to um so now we're about step seven right uh create Argo City click on Argo CD on the on the left hand side and click on create um here the instance name we're going to call it I believe the instructions say cluster manager so I'll just do that um we're going to use um uh you can choose your Argo CD version here I'm going to use uh 2.10 one right um version CU I believe that's the one I tested on and the little create button and this will go into progressing state right so that's up and running or going the process of up and running um this this will take Al again a little while to provision um and so what we're going to do is we're going to uh while it's while it's uh pending the installation what we're going to do is we're going to uh set up a user so we're going to go to settings we're going to uh on the so once you click on the settings tab uh you're going to scroll down on the left hand side there's going to be if it lets me there we go um the system accounts so under security and access on the left hand side there's the system accounts oops I clicked on the wrong one system accounts and then uh we're going to enable the admin account click confirm uh set password uh I'm going to uh just generate a password so I'll do that again because I did that pretty quick click on set password you can either set your own username and password that's totally fine or you can just generate one um click on generate actually sets the password randomly and you click copy and then close it that actually sets the password to some random um uh this random string of characters right and so if you ever you lose your password or forget it you just come back here and just reset it um and so uh yeah again regenerate password bottom right say password hit close and then it's going to ask you to uh log into Argo CD so it might not be up yet not not yet okay yeah so um my Gear's little spinning yours probably is two I'm going to wait one gives everyone a chance to catch up and two gives a chance to um uh for my my instance to be up and running so it might take a little while yeah so if everyone's hitting it at once yeah it might take a little bit uh for this to run here it actually does give you it what's that oh question yeah so the version I selected was uh 2.10.1 so that's the one um as long as it's 2.10 it should be fine that's that's uh where the lab was WR written for 2.10 there's there's really no um there's there's a few differences but it's mainly like uh security patches and things like that yeah specific acity specific features yeah so usually like um there are features there that we're pushing up stream that we release first on our platform um so they'll be on the platform first for our customers before they actually get pushed Upstream so not everything no no so whiches we have the different builds Yeah question [Music] yeah so the um so both ways on the um run on the acity platform so we host it for you whether you use the Acuity version or whether you use the the the uh the Upstream version whe and also both for both we use an agent ours is an agent based design so we use an agent to communicate um back to our platform that which is the reason why using kind works is because you don't need to open up API right so there's like an agent running kind of to proxy the information but it's it's mainly the accy version is like cve patches or um you know little features that we're pushing Upstream that aren't haven't landed yet H authentication with what respect Oho integration yes yeah like authentication integration things like that yeah yeah you do you but you can manage it with uh with the platform but yeah so any of you have the green heart all right a few of you okay spinning yeah who has a gear still spinning anyone okay oh so someone over here does okay oh gears over there so once you get this little green heart there's this little um weird uuid type link here uh if you click that it'll drop you in to those who have used Argo Argo CD should be familiar with this UI it drops you in here uh username is admin password you just paste uh the generated password if it's stilling your copy buffer or if you set your own password go ahead and enter your password C click sign in and uh don't remember that um and you should see this right and this is Argo CD so Argo CD's UI um so yeah so now you have a fully okay so this is kind of where we are all right so uh you should have this repo right up in a tab you should have your Dev containers here up and running um with this the information here uh the Acuity portal right that we use to you know spin up and you know self-host Argo CD and uh we're using uh this is the Argo CDU so we should all be relatively here give it another few minutes for those uh still waiting on the gear or trying to catch up what is that I wonder what screen that is what's that oh you have what does it say no matching applications oh you might have a filter on actually I wonder I wonder if you have a filter on yours oh interesting okay I can still create one yeah yeah that's interesting I wonder why it dropped that dropped you on that page some of you are seeing different pages like with a little uh magnifying glass that should be okay that's just a different view for whatever reason I wonder like oops I didn't want to do that okay I messed up don't do what I did okay all right next thing we're going to do is uh oh CU yeah we're starting to run out of time um next thing we're going to do we're going to um we have this kind cluster running inside our Dev container and we're going to add an agent to it and basically it in Argo CD for you so the way to do that is on this page here on the platform um as you may have guessed it you need to go to your clusters tab so once you're on your clusters tab um click on connect cluster so uh this is very very important you need to call the cluster Dev it's basically the same name as the cluster name on kind um click on connect cluster and then this will drop this big old thing here that you just copy to the clipboard once you copy that to the clipboard go back to your Dev container and just paste it hello okay yes and it should kind of look like that right it'll it'll look like that it's some you know token blah blah blah um and it'll run those things again yes yeah now everyone is everyone is inside now yeah yeah so again click connect cluster uh type the uh make sure to call it Dev that's very important and then um uh and then it'll it'll once you do that it'll have this you copy that to the clipboard once you copy that to the clipboard paste it right you paste that it'll look kind of like that and it'll apply all these manifests um what I like to do is I like to run a watch so let's do uh watch cctl get pods DN Acuity with a K so once you run that command here enter and it should you should see the agent up and running and start installing things again um kind of like what I went through in the presentation um there is the application controller uh there's the redus server there's the repo server kind of the components um that I was talking about before that it's installing it in the uh on this um uh on this local cluster here um it does take a little bit to run um you should see oops let me close that um right now it's a a a you should see a question mark because it's trying to add running these containers here um once this is up and running it'll send that back to the platform to tell that it's healthy so that's why I like kind of like like to run a watch uh on here is a good indication so how's the platform holding up good all right H all right let's good better over than under I guess better ask for oh there's a there's a a question down there someone has a yeah just in case wow e how many of you have gotten here okay cool all right okay I'll leave this up though you inst the yeah the agent because yeah that's how it yeah exactly yeah con yeah mine says uh container creating so I I think it's an issue with code spaces yeah cuz so your agents connected yeah agents connected it's just no it's just waiting for all the the health of uh of this guy the controller the notifications controller ironically although this the an container there is taking a while for these for the repo server it's got to usually once the agent connects that'll kick in yeah we thought Cloud was going to solve everything right oh yes exactly yeah throw more computers at it yeah that's what I was trying to uh yeah oh it's a TLS search thing okay what's is uh uh what shows you all the uh what's well I was just trying to see it does show you all the the environment variables Ki delete it or roll out uh rest restart QD uh deployment of the which one application I need a dash controller okay yeah oh okay yeah I got you got you yeah uhuh well the notifications control I don't know what's wrong with the notification that'll come back later yeah but the the controller it's just slow it looks like what's that oh the ID is uh was that lk7 I L and go uh let me do the watch yeah so it's I think it's the K get on yes confence yeah uh yeah I think it's just it's just that just the Wi-Fi it's just very it's just slow yeah yeah I mean really it's yeah I mean it's understandable yeah mhm it is a good problem though yeah like yeah see be ready for yeah okay see okay yeah all right thank you I appreciate it yeah fil to watch config map interesting maybe I think I'm thinking yeah I'm I'm G to try something that drastic here not deployments get delete oh the Sinker pod the side car yeah gotcha I heard someone got a further further head than me take it just might take time yeah I'm a little impatient looks like also the side car notifies the agent and the agent pulls everything in all the gotcha m mhm well we don't have 80 people oh really oh yeah uh oh the Sinker um application controller and then- C Sinker right I just says uh has prevented get Secrets Factor failed to watch Secrets yeah client go yeah I think it's maybe try to the rate limit reaching out from code spaces yeah it's that Sinker pod it's just just vomiting errors yeah no errors on there yeah go go go go has anyone gotten Beyond this no one person to okay I I was a turtle nowb yeah now you're the rabbit yeah you were the turtle now you just passed me what's that are they oh you see them there okay again um if you if you are past this feel free to keep going um you're one of the lucky ones feel free to keep going with the lab um the instructions are right there so I'll try to catch up so when I first tested this this I actually ran into this yeah and I think what was happening is trying to download things that were rate limiting me like trying to download at that time for whatever reason I did do something what I did is I deleted the codes space and restarted it but like that'll take forever here I'm assuming it is I'm assuming it's like a rate limiting thing um on that think that's the problem yeah so um yeah there's a side car that tries to download a lot a lot of things and inject that in I think it's just taking a while to download those tlss yeah once the agent connected it connects to the contr they syn up alligations are missing so yeah so yeah the TLs is missing because it hasn't downloaded it yet so is there any we cance it I think time yeah it's just it's just going to take some time separ yeah so like that that is that that's just taking time that's the step we're in right now yeah information to connect to the agent so when the agent connected goes the contrs trations server is it like projects else uh no those are all H posted on the control plane but with our agent based model and architecture yeah basically when you have you Gall challenges and you have hundreds of clusters yep you have an application controller in each cluster so the works distri out so you don't have one application control crunching for hundreds of clusters gets it okay so it's much more yeah that's what I was thinking well I mean it's supposed to be every two seconds right but what yours is yours running at least or yeah that's a weird error though m really oops wait really oh does it does it use the agent to download the configurations or does it go but it's through the agent right is is or is it does it go out on its own oh okay all right yeah go go up and do edit as yaml yeah Dev okay yeah right then that's right yeah that looks right to your yeah for for yeah I don't think this will ever kick over to be honest with you the AG I just deleted and reinstalled the agent so we'll see what happens so yeah so I don't know if I made it worse or better but we'll see this is the first one doing it this way yes we thought it was going to be better yeah I mean you know to be honest when I first years ago Le go down so often so oh so it's it's yeah we did another a different Workshop because we have an advanced Workshop um and cod spaces worked fine there oh okay yeah yes yes that's the one yes yeah that that the advanced workshop and we did did and we do that with Cod spaces and that works fine but k3d yeah we were actually thinking about switching um something lighter weight right for that connection EST I was just thinking of that yeah is that what you're doing k3d yeah does it work oh it's just the Wi-Fi yeah I'm sep yeah it sounds it looks we don't know there's something weird because yeah because we're using Code spaces and like that's separate from this Wi-Fi right that's it's hosted over there put straight on the Wii so it's it's combination of both but yeah depending on where they are like in the beginning it was the Wi-Fi but now it seems to be like we're being some rate limiting something weird with code spaces yeah yeah yes the what yeah yeah we yeah his went quicker yeah yeah still doing container creating okay since we're stuck what I'm going to do is uh yeah still same error what I'm gonna do I mean since running out of time anyway um yeah because it's almost four right we're here to five okay cool I'm gonna try something drastic yeah we'll see yeah bold exactly right I'm just going to delete it maybe it'll be better maybe it'll be better if I do it when no one else is doing yeah about 85 yeah well what also what I think well because we're all on the same Wi-Fi I think like it all coming you know since it's nated it's coming from the same IP so maybe it's some rate limiting some weird like that I don't know but yeah but once like once you're on it oh it doesn't really support IPv6 oh interesting do tra as of February 72% okay2 gole got2 sounds about right I think most people are waiting I think yeah oh me switch to set and just kind of just show the yeah it might we might be better off switching to a demo hold on yes still still running to the same thing yeah uh still has that same issue oh really yeah in gke yeah oh that's a platform problem um he's running this he's running in gke say getting the same same issue yeah yeah f same Rel what's that doing good I use work when you create an application that creates resource yeah there's no resource or application resource in the down cluster yes correct yeah so it gets created on the hosted um a city platform yeah is there way to look at it is oh oh is there like to do like um get like uh like uh cubsy tail get apps blah blah blah um no there isn't there isn't a wait no oh yeah yeah no yeah we we have a uh no no that's that's one we us now what's that which verion did you go version um I went with uh 210 2101 but I think but AK yeah but the AK the latest AK one default one yeah the default one yeah a yeah I went with a different one right now I'm just testing to see I think this is oh really oh interesting yeah so I used uh the platform and the agent installed just fine yeah so it's running are you I mean you try to live live upgrade okay so what I ended up doing um if if you'd like you can follow along but what I ended up doing is I went to ac. Cloud that is our production instance right and so um I'm going to be doing essentially switching to a um a demo versus a workshop because um it looks like our training instance there's an issue with the training instance so it's not we we we first we thought it were it was a code spaces they were rate limiting us something like that uh we're running some tests it looks like it is the training instance that we set up looks like there's some issues there um so what I'm going to do is um I'm going to kind of like switch into a uh more of a demo right um so what um so where we left off or what I I left off is I installed the agent which is now up and running um it has the Argo City application uh controller uh the reddis the repo server all the stuff that I was uh what I was explaining to before if you can remember that long ago um and so um which is around here right and so um um so what uh what this goes through and again if if you want to run this um um you know back at home or when you're or maybe on a more stable Wi-Fi with kind or whatever uh uh you know this will be up um and what we're going to do is create an application in Argo CD this is going to be a Helm application um we're going to do uh um we're going to use the repo that we're in so uh for example uh let me down here uh what we're going to do is we have under apps we have the guestbook uh application let me make this a little smaller there we go um and uh so basically an Argo City application um takes a source and um which is uh this the repo that we are working on and the destination right and again we assume you're using Dev as the name which is why we we we say that that's important um here this is pointed to uh say this repo and in this path so you can actually see that here if I click on that um you can see that it is a Helm chart that is uh stored inside of Argo um stored inside of a git repo but you can actually point it directly to a a Helm repo um you don't have to in install it on git uh basically this is the chart values uh file that's all included in the uh uh in the repo itself so um again switching back to apps when we go here um if I click on settings and I click on clusters it it shows that I did add that Dev cluster there um I can see that here as well I can click on new app uh I can fill this in uh using the wizard uh here it'll ask you all these questions but you can actually edit as yaml I could take all this information here right enter it there um click save and click create this will actually um what's Happening Here is um the repo server actually went out and did a a get clone right from um from the repo uh against head and um is looks in the path and it says okay you want this work Lo running on this cluster but is not only are you out of sync but it's missing you don't actually have anything there so if I actually click on this card um it'll tell you that you know you want a service and you want a deployment and again it says the resource is not found um and uh if we go back to this uh what is the key combination to get the there it is okay uh so let's go back up here if I do okay G pods um right it's just the agent uh the Acuity agent and the uh default kubernetes uh so easiest way to click Sync here let synchronize that um so what it's doing now is that the application server takes um uh the information from the repo server says okay I'm going to apply this information to my kubernetes cluster um and if I look back here if I do okay get pods you see oops it's actually you'll see it actually a little easier um you can see that it's actually um deploying uh this Helm chart to uh the cluster right and so um it'll give you kind of like an overview here let make this a little smaller so you see it gives you the um application status right it's healthy healthy that health information actually comes from kubernetes right it's taking look at the um uh the health information that kubernetes bubbles up and displays that it says a synct meaning that your desired State um and your running state are the same the last sync so the last thing that happened is that it was successful right has information about uh the G um uh get commit right information about like who authored it all that stuff here um I can drop into um here I can look at the logs from uh from the UI here uh I can look at uh you know my endpoint size uh service all that information I can see that from an overview here so um from the UI so this is kind of like the idea uh that Inuit had when they first created this like yeah we want a platform that I'm able to deploy my workloads and manage them without actually you know having to type the uh um kuber nettic Command right I did all this from the UI um you know after after I have it set up so um and you'll see this here one thing I I like um uh I do like to uh call out is that if I do I don't think I yeah I have Helm installed if I do Helm uh LS right there's nothing there right you're like whoa you installed a Helm chart but Helm isn't there right and so those of you who aren not familiar with Argo CD Argo CD um Alex actually one of our Co Founders um he says that uh Argo CD is a glorified Cube CTL meaning that I want to manage kubernetes with raw kubernetes files meaning that um that whether you use customize or Helm what Argo CD does is it renders those manifests and then it applies them long way of saying Argo C does a Helm template pipes to cctl apply so that's why you don't see that here um and that's a philosophical um Choice um is Argo CD wants to work with raw manifest so if you're working with Argo CD there's a few kind of advanced users here but those that are haven't been using them or thinking about using it just keep that in mind um especially if you're a heavy Helm shop um Aro C is going to render th those for you um if you want them to use the helm thing there's things like uh config management plugins and things like that you can make Argo CD uh kind of do that but uh the idea the original creators is like we just want to work with raw kubernetes manifest so little Sidetrack there little information about that there you have a question yeah yeah yeah so with Helm file yeah um actually with with um you can do a lot more with Helm file um if you're going to use Helm file you actually have to tell Argo C to use Helm file it doesn't know how to use Helm file so Argo C you have to use something called the CMP which is stands for config management plugin it'll handle Helm natively it'll handle customize it'll hand natively it'll even do jonet natively um but that's it like if you need anything else like if you're using um uh what are these some of these other things like there's one from grafana that does it and there's another one tanomi tanami whatever there's other ones right that um that that is used to render kubernetes manifest you have to tell Argo City how to do that with a config management plugin so if you want to look that up config management plug-in Argo CD you do plug that into your uh ducko or whatever Google um um that's kind of like the information you're going to need there so all right so now changes right so syncing changes manually so let's look at the um uh uh instead of running Helm upgrade guestbook blah blah blah BL blah right you actually since this is get Ops and get Ops tool what we want to do is we want to make the change and get right and so we're going to change the image tag so let's uh again go back down here and we're going to go to the values file and I'm going to change this image tag to uh let's go 2.0 right WR and quit that uh I do a get ad um don't do it this way but I'll do it anyway uh get add uh get [Music] commit know something like uh changed image tag I get a push origin main okay cool um so going back here let's do um so the um reconciliation Loop is about every 3 minutes for Argo City completely tunable right some people like it longer some people like it shorter defaults 3 minutes um but instead of sitting here waiting three minutes I figured you guys waited long enough um I'm going to click on refresh what refresh does is that it tells the repo silver hey go check to see if there's anything um anything new right basically get LS remote look at the hash see if there's a difference um and if it is it'll it'll download the difference right um and uh and there's a hard refresh right I I'll you know I'll I'll give you a little bit more information hard refresh there's differ hard refresh basically does an RM of wherever it get cloned and then it doesn't get clone again whereas refresh does a get get LS remote blah blah blah right just to kind of just to see if there's a fresh copy so that's that's the difference there as you see here it says your app is healthy again kubernetes you're happy be know it may be up and running um but my desired State changed my and it doesn't match the running state says out of sync so um by default Argo CD doesn't set up Autos syncing none of that right it leaves it up to you um so in order to get this in sync we click um sync synchronize um there's different ways to do synchronization right there's partial syncs there's you know you can say only sync things that are out of um out of sync right there's certain ways to optimize some of this but by default it just reapplies everything all at once and as you see here uh replica set changed because the Pod um updated to a different image um let me make actually let me show you something again we're going off script since um uh the platform wasn't working for you guys uh I wonder if there's another oh let's change the replica count right let's do to uh there should be um if anyone knows a shortcut uh to just push it real quick oops and then I just kind of want to show uh refresh you can do refresh or wait three minutes or do yeah yeah exactly or use a web hug again you know I was talking about get Ops it's like you're you're going to use web hooks because sometimes you want the change to happen immediately after you make a a get a commit right um and you'll set up a web hook for a city's um API to be like check now versus waiting three minutes right um but if you go click here in the UI it gives you more information but what's really cool is that it shows you the live manifests right which is all the you know yamal garbage um click on the desired right again that's the desired State uh if you click on diff it'll show you the diff um so it's which is like it'll show you the inline diff which is right here um better way to do that there there's a compact diff that kind of just shows you you know instead of the whole thing so it'll actually show you what what that uh differences is right so that's the something I forgot to to show you um so um let me and the next part is blah blah blah oh yeah uh autosync and S heel so this is um the Autos sync where is it here yeah so um again I said by default Argo CD just kind of just by default sets It Up This Way meaning like if there's a change or anything it'll just notify you it doesn't really do anything um you have to go go in and do that yourself uh if you click on details so let me go here on guestbook um oops details uh down here we can click enable auto sync so it says are you going to enable autosync yes I am um self-heal enable that yes so um so what self-heal does is essentially is like as soon as it detects if it um as soon as it detects a change meaning like once the three minutes runs and it detects a change it'll automatically apply those meaning that it'll um uh it'll do that for you without you having to click that that sync button um prune resources um I always enable this um but be careful with it meaning that like if someone's at um it'll if it if some this is like true get Ops right if something's in on running on the cluster that is not in git it'll delete those resources you may or may not want that um so that's what prune resources is meaning basically exactly how it is in get I want that exact you know even even if it means deleting things right so that's that's what prune resources means um I always enable it um uh self heal is basically it'll run the sync um but by default it won't prune resources I always just enable it so so this here um actually I think I need to run a patch uh you're seeing some behind the scenes things so one second uh where is my patch where do we put the patch uh there it is um in order for you to see this working so um this is not um this is not um this is something specific to the QD platform we made some optimizations um and uh I want I revert some of those just so you can see right um so then let's go here uh in the lab auto sync cool all right so let's do um another change so let's go to values this replica count to three add commit push okay again we're not going to wait the three minutes what I'm going to do is I'm going to click refresh but this time as you see when as soon as I click refresh it notices the change and it automatically scales that up I have three pods now um so that I basically didn't have to like sync or do any web hooks or anything like that as soon as it detected that change it'll sync those for you um and then then um I can even go here and then uh delete this deployment let cross our fingers so as soon as I delete that deployment Argo City says whoa whoa whoa whoa the the you know you you told me you need wanted a deployment here your desired state says that someone deleted this I'm going to redeploy it and so um you know and I can do that with any resource right the service here uh gu book delete that service Argo CD says no like out of band changes and that um and that's what autosync enabling autosync gets uh gets you with Argo CD so um so we did that and then um okay so this is kind of a um so now you see how Argo City does um does those things with kubernetes resources what's also really cool about Argo CD is that um it it does these with kubernetes resources but Argo CD applications the application manifests itself is actually a kubernetes resource as well and so a lot of um uh practitioners came up with something called an app of apps so it's an all that is is an Argo City application that deploys other Argo CD applications um and uh the reason for that is like originally was for bootstrapping um bootstrapping you know I can just do AR you know cctl apply my parent application and all my all the children basically all the other applications get deployed that way um including Argo City managing itself and all this other resources so basically you kind of get a convenient bootstrapping um out of the box with Argo CD uh you can do other things like um someone mentioned like syn waves and things like that ordering you can order things that way um as well so kind of do application dependencies actually if you follow acity on um on Twitter or follow me on Twitter I reposted I actually have a Blog that just came out today about application dependencies using Argo CD um and so um so we'll so that's what kind of what the app of apps um is right and so if you think if you look at it here um there's an app of apps definition what's really cool about app apps a definition is there's nothing special about it um it's another Argo City there's just another Argo City application right it says um you know this is the uh this is the repo URL the one we're using and the path is apps if I look under apps here um in this directory you see the guestbook app that we're just using and we there's another app called portal also that is there as well so basically Argo City application that's going to deploy these two applications so if I go up here with app of app s and then um again if I just do a new app edit as yl and then uh I click save here and actually let's do a let's enable auto sync where is that I think it's down what's that yeah Auto auto sync for uh for the for the Apple where is the actually there we go sync policy automatic self-heal again pre resources just this how I am so um so yeah you notice the uh uh this bootstrap right that's the name of the app or the parent application and that deploys guestbook that's already deployed um um so it didn't have to do anything thing right there's no there's no diff right the desire State and the running State there was nothing so I hadn't had to do anything but then there's then it created this other application um called portal right this one isn't set up for autosync and um so I have to uh oops sync this manually so then that uh deploys that um as well so um so what's really cool about this is let's just delete guestbook just delete the application you notice the bootstrap app um noticed that it was missing if I go here it noticed that other application was missing and so um so let's do uh let's sync that and then you know magically guestbook comes back um so like you can you know you can see that if I delete this app guestbook delete that app that's gone if I sync the it'll so that's say you can kind of see that like how um you can use Argo CD to kind of manage other Argo CD components having Argo CD manage itself um by using uh something we call um app of apps pattern right and so that is um uh uh what um most people used it for at there's a question yeah so deploy uh changes to the apps individually meaning yeah yeah so you can you can still uh uh change things in the guest guest book individually right so the app of apps all it does is just makes sure um your application definitions are there you can still manage those other ones individually um there's um there's certain things that it doesn't um doesn't allow you to do right so like if I do um there's certain caveats right did I uh this okay cool um so that's Auto things so like if I go to guestbook here and if I do enable autosync um and then enable self-heal um you know let's just enable all of this here since I have an app of apps running once if I uh sync this and I go back to guestbook um these things should be oh wait I think I disabled that I think I did yeah okay normally what would happen I actually wanted to error out and it didn't the one time I wanted to error out is because because I didn't set up autosync here if I set it up in the UI the um the parent app will overwrite that because there that's that's now that now my applications instead of being defined in the UI they're defined in git um really that's really the only thing you need to look out for it's like oh the UI isn't doing things it's like it doesn't write back to get it just does whatever it get does so um if you're managing things with app of apps you you really have to go really all in on get Ops because it's like now you have to manage now you have to think well now I'm managing things via G and the UI mainly becomes like a visualization observation type of thing um and mentioning that ironically or I guess um on on that same note um Argo CD ranks number three in observability tools which is really funny because it's like they didn't really build it as an observability tool but it does rank number three like number three or number four in the cncf of observability tools only because um you know once everyone's all into get Ops and managing these in get op and get it actually does become an you know on more of observation readon thing than anything else so um so as far as um the workshop that's the that's the workshop uh in its entirety uh we still have some time if those who are still trying to get it up and running um you know uh Justin is trying to get the training instance um back up but is there um are there any other lingering questions look I saw someone's hand up there yeah you is oh the reconciliation Loop the defaults is three minutes three minutes yeah and that's and you can you know again adjust that for whatever makes sense to you some people turn off um there's something called uh and you just use web hooks which is fine it's not get Ops but I mean not everyone's going to adopt that so that makes sense um Argo City is flexible enough there's also uh what we call sync Windows meaning like oh I only want the sync People to be able to whether it be people apis robots whatever to be able to sync certain amount um in a certain amount of time right like off hours right like I don't want a sync to happen during on hours and off hours but that's yeah so that's completely adjust ible it's 3 minutes but I set it to 5 seconds for the um for the demo yeah any other questions there was right here glasses yeah um yes for Argo CD but that's um uh uh you kind of have to hun hunt for that a little bit um if you're using Acuity there we do keep a uh uh a history of oh this is going to take a little while to about up but we do keep a history of what um if you're using the platform but that this data comes from Argo CD itself you just kind of have to look in the logs in and for things like that yeah oh the yeah that yes yeah yeah so there's yes I don't know how to read yeah yeah there is there's like a grafana there's something you can enable and point it to like a grafana dashboard or something so that way it it'll it'll populate that uh for you yeah umor service monitor is is what it's called yeah actually I should probably install some of these plugins so you can see yeah okay yeah uh any other questions I saw you in the back yeah yeah so uh the question was about application dependencies if they're already up and running um and you want to do an upgrade um how can um you know can you make them dependent on each other so um and I again Shameless plug I did write about this in my blog but there is um methods to uh there's no native way to say this application is dependent on this application in Argo City there's no native way um so one of the ways but you you can still do it one of the ways with Argo CD is to use um sync phases and sync waves right so sync phases or sync hooks is like you can have like a pre sync tasks you can have the sync task which is what we're watching now and a post sync task um so you and and they go in that order right prync sync post sync on top of that you can do sync waves meaning like with in each individual um um hook or phase so let's say like you can say you know you can annotate uh your resource with saying you know one two 3 four meaning like basically lower level and you could do actually do negative numbers but lower means higher priority so um and usually people do that like with like crds right like I want to deploy a crd first then the CR and then you know other things or I want the storage to be available before blah blah blah right so that's kind of like the background because then since you're using um app of apps right uh you can have dependencies that way as well because um you can annotate your applications right so you have the parent app controlling you know um and it'll honor sync wave so you can say all right database gets sync wave of one my back end gets a sync wave of two and uh my front end gets a sync wave of three so when I sync the parent app it'll respect that it'll basically deploy it'll sync the first one wait until it's fully synced and then it'll move on to the next one so that's kind of how you get it from uh using the app of apps so that's only possible with app of apps yeah so application set um and again like from a high level application sets are uh a templating engine so um application sets uh a stable version no so in application sets there is an alpha feature called Progressive syns that um that does what what you're describing there's a there's like there's a big Capital C caveats because there's like things that happen like it turns off it disables autosync for you like even if you have it enabled it'll just disable it um because the it relies on the application set controllers logic to do the sync it it it won't it won't uh it won't let you do like auto sync or anything like that because it'll just rely it on the on the upper ones but you can use Progressive syncs I've used Progressive syncs um and um you know uh you know between us and I guess whoever's watching the recording I know in does use it in production internally um but it's still technically Alpha I've actually brought this up in the in in the community meetings say like no one's complaining about this feature so either is no one's using it or they're using it there's no problems we should probably move it on to like beta but it is still Alpha so that's really the the you know no the um the the progressive sync logic will will will sync them for you in that regular interval so so you would have to uh you know either wait the 3 minutes or use a web hook if something happens all right any other questions Yeah question y people are people are still using both and just from like my experience well one my opinion and because of the experiences I've been using with uh with with our customers is that you're going to be using both for different things and you might be using them together so um I know um yeah Arts unless there are just different ways of doing it alls yeah so the yeah the um I've seen folks use like app of app sets so like they have like a parent app kind of like the bootstrap here and that deploys application sets right and then I've seen the reverse where I've seen application like the application sets all it's doing is deploying other Argo CD apps I you know i' I've seen I've seen that and yeah um well for the one I'm thinking about is like there's a project called the giops bridge is that basically they're they they want to be able to deploy um application sets massively because there's going to be an application set um per like environment for example and they just needed a way to deploy it using one manifest right so they'll use an app of app sets um or is it more like an application set for repos yeah application repository or like there just certain boundaries right because like um you know there there are places that are still kind of like siloed or they want to build like these um these gating you know type type of things where um yeah like I have an application set but I don't want like any like I this is my Dev application set and for all my Dev clusters and this is my um you know test and this is my PR just because they want that separation and they may have different repos for different things so um you know there's there there really is different use cases right there's like I mean yeah and and there is and and people do use do do that I mean I I would do that um I recommend that um it's still kind of hard to get people's mindset changed away from like because there's like a G workflows right so like all the all the separation of everything happens in get right your repositories like no one has access to anything right like at you know even to at some point even the administrators really don't have access to a lot of the the stuff and they use get as the interface but that's like people who are really far into it there's still some people that need and there's some regulations right there's still some compliance things where like I need these separation of yeah there's like yeah certain certain things there might be just like compliance right like hippoc compliance and you know governments yeah yeah yeah there's there's you're exposed to you don't think structure based on that correct yeah yeah there's little nuances in you know from every industry and you know people you know how you how you operate like something like in you know medical right like hippoc comps are very very strict banking right like banking like Swift yeah banking Swift like all like there's very there's very different regulations to where even though you can do it like in one repo with one application set there may be regulations that won't let them that's another another way so it's a combination of all those things yeah all right any other questions y oh the acity audit log um yeah I did um I might I may have to like generate like some things right right I I need to like generate some noise yeah so this is uh a plugin specific to the platform um so when you look at your applications like let's say bootstrap and then I say I I click on the information there's new tabs now uh audit logs right like who did what when Bas user B based on the user that that you're loged so if I this was like connected to SSO I actually I actually don't know if the exact if it Capt I don't know if it captures that actually um and the sync history like who did what when when you know kind of selladora is metrics um and like if I was logged in like with SSO instead of saying admin it would have like my username right yeah so this is uh specific for this like if you have multiple like because with a QED you can have multiple Aro City instances um and then it gives the the which one application metrics yeah replace it that I don't know I don't know what application metrics is do you it's not a replacement you still have toia stack inst but you were to install that it's just an addon you get the yeah n yeah natively here like in in in the UI and then there's like the AI assistant just because everyone so if I go go like if I go here and I say you know again you know why is this missing um who knows I actually haven't didn't prepare in the health field in the case application is missing from the target environment it means blah blah blah blah um we actually do have a customer that's saying that this um this has reduced his calls from his developers so U for what it's worth I mean I don't know if about the AI train I mean it this is actually is pretty useful because like I mean I know when I managed systems I used to have devs that needed complete handholding and devs that I never heard from so um so I understand the noisy Dev developer thing and this actually has helped that customer so it it is it is it is kind of cool um to have some of these plugins from the the QD platform so cool any other info yeah it's an AI plugin yeah yeah yeah more educational or like more kind of like uh yeah or or like kind of just like you know instead of picking up phone calling the IT guy the the developers that kind of like are using the the AI assistant um to help them debug some of some of this triage thing so and I think it um it uses I know it doesn't use they use publicly available data from like the kubernetes logs that people have posted right so it that actually doesn't um use customer data so it doesn't like learn from your right it it just kind of uses publicly available um information so kind of stuff that you should already know but like there's there's people that don't know right so I said here no yeah no I know um we have uh yeah well a lot of us are flying to cucon so I know I'm flying Saturday so um a lot of the team is there so cool yeah any other questions comments yeah yeah so um that's uh going to the application set so the application Set uh comes with a lot of generators right so when we talked about um and I may actually pull up the the docs uh Aro CD uh cd. read the docs okay um let make this here if I can we were actually just talking about how we hate our docs but uh operator manual application set so application Set uh as I explained before was is basically a templating agent right like it just basically takes input and then it turns it into an Argo City application so um the templating a um engine is based on things called generators meaning like how do I generate this template right and there's different uh generators um first um there's like a list generator right which is basically basic key value pairs right Fu equals bar blah blah blah right um and you can use that to build your Argo application um other other's a cluster generator meaning like based on cluster information I want to build my application that way right um for example it says like uh where is it yeah the secret here it is based on the on the cluster secret right so I can say based on um uh if you are a cluster named Fu I want you to deploy this application if you're a cluster named bar deploy this application you can any labels that you you add right um you know environment equals Dev right any cluster that's labeled that way is going to get that workload um so those are there's and there's a lot of those generators let me see if I can find yeah uh there's a git generator matx generator but there's also a pull request generator right which is what what you're asking about so um since you're you U since we're doing um so we're doing get Ops right in order to see something in your environment you have to merge merge your pull request a lot of people don't want to do that right A lot of people say well I want to see how it looks like before I merge this uh and that's what the pull request generator is is basically looks at your um poll requests and saying anytime um uh where is there's a good oh there we go anytime someone has a poll request and labels it preview I'm going to deploy that into an environment for you um so uh with Argo CD you would use an application Set uh and you use the pull request generator to say okay so anytime there's a pull request on this repo that is labeled a certain way I'm going to deploy that workload um soon as you merge it that pull request goes away and Argo City will delete that environment for you um so that's kind of like how there's a lot of nuances on how it works but basically that's that's from a high level that's how it works and that's how people are kind of like previewing environments yeah all right any other questions comments so again sorry about the platform the platform it was actually ended up being a platform issue we discovered we just discovered it too late uh we burned a lot of time trying to debug um again the this um you can do uh this Workshop um if you have the link um this scenario is up and running um and then we have uh we have our um our production platform if you want to you know sign up test it out kick the tires a little bit so all right thank you ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "PNILXc3YDPQ": {"video_title": "Workshop:  Hands-on Kubernetes", "video_description": "Talk by Tammer Saleh, Maziar Tamadon\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/workshop-hands-kubernetes\n\nKubernetes is a transformative tool, but its Lego-brick approach and vast tooling landscape bring massive complexity. This hands-on workshop will guide you through the golden path of running applications on Kubernetes and then explore the complexities needed to productionalize those workloads. You\u2019ll learn not only what Kubernetes is, but also when it\u2019s appropriate, and when it can be a velocity-destroying distraction for your team.", "transcript": "so like I said first we're going to be talking about Docker now something that I find really interesting is that people who learn about containers think that this is like oh it's this new Innovative technology and it's like changed the world I mean in some ways it did right but something to understand is that there's been a long history of containerization throughout at least my experience as an operations person uh Docker is just a new and standardized implementation of containers so let's talk way back in the 1960s when munix added the user account this was actually the first step of process isolation within Unix right now we've got the hardware the kernel separate processes all running under different users later we added we added chero who here's used chero I mean come on this is scale there's could to be a lot of people you gotta be my people right all right thank you you can put your hand down now sir um now isolated the file system it was very bad right it didn't work very well FreeBSD added jails which were even better they they isolated even more things who here has ever used Solaris zones yeah yeah right yeah zones were better than Dockers zones were amazing I loved them they're dead uh so in 2005 VMS became a Resurgence right uh it it was actually a technology that existed before but it became commoditized around 2005 so now you've got this multi-tiered cake you've got the hardware now you got a VM hypervisor running virtual machines each one's running the kernel is now running processes in 2007 is when Google developed cgroups and added that to the kernel right now this was the first time that the kernel had that level the Linux kernel had something close to solarus zones where it could do that level of isolation and then the Linux Community produced lxc which was their toolkit for running containers has anybody used lxc I've actually used it before it's pretty bad it's pretty painful right it's very much like a a set of small tools you have to kind of bind together yourself as opposed to a cohesive tool so now this cake this wedding cake is looking pretty tall right you've got the hardware the hypervisor the virtual machines the kernel now each one of these processes is actually running inside a container in 2013 Docker launched out of a failed startup called dot cloudcloud was a 12 Factor competitor to Heroku and these other platforms totally tanked and kind of like they hit the eject button and as this plane is going down a parachute emerged and this little tool called Docker kind of just floated down from it and it was Beed upon the industry and it really did change everything not not because it was the first thing to do process isolation but because it was the first thing to uh codify and kind of standardize what that process isolation toolkit looks like now one of the important things that you need to understand who here has used containers like in Anger okay only like like like 10% right right so sometimes we give this Workshop to like VPS and and Suits right and so this message for them is very surprising when we explain that the containers are not just microv VMS right the isolation that you get with a container is not perfect they uh they're very leaky and you should think of them in my mind I think of them as like Picket Fences as opposed to walls containers are a set of uh c groups and name spaces and uh and uh oh what just happened just lost the video yeah yeah a bunch of Kernel features working together am my back okay okay I won't touch it I won't touch it all right backing away slowly that give the impression of being in your own virtual machine and it's kind of funny because if you look at the history of containerization in the Linux kernel every time somebody's like oh that part's not actually isolated like that's the same thing that other processes other containers see the L Linux kernel team comes in be like oh yep that's a good one we'll just put a little a little pick a fence right there all right good and it's just it's just been growing over time right so when you look at a container you realize the container is sharing the same kernel I mean that's the obvious thing that is not ated between containers so the kernel and also that means all the modules that a kernel is loaded and obsessively all the features of that kernel one by one those features have been isolated into containers right so containers are not security but a better way to think of this is containers are like a a subset of Securities Part Of Your Balanced security diet they add a lot but you don't want to depend on them entirely on their own um running untrusted multi-tenant workloads in containers today is still a bad idea you open a kubernetes cluster to the big world and you say you get to run whatever you want and those containers are collocated on the same host and you're not using gvisor or firecracker or some other technology to to add even more isolation it's still a bad idea there are still exploits getting out of containers so given that Docker is not something brand new right Docker is just a a honing of previous Technologies and given that containers are this leaky abstraction in the container or in the kernel why do we even need to care about this containers are still transformative for application developers it means that uh you no longer have these dependency versioning issues right I was just talking about how python W the problem with python is its deployment story of virtual Ms and having to make sure that you got all the Right Packages ready for you in production and the exact same version is running you don't have that with containers right containers solve that um you can have one command that spins up dozens of micros services with like Docker compose locally for example you can containerize your own tools for portability we're seeing more and more teams adopt this pattern where as an operations person instead of having to write all my stuff as in as statically compiled go binaries where' the go the go guy left oh you're right there all right you moved um you can just containerize your your tools and you've got true Dev staging production parody which has always been the bane of my existence as an operations person and as an application developer like the works on my machine story sucks this solves it it also enables infrastructure as code in ways that you couldn't do before and for operators you also care about that Dev stage prod par you can standardize your deliverables you no longer have to worry about like a team adopting a new language or a new runtime and having the discussion with that team of okay how do we deploy your new fangled thing you can say I don't care what it is it's a container right as long as it it's a container and it listens on whatever Port I tell it everything's good and you can push that dependency management problem back onto the developers where it belongs I remember once a long time ago actually while I was in is is uh wonder if he's here while I was in Pasadena living here a long time ago I had a developer SSH into the prod server and upgrade Pearl by hand by patching one file right they're like well this is the only change that happened it'll be fine after I told them not to do it right that was a fantastic conversation but I understood his frustration he needed that Pearl version to be upgraded and I was just blocking him I was not adding value by slowing him down containers solves that and it also enables immutable deployments uh which is very who who here practices immutable infrastructure sort of yeah one one person over there wow what's your name sir Eric all right immutable infrastructure who here knows what it is about a third so IM mutable infrastructure says instead of having a fleet of servers that are all pets and every time you have to upgrade something you run like uh uh puppet or anible or chef and it just updates the little files in there and they kind of slowly accre over time instead you say it we're going to rebuild the VM from scratch every time so we have a golden image that gets kicked out at the end of our cicd Pipeline and we roll the entire fleet of servers I actually talked to an an administrator that did this nightly as a rule even if there wasn't of uh any changes to the servers at all just because they had stringent security requirements the problem is on AWS like uh burning a new golden Ami and rolling your Fleet of server servers can take hours with Docker it could take seconds and it unlocks kubernetes which we're going to get into next but it's not all Rosy it's a security nightmare when you have to worry about where heart bleed is actually active in your systems right you no longer have that same level of visibility into what's deployed on your servers it's a different set of tools for that um and remote vulnerabilities are happening everywhere right everyone's a devop this is the part that that pisses me off the most actually right uh application developers who are in charge of building their container images now have to understand all this black art Voodoo that I just happened to love right and that's wrong this is why this is my bias I came from cloud Foundry Heroku 12 Factor we're like you should not have to worry about any of that but kubernetes two steps forward one step back this is the compromise in order to to build a Docker container you need to understand not just Linux but like auntu or whatever distribution you're using and instead of having uh you know a hundred bare servers to manage or a thousand virtual machines to manage now you're managing tens of thousands of containers but there are tools that help with that really I just made this point because this this GIF is just adorable so it's still a better world than what we had before like I said before containers aren't new but docker's a lot more than just a step forward in containers Docker does three completely separate things and they're all very important they could all also be three separate tools but Docker was trying to be a company I mean they are a company they're actually a very successful company we just don't think of them anymore and so they kind of tried to bundle everything into this one tool so docker standardizes seriously Docker standardizes the packaging the running and the distribution of containers so how do I create a container image in the cloud this is how do I create an Ami it's the parallel to creating a container image how do I run those images in containers so how do I take that Ami and produce an ec2 IM uh instance running that Ami that's the analogy and even better how do I share them and there is no good analogy with Amazon or with the various clouds but with Docker I can very easily just start running engine X or any other third-party tool without having to like download and configure it and all that I just say run it because Docker also has Docker Hub so let's talk a little bit about the docker architecture Docker is actually a client server system when you're using Docker you don't necessarily think of it that way because you're usually doing it all local on your laptop or your server or whatever but Docker is client server so there is actually a Docker Damon that's running it's responsible for doing almost everything it's responsible for storing all the images for running the containers and monitoring the containers it's responsible for talking to the registry to either push your container image to the registry or pull down one that you've asked for and you you to drive the docker Damon through the docker CLI Docker pull Docker build Docker run those are the main ones there's a lot more so like I said the docker Damon is responsible for managing containers images volumes networks all that kind of stuff the talking to the Registries the command line tool the docker client is the thing that talks to the the the Damon through a restful interface and then you've got the registry which is there for uh it's basically the market place of containers that you can use so again that's the image now one thing to keep in mind is Docker D itself runs as root I like I want to say it has to I think when it was first developed they thought they had to in order to make this work since then theyve figured out some other teams have figured out you you don't actually need to run it as R we're not going to get into that but Docker by default runs as r all right um I'm gonna like half ignore what he just told me because he wants me to move away from the screen and and optimize for the video I'd rather optimize for you people here sitting here right now so Dockery runs his route which means and you'll see this in the lab if you give someone access to Docker you've effectively given them root on your machine it's a very important security fact I don't care what user they're running as but the moment you give them access to the docker command which is why when you install a Docker on auntu you actually have to explicitly add users who are allowed to run Docker to a special group because you've given them route on your server so like I said Docker comes in does three main things creation running and distribution which is Docker build Docker run Docker push we're going to talk about creation first uh so a container just make sure we're using the right par Lance here a container is a set of processes running with isolated process IDs isolated networks uh file systems all that isolation that's the process running with that isolation is the container the image is the set of directories and files that you use to create that container right the analogy in AWS is the image is the Ami and the container is the ec2 instance Docker build creates the image that's the thing that creates that image using what we've probably all seen before a Docker file so a Docker file is an atrocious format I hate the docker file so much but it's a set of instructions that are run one one at a time in order to generate this image so you use Docker build to do that from the Docker file this is the docker Docker file format it's this instruction and a set of arguments you can usually use backslashes to break the line up into multiple lines sometimes you can't because the docker file is not wellth thought out format you've got a bunch of instructions that you can use here um we're only listing what is that seven of them I think there's more like two dozen different instructions that are available but these are the most common ones so you've got the from statement that says I want to start from a known image and I'm going to add to that known image it's a lot easier than starting from scratch which actually is another way you could start a run command executes arbitrary Linux commands inside the Builder image in order to generate the the image itself you can set environment variables you can set the working directory you can copy files we're going to go through these so here is an example of a relatively re life Docker file they're usually about three times as long as this but they can be very simple so we're starting with the base auntu image we're saying I don't want to do everything myself I'm going to start with auntu 204 I'm going to install python by hand so I'm doing appg update Dy app get install Dy Python 3 pip Python 3 Dev again your app developers now have to understand this Voodoo right I'm going to install specific python packages here I'm going to install flask I'm going to create and change to the slapp directory remember the file systems isolated you don't have to worry about like oh where should I put the app should it be under like somebody's home directory or should it be under like opt local application or something like that nobody else is using this file system just go straight into root it's fine I add the application files I set a couple of environment variables so I copy the application file in I I set these environment variables and then I set the default container command so when I run a container using this image by default it's going to run Python and the unicorn. piy file it doesn't have to I can override that but it's nice to have that default makes things easy now these instructions are run inside a temporary Builder container that's this little guy down here there we go so one by one these instructions are run through the docker Damon into this Builder container and when it's all done it snapshots that container and that's how you get the image that is correct that's a very good question in fact I'm going to repeat it this run command only happens when I'm building the image right which means the app to get update and the app get installed the installation of that flas pack package happens once as long as I don't rerun Docker build and even then there's some caching that we're going to kind of skip over but as long as I don't rerun Docker build that image is always going to have exactly the same versions of these uh of python and and a flask and everything yes it does it eliminates the need for a for a virtual an in Python because this is your world you're back to Old School like I'm just going to install these things all over my system because all over my system is actually very small yes sir's item there's item potency that is correct that is correct it's almost hermetic and if there's any Nick people here I will fight you go ahead it's item potent because of the cache the layer cache you'll actually see that in the lab very good question though if you destroy the layer cache you run it again it's not that important go ahead sir just because it's poorly thought out why don't I like the syntax let's talk about that over beers later I can go on was there another hand no okay um where was I right so I'm going to look at I've got a local file system here this is on my laptop I guess I can see I've got a Docker file and I've got a unicorn. piy file right the docker file is pretty much what we just saw I run Docker build- T unicorn dot all right-t means I want to tag the resultant image with the name unicorn dot just means your current working directory right so this is It's just saying like look for the docker file here it's building it and at the very bottom you could say naming to uh docker.io SL Library unicorn um that feels weird that's fine then when I do a Docker image LS I can see that I have a bunch of other like images these are probably actually the the layers while I was being built I've also got like this bash image that's just from that I was doing but I've got that unicorn image image up at the top so that's how you create the images now I want to run it that's through Docker run I run Docker run- d-p 5000 to 5000 unicorn and then I curl Local Host 5,000 and I get a response from our python application what are these flags mean does anybody want to tell me what Das d means I give you a hint we mentioned it earlier demonizing yes that's right yes I know you all had it I know you all had it and then- P means mapping ports now here we're just going to map the same port inside the container to outside the container remember containers have their own network stack their own version of Local Host their own ports right so just because my process is listing a 5,000 inside the container means nothing outside the container so Locker run does a ton of stuff it'll run a new container from an image it'll also take care of downloading an image if you want it to so if I want to just run Docker run auntu it'll download auntu it'll map the ports it does permissions attaches networks mounts volumes sets up users all kinds of stuff which is why it has over 90 sub commands again Docker didn't have to go down that road where creating images running images and dealing with a registry were one tool and yet here we are so Docker runs the Workhorse and we're going to go through just a few of the key commands with Docker run uh first of all and this is important um The Ordering of the flags is important with Docker run so here we've got Docker run whatever flags are needed then the image name and then the command you cannot swap those so here I've got Docker run- V which means Mount uh my own slash from my laptop into SL host I'm running the image name auntu and the command is going to be rm-rf host Etsy password so the order is important right you want to make sure that you set the flags don't actually run that command please I have to reprovision your uh I have to reprovision your workstation it'd be bad the order is important the flags then the image then the command now the image name often looks like a command itself that's some misguided uh like optimization that Docker tried to provide it's not the command it is the image name so here I can do Docker run-it RM bash Echo High it'll Echo High because that's the The Bash is actually the name of the image not the actual command bash but the bash image is set with an entry point and whatever we also have Docker PS which shows you the running containers you can have a couple of flags here you see all the containers you could just show the most recent you can be quiet and only print out the numerical IDs um by default PS only shows you the running container so here I'm running Docker run I'm going to call it ID and I'm going to run the bash image with the command ID and you can see the bash image actually runs as root it's kind of interesting right then I run the engine X image and I call it engine X when I do Docker PS I can see that the only one running is engine X because the ID command just exited cleanly right it's done if I do Dr ps- a I can see that I've got both engine X and the ID container sitting there so Docker keeps track of those old containers so it can give you the exit code and the logs and all that kind of stuff from there yes that's correct without it down here Docker run name engine x-d engine X if I had not specified DD it' just sit there running engine X attached to your terminal right yeah now if I do dock ps- quiet it just gives me the IDS that's ex extremely useful for scripting right and I can do AQ to give me the IDS of everything then we have Docker exec which runs a command in a running container now remember containers are not virtual machines which means we don't have SSH you should not be running sshd inside your containers instead we have Docker exec so here I'm going to run a container called color I can do Docker PS I can see that color's running and I can exec into the color container which is the moral equivalent of SSH now I have a running shell inside that that uh color container and if I run PS I could see the Ruby is running there and also my shell is running there so I'm grafting into the same uh name spaces as the running container I also got Docker logs Docker tries to adhere to the 12 Factor model which is there's no like weird logging protocol there's no like talking to Cy log or anything like that anything you spit out to standard out is going to be your logs and the docker logs command is fairly rudimentary actually it doesn't do a whole lot so here I'm running a container called Loud all it's going to do is it's going to it's the auntu image running bash DC while true print out the date sleep a second and continue on when I run Docker logs I can see that I get that output it's just printing it a standard out so it shows up in the logs and I can also add some time stamps and things like that all right that was actually more talking than I promised you I'm so sorry about that but we are now ready to begin the lab remember you and your pair uh are on the same IDE so only one of you should actually be typing the commands at a time uh and if you have any questions just raise your hand Ma and myself are here to walk around helping you out I see we've already got a question so there's there's a few up here if you want to relocate down to here then you'll be able to join in uh yeah we can grab it here we go yep yep that's a good point so if you came in a little bit late if you want to participate in the lab we have handouts some flyers here that have specific information for how to get into the cloud idees they are in pairs so uh there are some open pairs down here so if you want to pair up with people who have cards that be fantastic you don't need anything except for Chrome in order to participate in the labs remember though you are paired up so if you hand somebody else a card there's another person with that same number do not just start typing commands you need to be coordinating with that other pair programming people it's pair programming second again if you do not have a pair go ahead and raise your hand so if you're solo raise your hand so that other people who are looking for a pair can join you so we have a few solo people if you want to find somebody to sit with find somebody with their hands up make a friend all right 13 who's number 13 by the way lucky number 13 oh you're still love very good3 e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e there's an a a worse bug so for some reason flask is not uh listening to the environment variables that we set and the environment variables are saying two things one is saying listen on Port 8,000 and FL like no off I'm gonna listen on 5,000 okay fine but the worst one is that we told flas to listen on 000000 Z who here knows the difference between 000000 Z and anything else I guess right right what it's seeing is that flask is only listening on Local Host now each container has its own network stack so its Local Host is not your Local Host you can't get there from here now being able to curl the Unicorn process is not actually an important part of the lab the more important part of the lab is fussing around with the containerization itself so my apologies about that horrendous bug I don't have a quick fix for it move past it are because I think e e e e e e now I know that um probably about half of you are still working on the lab but in the interest of getting through the rest of the content and because we are going to be sending you your home directories and the slides and the labs and everything after the fact we're going to move on we're going to start talking about kubernetes all right all right let me uh I gotta switch back over here there we go all right so let's talk about kubernetes we just saw what it was like to use Docker but as people have noticed there's a lot of stuff that Docker does not do for you right somebody had the question how do I wire up the networking so I can get Ingress from the internet into this container that's something the docker totally does not attempt to to do right now kubernetes is the solution to a lot of these problems kubernetes was born out of Google and because of that a lot of people have a misconception about kubernetes that uh well let me just ask how many people here have heard that that that Google uses kubernetes that kubernetes is technology that Google uses and is Beed Upon Us mere mortals right um it's a it's a common misconception in reality Google still uses something called Borg which is totes old school like it's not containerized it's just raw VMS and that drives the vast majority of Google's workloads Google tried to build a modern dynamic containerized version of Borg called Omega anybody hear her here of Omega read the Omega paper all right and as far as I can tell I'm not a googler but from what I've been able to sus out from talking to googlers who are willing to be a little bit Loos lipped it's basically failed right and then three googlers said it we're doing it again and we're going to do it open source this time so it's part of the community and that's where kubernetes came out of so it it's not actually something that Google uses internally it's not even used for Google Cloud run which is a common misconception the only Kates that Google runs is your Kates right when you're doing uh GK somebody just looked at me like Kates kubernetes k8s I call it Kates whatever I'm lazy the only kubernetes that Google runs is when you run a go like the your Works your clusters that's it now the nice thing though about that Google lineage is that it does have the Google philosophy um so it does have like the the amazing API and just the Google philosophy about how things should work and it's really good stuff now kubernetes runs your containers in production but the real value of kubernetes is that API in fact I I believe if you take the kubernetes API and remove everything having to do with containers it's still an incredibly valuable tool just because of how good the API is now kubernetes provides are you serious oh my gosh I'm so sorry it provides a declarative item potent and extensible API later I can talk about like over beers I'll talk about crds and how genius they are to run containerized workloads in production enabling a mutable infrastructure as code so let's res let's some of these key Concepts so first of all declarative declarative who here wants to tell me what declarative means no okay yes yes I like it write it once it becomes the thing that you have written declarative means yeah you are declaring your end State and it is up to the tool to get there right it is uh pseudo make me a sandwich right instead of saying exactly how a sandwich should be made you say I expect there to be a sandwich right here why is there not a sandwich right it it yes and then you've got IDM potent who wants to say tell me what I poent it somebody was arguing about it run it twice you get the same result concise love it so yes the same command run twice five times 100 times doesn't matter you get the same result at the end every every time uh extensible API kubernetes supports this thing called crds again I think they're genius you can actually teach kubernetes about new object types and they become first class citizens in the kubernetes API it's the first time I've ever seen that in an API say did you just well actually me sir I think you did we're we're going to move on uh because I will I will die on that hill but uh it runs containerized workloads as why we just learned about Docker in production it's got all of the Google juice all the the the philosophy all that Sr philosophy in it like what production means for running enabling a mutable infrastructure instead of instead of Docker exing into all these containers to change the version of python you just generate a new container image blow away the whole thing spin it up with a kubernetes supports this as a first class feature infrastructure as code everything is defined in yaml and Docker files it's very good all right so let's talk first about the kubernetes architecture there's multiple components to the kubernetes architecture the nodes the control plane and ETD are the main ones that we care about so first we're going to look at the nodes these are the muscle these are the things that actually run your Docker containers all right so it's just a bunch of empty VMS sitting there waiting for your workloads um going back a little bit the nodes do have a couple of components they've got the kuet which is responsible for talking to the API the cube proxy which is responsible for setting up all the networking and they talk to container D or Docker D or whatever containerization technology you want inside the node I see one question no no okay then we've got the control plane which is the brain this is running the API and all the controllers the way that kubernetes internally the architecture works is that you've got an API who's pretty much its only job is to act as a a a gateway to store the data into eted which we'll talk about in a second then you've got these controllers that are basically glorified for Loops little myopic robots doing one thing at a time and they're just constantly asking like uh like the schedule is a controller that's constantly asking are there any pods in the database that are not scheduled on nodes and the API says no and the controller says are there any pods in the database that are not scheduled on nodes and the API says no and it repeats forever right that's basically what controllers are and if it does hear that there's a pod not schedule on a node it'll say okay put this pod on this node and the API is like cool dude and it just repeats over and over again that's how you should think of the kubernetes architecture a bunch of little robots just knocking at the API over and over and over again then you've got etsd who here has heard of ets a few of you okay it's a database it's a very small distributed database that um uh is designed in the cap theorem to be uh available uh no consistent and uh partition tolerant not highly available if I remember correctly I might have gotten that wrong um he's like I got that wrong okay that's [Laughter] fine the name at CD is actually supposed to be a mimicry of linuxes eted it was designed by core OSS before kubernetes was a thing and the intention was this is a distributed version of linux's etsd it's supposed to hold small amounts of configuration information which is actually all that kubernetes needs so kubernetes uses ETD as the only memory in the cluster all right so that's the architecture of what is a kubernetes cluster once you have that cluster running you're going to want to provision resources into that cluster these are the API objects that the API server understands we're going to talk about just like three of these resources a normal kubernetes cluster has over 90 we're just going to talk about three of the main ones these are your bread andb butter resources so first of all we got a thing called a pod I'm gonna freak out right now who here knows what you call a group of whales there you go it's a pod you see the joke right it's Docker with the whale in the group okay so a pod is a set of containers sh in some containerized namespaces remember containers are just a set of Linux kernel features so you can actually bend the rules right so all these containers actually share the same network stack they have the same uh IP address for example uh each one of these containers has a different file system that they're viewing you should think of a pod as one main container that is the main reason that pod exists like it's running your your unicorn container and then maybe some ancillary containers much like little fish on the whale cleaning the side of the whale yes it is not like a subnet no I'm not sure what you meant by that I'm gonna say no no it's actually it is the same network stack so they all have the same Local Host right so if you if this container binds on 5,000 this container cannot bind on 5,000 internal Port internal to this network namespace I'm going to stop this entire line of questioning because this is far too [Laughter] deep they usually do but remember I said you can bend the rules because it's just Linux kernel features right so they actually when kubernetes runs these containers it says and you're actually going to use this network namespace instead of instead of your own it tells Docker that that's correct yes sir no you should think of these as these are just different processes and this is sort of the I'm I'm not going to go down that path because that's too confusing I'm gonna go ahead and say yes so that we can move forward sir um now a common mistake is to say oh a pod runs a bunch of different containers so I should run my API server and my database and my engine X all inside the same pod that is incorrect do not do that the pods are something you also never run uh by hand you should never provision a raw pod you should instead provision a deployment a deployment is again these are just API Concepts these are the deployment is not a thing if you sshed into the node you'd be like oh there's the deployment all you would see when you SSH into the node is containers you would have to figure out that oh these three containers are part of this pod and the fact that I've got this whole set of these containers that look relatively s the same they're probably part of a deployment right but a deployment job is to basically allow you to scale out your pods I want to run a 100 engine X pods right so I have a deployment of size 100 those pods will be running across all of your nodes right it deploy is not locked onto one node and then we've also got Services again somebody asked how do I Ingress into that pod or or sorry that container when I'm running on a Docker you can't with Docker but kubernetes does take care of that for you with kubernetes you can have these rules for routing traffic into pods which kubernetes calls Services I'm very disappointed that the kubernetes core team chose the word service for that I think it should be called like Network routes something a little bit more obvious when I first saw Services I thought oh that's my workload that's running no that's that's your deployment with your pods but a service you uh point it at the same pods and you say I want this to be a load balancer service and it'll actually go and provision a load balancer in Google and then it'll start routing traffic through that load balancer into those pods all the way through the nodes into the pods Network namespace so it takes care of a lot of that routing detail for you all the way to the port that the Pod is listening on yeah just the web server yes yes that's correct so he asked if each one of these pods is for example a python web app right it's actually they're running the same container they are the same python web app right and they are all each one is listing on its own port like 5,000 and this service is configured to know that it can route into that ports that pods that containers Port 5000 so you don't have to worry about Port contention with kubernetes so this is these are the the objects that you can create in the kubernetes API primarily deployments and services and deployments in turn create pod objects in the kubernetes API but how do you actually tell the API you want these objects well kubernetes doesn't have a gooey you don't click around to create stuff you can install that but then you probably get fired right instead it's all about the CLI Cube cuddle it's adorable it's Cube cuddle right now Cube cuddle is actually very predictable it's very consistent in how you use it Cube cuddle verb type and then an optional name and and optional Flags so I could say Cube cuddle get pods show me all the pods and it'll show them to you I can say Cube cuddle get deployment application show me the information about the deployment named application I can describe the service named front end it gives it to me in a in a more human readable way I can scale the deployment named blog so I'm changing the number of pods in the deployment named blog to three and uh excuse me so those are just some some examples of that pattern so once you internalize that pattern Cube cuddle becomes very easy to use now in order to specify when you want to create one of these sets of resources you do it through yaml right you get used to using a lot of yaml in kubernetes but it's actually great because everything that you put in that yaml is every I me let me let me flip that upside down there is nothing that you can tell kubernetes about that resource that you can't put in that yaml that yaml is the total and entire description of that resource gives you complete control and you can create many different resources in one yaml file you just have to use one of yaml interesting syntax things of triple Dash means start a new document and each yaml document inside that file is a different resource so here I'm creating both the engine X service and the engine X deployment that it will Route traffic to in the same file two one one file with one manifest with two different documents inside of it sorry to be pedantic but I want to make sure we're on the same page now in order to you send that file up to the kubernetes API I use something called Cube cuddle apply and I say Cube cuttle apply dasf engine x. yaml here's the file take all this yaml send it up to the API and create that stuff now GOP cutle apply is declarative and item potent it's declarative because everything that you need to be in that converged state is in the yo file you'll notice for example that we cre if we just did this imperatively the service is first and the deployment is second and that would be maybe an error in an imperative system may' be like I can't route to something that doesn't exist but KU kubernetes is like I don't care it's fine like eventually maybe that deployment will exist and then Lo P now it exists it's declared it over time converges to that desired State and it's item potent because I can apply that enin xaml modify the engine XML file right and then apply it again and it will only change the things actually that's not an example of not impotent that's an example of it being uh still declarative and convergent right so I modify it I say apply again and it's like cool I'm just going to converge onto the new desired state it is item potent because if I try and apply the same file twice without making any changes nothing wrong will happen all right any question questions before we jump into the happy path kubernetes lab one in the back yell it out say that again uh inside the Pod there is one network all the containers are sharing that network name space the PO you could think if you want to just squint if you want to just like like gloss over the details it's totally fine to think of a pod and a container as being the same thing like a pod says run this container in fact I think in the lab you're only going to do single container pods any other questions before we jump into the lab all right have at it e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e okay everybody I'm seeing a lot of uh seeing a lot of slack open right now and uh okay um we we could take a look at it later but in order to get through the next the Final Chapter I'm going to go ahead and move forward all right so let's talk about the rest of the iceberg right now kubernetes is fantastic it has this uh basically standard API for being able to deploy like an entire unicorn in a box right um but it's not what we would call a platform what I mean by that is it's really a set of Lego bricks that you can put together in order to achieve almost anything in kubernetes and that's where it really succeeds but the Lego bricks are numerous like I said before in a normal like this isn't even like a heavily used used or heavily configured cluster there are going to be about a hundred different resource types we haven't talked about endpoints or uh nodes even or or uh pod templates or Secrets or service accounts all this stuff is things that you have to learn you actually have to know most of it there are so many tools in the CN CF landscape that this image is often used as a dire warning about adopting kubernetes right this is just the landscape of vendors that are available to you or open- Source tools each one of these has Myriad complexities right and it's up to you to choose the right ones and to put them together we haven't talked about a bunch of things that you have to do in order to run production workloads in kubernetes like how do you limit resource usage um how do you scale automatically how do you ensure workload Health the this isn't even that deep to be honest we're not going to talk about stateful sets and we're not going to talk about um like Advanced secret management or uh Advanced C cnis we're not even going to talk about ISO right Jesus you open up service mesh you got it I know for a fact that a large uh furniture company has a service mesh team that was uh equal in size to their kubernetes team because the complexity is that big now what we're GNA do during this next hour is we're this last hour we're going to give you a chance to explore some of these deeper topics and it's going to be Choose Your Own Adventure style so so at the at the top of this next Lab you'll see a table of contents listing out the various things you might want to play around with you get to choose you and your pair get to choose which ones you want to do which order you can go as deep or as shallow as you want if you want you could spend all of your time on one and just play around with it and see what things you can do it's entirely up to you um you are not well I don't think any of you are going to be able to finish all of them that would be shocking to me so just choose the ones that sound the most interesting that's it jump into your laptops e e e e e e e e e e e e e e e e e e e e e e e e e e hey everybody we're going to change the format just a little bit because I I can sense some people like some people are like oh I'm I'm not so interested in the the these Labs uh and sort of sort of a trting so what we're going to do is we're going to go through the final slides just to talk about like why let me let me get to that after we get through the last slides okay um so we're going to go through the last slides and then come back into this lab and let you continue on so that you can spend as much time as you want on this lab until they kick us out of the room all right so that way we get a little bit more time out of it too all right so we just saw that that kubernetes isn't a platform right instead it's a platform for building Platforms in fact the core team behind kubernetes literally said that they never expected developers wrangling yaml they never expected this world that they have brought to us be us so if all that's the case why is kubernetes so exciting well because it really gets the foundations right it meets you halfway and it normalizes the cloud something we were just talking about with like bare metal and it's ubiquitous so what do I mean by all these it Nails the foundations kubernetes didn't come out of Google but like I said it has all that Google juice all that Google Sr philosophy built into it and Google's been doing this a long time so the kubernetes foundations are fairly close to being perfect we put this up on the screen before these have been practices for years before for kubernetes was a thing and only now do we have a platform that really encapsulates all of it and the API is truly genius this is an s's dream right it meets you halfway what do I mean by that well kubernetes is very pragmatic we talked about Heroku and the 12 Factor application model um I actually used to run engineering for pivotal Cloud Foundry who knows Cloud Foundry I'm curious oh God that's so sad there's like five of you anyways Cloud Foundry was a 12 Factor Heroku in a box I swear to God we were in all the big Banks like we were making so much money right but C kubernetes beat us even though Cloud Foundry had a such a simple I could not give training on cloud Foundry it would take an hour I'd be like it's CF push and now your apps running like I don't know what more you want right it was so simple and it work works so well but only for 12 Factor apps so only like it wouldn't work for databases it wouldn't work for like your legacy weird Java app that wants to talk directly to each other and all this kind of stuff right so my hot take is that is that 12 Factor was arrogant we didn't meet people where they were all the Hashi Corp Technologies are great because they're pragmatic they meet you where like terraform says you're wrangling VMS I'll help you Wrangle oh my God I'll help you Wrangle VMS right but uh is it working again yet oh come on there we go kubernetes gives you these building blocks where you can run anything your legacy weird ass workflows that are doing weird ass things you can run them in kubernetes ketes still loves 12 Factor but it loves Legacy as well so it meets you it's very pragmatic now all of these Legos that kubernetes gives you let you build whatever you need we haven't seen a system that can't be built in kubernetes we've seen and again we do a lot of this kubernetes stuff we've seen a few systems that shouldn't be using kubernetes but you can still make it happen right it normalizes the cloud all of these different Cloud providers I was saying this to somebody else they all have all these different apis these different ways of operating sometimes different ways of thinking about the different uh things you can provision inside these clouds and then kubernetes came along and just slammed them all to the ground and said we've had enough and even Amazon was forced to adopt kubernetes and to provide a kubernetes distribution they were the last to the game because Amazon recognized that kubernetes was infiltrating their moat but ironically they actually produced the best distribution eks is fantastic so kubernetes provides its own vocabulary and if you can no matter what cloud you're given if you can put kubernetes on it you can now use it the same way as everything else so most of the time there's some small changes that might have to be made but I'm talking like less than a percent most of the time the yaml that you've deployed to one kubernetes cluster is going to work just fine in another kubernetes cluster and now every cloud and I mean every cloud even the shittiest little cloud has to have a kubernetes distribution and that's kind of great for us right it's bad for them but it's great for us it is ubiquitous right that image that I showed before it's very scary of like oh look at all these things but it's also great because there's a huge ecos system of tools for you to lean on right if there's a problem you have that's probably already been solved when was the last time you saw an ecosystem this big not a rhetorical question I'm asking when's the last time you saw an ecosystem this big come on hint we're at the scale conference I mean come on it's exactly right we are not the first ones to say this this is not like some genius thing that we're coming up with but developers expect kubernetes now in the same way that developers expected Linux when Linux started taking over the server world and I was there I remember it's the same thing kubernetes is the new operating system for the cloud all right so that was that was it for the slides uh hash sales I am the I run a company I have to do this right I have a sales adviser says what are you talking about you don't have this I'm okay fine so this was just a small taste of what we provide we do a bunch of uh workshops we go extremely deep deep these are usually like weekl long workshops we de deliver them inhouse to teams right my favorite core kubernetes is is the most popular it's the everything you need to know about kubernetes my favorite is actually way down in this corner because nobody ever buys it it's my favorite one it's containers demystified and I deliver that one person personally um because none of my developers could be bothered but that that's the one where we go into all the Linux systems programming that underpins Docker it's a full week and at the end you actually build your own containerization and C from scratch so much fun right I love that Workshop please buy it because I don't want to get rid of it um but we have a bunch of these programming kubernetes for building custom controllers and all that kind of stuff so sorry hashtag sales all done um and as I said before this is the QR code so get it out scan it fill out the form and we will send you the or just email hello superorbital IO we'll send you the slides the labs and a tarball of the entire home directory so that you can continue playing with this stuff if there's any more of these Labs that you didn't get to you can try them at home all right so like I said I just want to get through those slides so that now you can focus entirely on the Choose Your Own Adventure paths the deeper kubernetes and we're just going to be here until they kick us out of the room depending I mean I don't know how much force they have we might even be here longer so we'll see all right thank you everybody ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "n_SncasXnAA": {"video_title": "Ceph Storage: a Caffeinated Primer", "video_description": "Talk by Federico Lucifredi, Gregory Farnum, JC Lopez\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/ceph-storage-caffeinated-primer\n\nCeph is an Open Source distributed object store, network block device, and file system designed for reliability, performance, and scalability. It runs on commodity hardware, has no single point of failure, and is supported in the Linux kernel.This tutorial will describe the Ceph architecture, share its design principles, and discuss how it can be part of a cost-effective, reliable cloud stack.", "transcript": "going to try to introduce you to the wonders of SEF and um Enterprise storage if you are not storage person how many here are would consider themselves open source people all right how many of you would consider yourselves um Enterprise computer storage people okay we have a bunch of s admins perfect okie dokie so uh um well without further Ado our agenda is to give you as much uh information as needed to be able to do some damage around SEF and um to walk you through uh a cluster live um for as much time as we have here so that you can can see handson how it works now um what is SEF so SEF is um a a scale large scale distributed storage system so if you think of the storage on your computer the hard disk or solid state drive um SEF is more of the kind of storage array that you find in a company something like the net up or EMC box the storage appliances that you have there SEF typically is used when you have petabytes of storage you don't use SEF when you have 10 terabytes I mean there are some cases where you do uh notably in kubernetes you do because kubernetes doesn't uh use much storage tends to be stateless um but in most cases St makes sense when you consume a lot of storage uh or when you need all of its functionality as in the case of kubernetes it is an Enterprise class um storage technology so it has a lot of functionality that uh consumer storage like U laptop hard drive wouldn't have um I see what you meant G I didn't understand bouncing we don't want that icon bouncing around all the time it doesn't want to die okay so um um you have complex storage requirements and you have a lot of data that's where this comes from now traditionally the storage Market is dominated by uh a couple of vendors EMC and netup historically are being the big guys and it's dominated by Appliance styled um Storage Solutions so you buy the hardware as a rack with the storage in it the software comes as part of the deal it's not something that you buy separately now that gives gives us the first buzzword here software defined storage software defined storage is something that's been happening in storage where um vendors are unbundling hardware and software so you buy the software from one vendor but then you use Hardware from um typically your compute vendor of choice your x86 vendor of choice Dell super micro whoever what is going on with the pro oh that's um okay so um the Dynamics that software defined storage bring are kind of the ones that you are used with in open source just a lighter version typically as a buyer when you buy open source you know that the vendor doesn't have the ability to dramatically raise prices on you like um y um or recent events because they don't have exclusivity if an open source vendor does that well you go to another open source vendor the software defined the storage or software defined at working scenarios the hardware vendor doesn't have that kind of power but they still own the software part so there is there are scenarios where things could go U could go sideways but that is what is driving things giving you choice and Hardware is part of it moving to off-the-shelf Hardware is part of it and the economics are part of it unified storage system is another buzzword that rings around Seth because typically Enterprise storage systems provide only some types of Storage storage comes in Block object and file forms block being your hard drive when you're formatting it if you want to think about it object being um aw ss3 is the most common um case for object storage out there you put things you retrieve things they're stored in buckets that kind of thing um file is traditional f file system overlay SEF is a unified storage system in the sense that it provides all three and it's designed to provide them with um as leading use cases as you can use these to the full extent of you can use them in Anger to solve your problems there in a lot of Enterprise storage systems the solution is one or two it has maybe it's something that's great at file and it has some object tacked on on the side it's something that's great at block it has some object there because it needs some object to run open stack maybe uh things like that so um SEF is truly designed to be a unified storage system so it stands out scalable in multiple ways uh we'll see but uh performance and capacity are the big things so in theory you could continue to add storage to uh SEF architecture forever in practice obviously there are implementation limitations what we see uh in our customer base is about 100 petabytes for Object Store use cases around 10 petabytes for file and block use cases in the community we see larger scenarios um like I've seen a 30 uh I've seen a 30 petabyte U for file and block but those are the typical numbers for what we see in in IBM or red head customer um the other thing is performance traditionally um storage of the Enterprise kind loses performance when you add more to it um because you're running through some against some bottom neck so you use more storage what is the bottleneck you're running out of it whatever that is in distributed St storage systems where things are spread across many um dozens or hundreds of nodes the opposite happens the more nodes you add typically the more performance you get either you get linear Dynamics where it kind of stays the same or because you're never running the the storage cluster at 100% all the time because it's underutilized you get these scenarios where suddenly you have performance bursts or you have to uh manage the expectation of customers because you they expect you to be always so incredibly fast um a while ago I was visiting the free software Foundation they are running their own uh SEF cluster there they were telling me improbable numbers of how unbelievably fast it was and so I looked at the cluster and I said well it's because your bench mark is so small and you have 12 nodes basically your storage is all coming from the ram of the nodes you never touch the dis maybe it even came from the caches in some cases it was a very small Benchmark but that's the kind of dynamics that you get when when you have scale out and the cluster is underutilized um we like to call SA the future of storage because we're a little bit full of it but hopefully that is the case uh Linux of storage however is a more useful metaphor Linux is the compute open- Source layer of choice for basically half of the world the other half runs Windows sadly for them um uh SEF aims to be the same thing for uh for storage the open source layer software layer for any and all storage now the reason why the metaphor is interesting is that if you think about Linux maybe I don't know 2002 kernel 2.4 what would you do in production with that file servers web servers certainly uh print servers were a thing back then databases were still off limits Oracle hadn't jumped the gun yet so there were many compute use cases but most of them were on Solaris in the Unix world uh for seph it's similar story there are if you look at the price for EMC there are at least 60 use cases for storage um Seth probably will cover all of them at some point but our aim today is to focus at least commercially on some and be very successful in those before we go to the next one and so some things seph does really well some things are more ambitious future projects and um yeah so that's the word of caution now SE is fully open source lgpl primarily um and it is not open source just in the sense of throw it over the wall open source it actually has a community and a very good one so uh we're very proud of that we try to spend as much time and effort as we can in grooming this because it's it's an asset for us commercially it's an asset for the project in terms of all the contributions that come in and um it ensures the healthy continuation of the project um as I was mentioning before SEF runs on commodity Hardware so if you're vendor of choices Dell IBM Cisco whoever you get the hardware from them you usually choose the same um vendor that gives you your x86 servers that's where you typically start from and then you um you install stff on it and you have another vendor provides you the support for uh the software defined storage layer as I was explaining earlier now use the wrong Hardware this is the tradeoff when you're buying integrated they figure this out for you already when you're buying disaggregated you make sure that you have a reference architecture that's probably the most valuable thing that the support vendor can give you so that you buy the right Hardware because if you buy the wrong Hardware the performance is going to suck there is absolutely no way around that you have to have the right Hardware or um well either you buy to poor hardware and the performance is not good enough you hit the wall immediately or you buy really good Hardware but remember the story here is scale out you're going to continue to add it's going to be a problem because it's too expensive oh I cannot afford to add more so you really want to work with the software side vendor tell them oh I want super micro Hardware what is the reference architecture for that and figure out what is the right Hardware to buy um obviously we have our own Solutions but you get to choose like we say in open source you have the right to choose your own destiny and we discussed the fact that object block and file are already covered so um all the various freedoms uh of the FSF are covered um freedom from vendor lockin is probably not an FSF freedom but it's very much near and dear to our to our heart in SEF and um um you will know we have transferred the SEF Team all 300 of us from Red Hat to IBM last year and as part of the transition IBM has uh confirmed with a public statement that you can find as a press release that uh the team will continue to contribute to open source we're not U closing off things or or forking off on a strange side um and uh the contribution model will continue to be Upstream first so things continue as before yes I'm sorry um so no I cannot share that so um well there is an interesting story but the answer is no we have clients for power so if you want to consume Self Storage from Power you can and there are some people who do uh primarily open stack they have open stack comp planes on power that have their x86 storage with SE and they consume the storage from uh from power but Power tends to be a rather expensive compute platform so it's not it's not an obvious choice to build uh to build a relatively Affordable Storage solution um yeah there there is an interesting story about that that can tell you off the Record but ask me later similar thing about s390 s39 90 as clients and um there is also the open shift storage product runs on s390 because the amounts of storage they are small and so it makes sense to just encapsulate the entire uh open shift um infrastructure in in the Mainframe if you're doing that but otherwise um Mainframe storage is atrociously expensive so you wouldn't want to use it as a layer and then put an overlay on it you just use Mainframe storage at that point um SE has a very long and Rich history that we're not going to go into um but the point that I usually like to make is that aside from um all the many users and um and customers and Engineers that work on this there are also about um um six six and a half exabytes of use of SE so that makes it I don't know if it makes it the largest software defined storage solution out there but it certainly makes it the largest softwar defined open source storage solution out there so um when we're just getting started oh actually while I am on that point what are people using seff for so primarily they're using for three things today the community there is a lot of um sefs uh that tends not to be uh the focus of the of the Enterprise vendors as much so actually I'm lying there are four things so in community there is a lot of sefs primarily in HPC and uh research space universities in U both the community and the Enterprise space there is SEF everywhere there is open St open stack is sort of uh limited to the niche of uh Telco to the vertical Market I guess Niche is not vertical Market is the proper way to say it um of Telco is where uh open stock is dominant now and there is pretty much two-thirds of the install base of storage for open stack is is set whether it's Community uh or Enterprise Community the numbers are even higher actually um then uh SEF is used as Standalone on premise S3 so you have S3 in AWS you have S3 on some other Cloud where probably it's backed by SEF in fact or you can have S3 in your data center and um for these kind of hybrid scenarios SEF is dominant that's where the very large clusters come from the 100 petabyte scenarios the 30 petabyte scenarios tend to come from object stor and the last thing is um overlay storage for kuber is tends to be very popular there because SE is a very sophisticated and mature storage technology it's a little bit of overkill for kubernetes storage today but as kubernetes gains ground in different uh use cases and it moves Beyond being purely stateless it makes sense to have a robust storage layer there so right now the amounts of storage that kubernetes are small but um there are very mature things like open shift data Foundation that use SE as a as an overlay on open shift um to u to provide storage kubernetes so as I said SEF is distributed so instead of having one large rack you have many many nodes the many many nodes split resources compute RAM storage and so on it turn out at building distributed systems is hard uh that's why we all have jobs but the other thing is that um distributed systems like kubernetes and open stack are a heck of a lot harder than Seth so system administrators who are familiar with SEF I'm sorry who are familiar with a compute plane like open stack or or kubernetes they do this um they do this in their sleep they go oh this is easy I'll I'll just to it uh storage administrators are in for a bit of a hurdle because it is not a monolithic thing it is something that is has its own Dynamics um s Seft uh scales out by adding resources so you add more nodes uh when you need more storage when you need more capacity or when something fails uh the important constraints in terms of Designing the architecture are that SEF is um is reliable system built of unreliable components or as um Community member Florian H likes to say SEF is an infinite hotel that continues being open while some rooms are being remodeled and some other rooms are on fire so the system is um remains operational even through these incidents and there are different levels of resiliency that you can seek depending on what you're using the cluster for um the core thing that you expect of every SEF cluster is that services are going to be highly available at least some cases ha plus one for some vendors but at least ha so if one component fails there is another one and at the level of data resiliency you're are going to be able to tolerate uh to uh to um uh failures so you could lose two drives Two Hosts two racks if you're looking at that scale um because we have three copies of the data in uh in play when we do replication or the replication designed by your uh by your arer coding scheme in the other in the EC scenarios upgrades uh uh and other scenarios of operational maintenance like that are also carried out without interrupting service and um the SE is very much on the side of uh keeping data safe over um over giving you uh more performance so if there is a choice between safety and performance saf team always falls on the on the um data safety side there is also something that uh you can get as sort of a battel story which is um a couple of times a year you see some vendor that comes out and says my product X is faster than Seth usually a startup and the problem the problem for this or I don't take it as a problem but the reason for this is that in Enterprise storage you cannot Benchmark against another Enterprise product because the license of the other Enterprise product forbids you from doing so cannot get the snazziest appliance and say I'm benchmarking against that because the terms of sale of the appliance say no competitive benchmarking so what is want to do when you do a startup on storage well you Benchmark against the open source dominant choice so um a couple of times a year I get phone calls that say oh my God so and so sze that they are faster than SEF and usually the phone call is fraught with uh nervousness and I'm like that's great they just told all of their customers about Seth because really we're not in the business of um of being the faster for for Speed sake we're in the business of being fast enough for what you are doing with the storage and we're always fast enough for what we are doing what you are doing with the storage and then the game is scal it out to many users or to whatever scale out Dimension you have in terms of numbers it's not about a single point Benchmark um but yeah it's kind of funny with with how it plays out because of the proprietary licensing about benchmarking that's a bit unexpected but we get free marketing out of it so hey yeah I'm sorry can you speak up louder oh upgrade methodology right so I need to show you a bit more of how the technology works but in terms of just bullet note the way it works is that we roll through the nodes and upgrade one node at a time and while that node is being updated it's out of the cluster and the rest of the cluster is serving your CL mhm so SEF is elastic in the sense that you can add or remove nodes adding is a much more common scenario but um there are cases where you want to shrink the size of the infrastructure um uh like virtual infrastructure you don't want to keep virtual machines with your virtual storage running for no reason if that's that's the case in an overlay um but typically the the thing is adding storage capacity or uh or carrying out upgrades not just in the scenario that the gentleman was speaking about software upgrades but there are also Hardware upgrades you want to replace the hardware at the end of life of the of the hardware after five or seven years you're basically doing the same you're taking one node out replacing it with a new hardware and so um there is one more scenario which is so the majority of self deployments are single clusters because as we said before SEF is highly available and highly resilient seems robust enough most users don't worry beyond that point but then there are users that have uh typically compliance needs regulatory needs where they have to um they have to have a backup scenario for losing the the entire facility so what happens if I lose the the New York data center well I have the Austin data center so those scenarios are usually spoken about as Disaster Recovery configurations in the industry um and they're expensive because essentially you have your infrastructure twice you can do a few tricks to try to shrink the secondary site a little but they never really work out you're still paying 90% of the bill so I think the most successful scenario is that you have the primary site and then your Dr site is basically your test um your test facility you have a really really nice test facility as a result which is not bad uh but at least you're using it for something while U while no no great disaster is happening the customers that who do this uh are under regulatory need to do it so for them it's not a choice architecturally for SEF it's an interesting thing because we operate in one way within one cluster and then we have um another operating when we're replicating data between two clusters where we're copying data synchronously so that we don't affect the performance of the clients of the of the primary cluster the other thing that's interesting is that um SEF storage as we'll see in a second is operates all in one mode within one cluster we store everything in a layer called rados whereas when we are doing replication asynchronously between clusters our replication modalities are very much uh um data mode dependent and that makes sense because uh the needs of of the replication needs across different clusters for let's say object are different than the ones for block and so you want to fit What U these customers are trying to do with these U geographical replications so we've touched on the fact that SEF is Unified and provides all the interfaces um the underlying layer that I was just mentioning is called Ros um which is um um reliable um I think a stood for autonomous or autonomic at some point but um the what it meant to say is that it's self-managing that it takes care of things that may happen to the cluster so that the administrator doesn't have to if you are a if you are an storage Old hand you may remember 20 years ago things like your unic system with directory Nam that with AA BB spread the directories in different places so that they would go to different partitions and different different storage arrays different locations or whatever it was that goes backing your thing uh seph frees you from that kind of thing or really any modern storage does but the way seph does it is that when data is out of place it moves it around the cluster on its own when data loses resiliency because one note fails and so you don't have three copies of the data that was on that node anymore you lost one copy it starts making a third copy on the remaining storage on its own it doesn't wait for you to replace a drive or ask you for a drive just uses the free capacity of the cluster so you just make sure that your cluster has free capacity that's all you need to do and you also need to as I was saying before about architectures you need to design a cluster so that I mean if you make a cluster of three nodes and one note fails suddenly you're looking for a third of the capacity that's not not a good scenario right so um SPF can be supported in configurations of three or four clusters three or four nodes but generally it U looks better when you are uh building architectures of seven nodes and up um 11 is a is a good number to start with depending on whether you're looking at performance or cost Dynamics you're looking at cost and maybe it's an object store you pack drives more tightly if you're looking at performance and Block store your you're tuning for getting enough performance for your block use cases anyway so rados is the underlying layer it's an object store and stores the data for all of these uh um types of Technology whether it's image um file or object and we'll see how in a second it provides the facilities for Erasure coding it provides the facilities for uh data replication so if your system set up with replica three three copies it does that if you set up with an oraser coded pool it does that you can have pools that are in different configurations so that you can optimize the economics maybe have a pool that's cheap and not very performant for certain use case and the pool that it gives you the maximum performance but it costs more to deliver Hardware to and you can serve both out of the same cluster and you can reallocate capacity between the two so SEF is very nice in terms of not having wasted capacity uh to one to one silo or another there is just a single storage Silo lebros is the is the API that you use to program this it's not uh it's not a rest API it's its own protocol and it is what clients use um very few users write liberados applications some of them are true Geeks and they do and it's very nice because they can extract the maximum performance of the cluster and they can do very clever things but it's kind of a science project it's not you see normal users do normal users go through the three interfaces at the top rgw is the um is the object store uh it stands for rados Gateway but uh Object Store Gateway is a better monitor for that it provides the aw ss3 uh interface and the Swift interface when you are running this uh as open Stack Storage um rgw is um run by about 70% of community users and about 50% of our of our supported customers and the biggest clusters are all big rgw object stores so the rgw team has to deal with fun things um like the customers pushing oh I want another 10 petabytes so they um they are the ones that have to deal with 100 petabyte clusters and um the implementation tweaks that go with that the architecture of SEF is made so that it can scale inde definely but the implementation you always find right so as the customer keeps pushing the limits um the rgw team has has to keep up with that um yeah I think that's enough on rgw RBD is the block storage interface it tends to be the highest performing one when uh when a comparison makes sense I mean it's a little bit of apples and oranges um it is the dominant storage technology for open stack sender glance uh pretty much anything that consumes storage in open stack uses that except uh Keystone that means an object store and so goes to rgw uh because of SE dominance in open stack about U about um 75% of our customers use RBD um between open stack and kubernetes we have that all over the place um uh what else I think we're we're going to leave the rest of the technical details sefs is the file interface and it is probably the most complex of the three because it's kind of magical to do in a distributed system a posix compliant file system but um we pulled it off we actually have the lead for sefs who will speak right after me here so you you will get to hear all of it from straight from the horse's mouth and um since we didn't do introductions for for a very long time I was the the product lead for our for Block so I was affectionately known as the blockhead now let's look at the rados layer I think we covered a reliable and highly available at length so I'm not going to to that again it is the fact that rados is scalable is ultimately the reason why SEF is scalable I mean rados does all of the hard things there are complicated things at higher levels too to be sure but um the storage parts are all in here um data placement replication rebalancing repair all of that comes from rados it is strongly consistent um and because it solves all the storage problems in one in place it simplifies uh the implementation of of services on top of that whether it's the three services that almost everybody uses or the custom applications that some customer that some Adventures customers write directly against ridos so rados is built of different demons the SEF system has U more than a dozen demons um running on different nodes that do different things but these are sort of the superstars OSD is the demon that takes care of the object and of the object storage um paths so the job of the OSD is making the hard disk that it's publishing typically it's a hard disk it could be a partition but typically it's a storage unit hard drive or solid state being exposed by one instance of the USD demon um it will re and store store data from there it will work with other osds to replicate data um and um uh and to rebalance it and um this is the thing of which you have hundreds or thousands in a cluster this is on every storage node for there is one for every um storage path the monitor is um is the entity that controls the state of the cluster so it makes a map of the cluster What machines are operational what IPS they live at and so on um using an algorithm called paxos it decides when to um to release updates to this map and um there are three of them typic in a typical cluster in a mediumsized cluster are five some very large clusters have seven but it's always an odd number because they need to vote in case uh in case because of that's how the paxus algorithm works that's not go there uh and um uh essentially what you get out of the monitor is the cluster map when a node fails when a rack fails the monitor is going to go oh update to the cluster map this rack does not exist anymore and then the osds get the update they start recovering the data that is presumed to be lost by making additional copies the clients get a new map and they start writing data in different places or retrieving data from different places the manager is a relatively uh new entry I think it came in in the last um six years or so and essentially what happened is that we had too much cod in the Monitor and the monitor is Mission critical if the monitor goes down your cluster is you have to lose all of them but if you don't have monitors you cannot get to your storage but uh the manager contains management functionality for the cluster and so it's not as Mission critical if you lose the manager you lose the control plane to do administrative operations it would be annoying as an operator um but um but it wouldn't affect the clients consuming this so by moving code out of the monitoring to the manager we' made the system more robust because we touched the monitor a lot less functionality that changes more rapidly like systems management type functionality lives in the Monitor and if somebody fat fingers some code and crashes a monitor well no big deal we can fix the bug without without data without data problems so historically this is how storage works you go through some Gateway ask where the data is and you get the data and then you have ha pairs or some fail over scheme for when that Gateway fails so that you have high availability um but this is instead how SEF works the application goes to the entire cluster everywhere all at once and U the magic of this is that it does without the Gateway so uh the client knows where the data is in the cluster by using um well by using the map uh that it uh that is going to ask um well this is this these slides are showing how the lookup would work if you need to go to a server and ask for lookup data you have a choke point which is that box with the three question marks I always need to ask where the data is so you have a big bottleneck same thing as having a Gateway so SEF instead uses calculated placement which is having the um the um the map of the cluster available the application can calculate where the data is and the map doesn't need to be retrieved for every operation the map needs to be retrieved only if um something unto happens like failure of the node or failure of parts of the network or things like that so only those things will cause a map update so the normal operation for the clients is just talk to every node by determining this is the data object that I want to write or this is the data object that I want to retrieve you run an algorithm on some properties of this data and then you figure out where it is on the cluster with no intermediaries in a sense SEF is more of a peer-to-peer system than uh than a storage system if you look at it it's uh because the the lookup system is a distributed uh hash table so you calculate where the data is and and then you either write it there or retrieve it from there as I said when a node goes offline then that generates an update to the map and the osds and the clients get an update saying new map adjust your adjust your behavior accordingly so there are three copies of the data on the cluster at most by losing one OSD you lost one copy only of the data that was on that OSD there are let's say 20 osds in this slide so you lost 20th of the data you lost 12 12th of your replicas you have still two copies of that data so the adjusted map lets you retrieve that data from different locations and you can um adjust your writing strategy similarly by having the new map that's really what's going on it's a little bit more complicated than that g will we go a little bit deeper but that's that's the core of it rados objects which are the the chunks that get stored in the rados cluster contain data but they also have um some metadata attributes that they can carry with them like U version number and things like that there is also the possibility of using um SEF as a key value store which is what um we call omap um and um uh and in that case you you have um keys and values instead of just um plain uh Blobs of data objects live in pools I think I hinted at this before so that you can differentiate some of the rados behaviors between different pools typically a very common scenario here is um choosing which type of replication so you have a replica three pool it's going to be high performance for Block it's going to be high cost you have your data three times uh or you have another place where you have um I don't know eight plus six eraser coding maybe that's not great for compute uh but it's perfect for Object Store and it's going to make your Object Store cheaper you can have the same you can have both in the same cluster and um sort of get the best of both worlds and that that kind of choice lives at the pool level um so um I think I walked you through the algorithm up to the pool level which is in a sense the the hash table part and now I'm going to hand it over to Greg to explain pgs yeah there are three identical copies so it doesn't matter which one I think so more or less random I mean it's what what happens based on the there are some things that we do to uh produce Affinity but they are not very sophisticated and in most cases within a single cluster affinity for a specific copy in terms of makes no difference Affinity makes sense in where you have a stretch cluster parts of the cluster one location and parts are in another location is kind of airly advanced topic that we're not touching on here today that scenario want to look at which uh one other thing while you're talking about that is interesting is uh Seth doesn't use right so most storage is already replicated at Hardware level so you know when I when you tell someone that's not into storage oh you have three copies of data that sounds very expensive you're using a lot of storage well not really because any enter and so have repli at the doesn't use ollers but we don't the replication is all at usev overhead factor is not as large as it may sound all right I'm sorry you have three you have three sub managers you have three monitors or man so the manager is not in the data p and neither is the monitor but the manager whether they go offline or not is relevant the monitor is relevant because we have access to the of if you lose one monitor it's fine because you have the other two so your clients still have copies of the map they still get updates of the map so everything works well you still have one so you're still okay but you're you should really run to the data center and start fixing but yeah in general you can survive failures no it doesn't count as much as I like s that doesn't count it's it's the same story as raid is not backup I don't have that slide in the deck but raid is not backup right doesn't copies you have yeah so within seor is that Disaster Recovery scenario that I told you it is pretty heavy-handed and expensive but it is in line right so you basically have your data pretty much all the time for backup there are plenty of tools that back fromer run your backs so for for offline basically keep doing offline backups the way you've done before all right we'll keep taking questions so raise your hand if you get curious um right so as Fred was alluded to we have this algorithm the algorithm is called Crush um which we use to place data in SE and when you have an application the information you need to that the application actually gives to well when I say application I hear mean something like sefs or RBD or rgw anything that talks to Li RBD or to Li ratos provides two pieces of of information it provides the name of the object and it says it lives in this pool and then Li RBD takes those Maps um it's the map of the osds and it runs the crush algorithm and it says Okay so the object lives in this placement group here at the far end and that placement group mat apps to these three osds um and those osds like Crush spits those out in an order and the first one in that list is the one we sent the iio to and you know that's just sort of the way the system works um and then all these placement groups are mapped you know as I said to three osds um you might ask why we use placement groups instead of because sort of the not the the obvious options are actually we could replicate to the Diss and have device mirrors um or you could replicate each object just to its own set directly um of osds but that comes with some costs um if you replicate discs you need then the discs need to be the same size this would be like raid one like if you or or and if you've ever you know run a raid system and had a drive fail then you need to like put the new drive in and res silver it and it's just like that one drive sits there and copies you know an entire drive out and that that storage wall can be really really high um if you try and replicate the objects individually out to whatever set of OCS you want that's a thing you could do but part of doing recovery when a when an OSD fails is that you need to make sure you have the new version or like the newest updates which means you need to go Trace out all the osds that have hosted the object and make sure that you get the one that's newest um and that means that every single device participates in every single recovery and you need to like do lots and lots of expensive calculations to go trace the history of who owned that object in the past um so we take a middle ground where we we Shard like the pool Nam space like um up you when you put an object into a pool we take a a very straightforward hash of that name and then we hash that name that or and then we um chunk that that H that hash range up into you know a 100 placement groups or a thousand placement groups or 10,000 placement groups whatever is appropriate for the cluster and the osds track the history of those individual plac groups and when an OSD fails then you know maybe it had 20 placement groups and so it proba and so all the osds which also held one of those placement groups go oh I lost one of my peers I lost one of my copies who's the new person in charge tell them they're in charge and let's let's get on with life and and recover the way we need to um oh sorry yeah um there's also differences in the way failure Works um but honestly I don't think this is that interesting right now okay so seph is very very big on keeping data safe you might think this is an obvious thing in a storage system it turns out it is not um so we always want our data to be safe we if you say that you want three copies of something we strive to keep three copies available if we don't have three copies available we tell you hey we are currently degraded and you know here's how much of your cluster is working on on recovery to get back to three copies of your data um and there's sort of this keyword here um when you're because you can decide how many placement groups you want in a cluster within a range um and you know the more placement groups you have the more peers every single demon has because you know if you have 20 placement groups it's Pro then and three copies then you know every OSD is is peering with what would that be 40 40 other osds um so if one fails then 40 other osds get involved in recovering from that failure um which in some ways is really good because it means that they only need to recover you know 5% of their capacity out to somebody else um and instead of all targeting one particular OSD or like one particular drive to take that recovery band which it's you know recovering about 5% of a capacity to four to 20 other nodes which is nice I guess yeah um so placement groups are just kind of you can't do every single object individually for technical reasons and you don't want to do a drive so we you know have this in between thing um the other thing that's nice less about placement groups but about crush is that Crush lets you describe the the way your cluster is shaped you can put in your racks you can put in your switches you can put in you know hosts living or drives live inside of a host and you describe that on the crush map and you say hey I want you know one copy in each of three different racks or if you have you know dark fiber and are and and have a cluster spread out geographically you can say I want one copy in each of three data centers um and then it you know aderes to those when it's in placement um this Crush algorithm is studo random um basically you know if you take a bunch of different inputs and and provide and um to the algorithm and look at the outputs it looks similar to a random output which is important because you know don't want to AC to put in 10 things that all start with the letter H and have them all mapped to the same drive that would be very bad it's similar to consistent hashing but um better and the reason it's better is that if if you um if you're familiar with consistent hashing if you have five nodes and you had a six node then almost all the data moves to a new node and you don't want to do that with storage because moving all of your data around off of a hard drive is expensive um so we try so we we have bounds where if you have 10 nodes and you and you add another node then you're bounded it I don't remember something like 17% of the data will move it's a it's a bit less than half of the proportion of added storage space the inputs to the crush algorithm are the cluster topology this is just the OSD map basically um and the description of the pool pools we've been talking about three copies and that's the normal system but you can have a two copy pool or a six copy pool um you can have eras your coding which is very very wide and involves 20 osds and a PG um and then and if you and then you know the object name is actually not part of crush the the object name just runs a simple hash and says it's this placement group and then the output of the crush algorithm is just this ordered list of IDs and in a replicated pool then the first one on the list is the primary hooray they get all the io um and they manage coordination with replication for the other peers yes this is just a logical construct and honestly we're spending too much time on it yeah e yeah right so there's that that's one of them one of you can have discs of different sizes in a SEF cluster and that has implications on the way it performs but it's totally kosher um and the other one is that when a drive fails seph like the seph OSD is autonomously Will re replicate data to get enough to get to get back up to three copies we're going to talk about three copies because it's simpler but it's all same principles um which means that they need to know which means that I need to say oh I have this pier they died I need to make sure that all the data I have that is in common with that Pier is now on is now getting moved somewhere else or getting copied to a new and if it's just like oh I have well I mean so it could just be I'm literally this the exact same thing as them which means I need to stream all that data out to somebody else who needs to receive it all but that means that you're limited in a recovery scenario to one disc's worth of through quod for recovery and that you know that one dis is extremely extremely busy so any IO to that dis from like actual users is is like totally totally stopped um and the other reason is that if you have you know a 4 terabyte disc and the average object is one megabyte you've got I don't know what is that like a million objects on the disc and if I have to go through and say for each of a million objects what's the history of this object over the last you know 30 days because I need to make sure that I'm actually actually the newest version and we're picking up the newest version like that's a lot of calculation it's a lot of disio just to find out who's supposed to be in charge of a thing and who has copies um so what we do is these placement groups there's between 10 and 100 of them per P per per OSD usually and that's a manageable amount of tracking metadata and it's a and and you know it's a manageable amount of impact to the Clusters IO needs when when it's in a recovery State and you know you're not always in a recovery state but if you talk about those 30 and 100 pyte clusters there's always a recovery happening the users of sefs and RVD and rgw have no idea about placement groups um Li Ros clients do not know about placement groups but obviously lios does um and if you're an advanced enough user you can do queries to do interesting things but they but yeah there's no there's no need to there's no expectation that they do it's not part of the sort of the standard contract but yeah if you're very Advanced you might you might care for I don't know does anyone here do like super Advanced to dup Administration yeah okay you don't care that all right um so Ros supports both replication and eraser coding for data durability um we want the data to be extremely durable that's you know sort of the whole reason we exist is we make data durable at data center scale um the earliest implementation because it was definitely the easiest is replication you write something to the set that it goes to what we call the primary OSD just the first one in that list and the primary OSD writes it down to its disk and what and and in parallel it sends it over over the network to probably two other to two other osds who write it down to their discs um that means you know 3x replication you only get a third of your total space is effective useable capacity so it costs that much um but recovery is incredibly fast you need one surviving copy and you can just read it off there and write it down somewhere else um this is it is not the same as raid one but you know in a lot of ways it is it's you know if if you lose two two ostds out of the cluster you just you just read it from somewhere um but some people want storage to you know to have of an inflation overhead on how much storage they like raw storage they need to buy for given capacity um so we also have eraser coding eraser coding is math I understood it once 15 years ago but the important part is that you do Matrix Transformations on a piece of data and you can set up an arbitrary number of actual pieces of the data and then and then basically parody chunks it can get more complicated than that but we'll put that re solement encoding um and so um yeah basically you say like hey I want I want to take eight chunks and then have and then be able to support and then be able to support six failures so you that's an eight plus six code and so you have 14 pieces of data um and and you need any eight of them to recover or to be able to read the io so your overhead is 14 over eight oh yeah it's 1.75 in that case um um this is you know this has impacts on the number of dis iOS you need to perform to serve any given N.O um but it's really good for object storage with large like like you know speaking S3 with very large objects um it's good because you know if you're just doing a lot of streaming streaming reads most of the time or streaming wres anyway then this is a good way to do it it makes it cost way less and you usually have a better ratio of iops to drive or capacity in that case and I'm about to change slides so I'm really hoping that this comes back okay um so pools are logical name spaces within a cluster um it is very common to have a seph ofs data pool and a seavs metadata pool and an rgw buckets pool and RG which rgw data pool and an rgw metadata pool um pools frequently share devices um and so you can set different Crush rules that say you know where data should be stored and then you pick one of those rules for for a given pool um you can also say hey like I have my SSD drives and my hard drives and like the metadata pools go on the ssds and the data pools go on the hard drives um but it's perfectly fine for them to to to share to share osds and to share physical storage um that's totally fine um let's see as you use up your cluster you know you will have less and less free space available and the way that you expand your cluster is you say hey I want to I buy a new storage node that looks like my other storage nodes or maybe it's you know the Next Generation and has a little more capacity and I plug it in and I run the software commands to add it to my set cluster and then the OS say hey my map has changed there's this new thing that is supposed to be replicating some of my data let me replicate some of my data there instead um this is an online operation it uses some of the raw dis IO capacity but it is otherwise totally transparent to any users of the software or the system um it's really great um within a given rados cluster you know it is perfectly possible to have hard drive pools and SSD pools and erase your coded hard drive pools or even erase your coded SSD pools though usually you don't do that because they're sort of across purposes but in the end you have this reliable autonomic distributed object story you took a bunch of you know perfectly normal hardware and you ran software on top and now you just have this giant pool of storage and your application says Hey I want to use it you know as an object as a well for r that says I want to be able to read and write objects in in in this pool um but then you know on top of that we build a file system a block device and an S3 compatible Object Store so rgw is itself an object store so it makes sense to put it on top of an object store this system speaks you know it's S3 and Swift these are HTTP interfaces or restful restful apis um and the radios way is another one of the Demons you run it either collocate depending on your you know system architecture desires you can run it collocated with a monitor or with an OSD or you can run it on its own servers um you will likely stick a load balancer ha proxy or something else in front of it so that you only have one point for your for your end users to talk to and it speaks S3 out the front end and there are a huge number of S3 clients at the back end it speaks liberatos um and and the Gateway is responsible for sort of that translation there are a few differences between S3 and libros first of all bros likes relatively small objects on the scale of four megabytes it's not a fixed size but like you know sort of that sort of thing an S3 object can be I think a maximum of 5 gigabytes and often is um so rgw has to has to chunk up those very large S3 objects into smaller rados objects out the back end um also um since rados is is calculated placement there is no Central system that knows what all of the objects in a rados pool are if you want to list the objects in a rados pool what you do is you go to every single PG and you say hey tell me what your objects are and usually it'll give you back a thousand at a time and then you'll say give me the next thousand objects starting it at the letter B um this is expensive it takes a while it's not an operation you should run S3 lets you list the contents of a bucket and it you do this all the time and it is very important that it be fast um so radios Gateway handles that and it uses the omap interface on objects that Federico mentioned earlier to store those and as they grow larger because there is a limit on you know how much data you want in any given object in their R map then it will you know then it does the work to split those split up that bucket index into a larger number of rados objects and make that be transparent and scalable and on and on oh I should have advanced slides earlier um frequently these the um the bucket index pool and the user info pool and the data pool are going to be separate pools um you can point them in the same place I think but like you want your bucket index pool to be very fast if you're doing a write you need to go do a hey I'm gonna write this object in the index pool and then start the write off to the actual data and then you know you need to like like like there's some transaction processing happening in that in that index pool so usually in an rgw cluster you'll have some number of ssds that provide the bucket index pool and then the object storage might H also happen on ssds but frequently is just very large hard drives because you're doing very large streaming reads and wres rgw also has some very sophisticated functionality for replicating across different SEF clusters um the seph project itself or like SEF at its core is sort of designed for a data center you can run it you know stretched across dark fiber but it really it expects land sorts of latencies because we are doing replication three times and we are very concerned about consistency when you do a write to a SE cluster and there can be caching in front in some interfaces but when you do an actual right to a rados cluster then your right latency is going to be the time to send the message to the OSD the time for the OSD to send its message to two other peers the time for them to all write their data to dis and then to send it back you so if you have 10 milliseconds tacked on between the client and the primary and each of its replicas that adds up fast that can be an unpleasant experience um so you know think land speeds for Seth um but when we start when you start deploying big optic stores people are like hey I can replicate my Amazon S3 to a different region on the other side of the world and it happens for me why can't I do that with my SE with my SEF S3 Object Store and the answer is you can but it uses a different mechanism than the than Ros this this happens at within the rgw demon cluster itself um so you can create zones of rgw and a z and an rgw Zone consists of a set of demons and the pools that they talk to and then you can do asynchronous replication of the contents of an rgw across different zones and usually you would expect those zones will be in different clusters that are geographically dispersed um but they don't actually have to be you probably don't care but it's good for developers it's good for people who are you know like writing software that needs to do this later on but are still in prototyping um and you can get a global view of users and buckets with like across all the zones but a particular zone is going to be sort of the primary for a bucket and it'll get all the data first and then it replicates the contents of the of that bucket in that Z zone out to the other zones that are targets for that replication but this is great for you know having S3 compatible object storage that gets spread out all over the world um let's see what else about rgw this is the part of the system I'm least familiar with yeah so a bucket is or a particular zone is responsible for a bucket but you can have buckets on both sides getting replicated to the other am I saying that right oh good yeah um so they can be active active sites it's just that if you try and do a right on to the exact same thing on both sides then there's some it that I think it redirects it yeah there's there's yeah there's DEC conflicting but it it gets it's you know DEC conflicting is always hard to do well um so we yeah but it'll be like you know one side wins it's not like like yeah okay so rgw has incredibly strong S3 API compatibility the last time I looked and as far as I know it's definitely it's the most S3 compliant Object Store in the world outside of S3 we've had people who were very strong competitors come like come to us and say hey we noticed that you don't have a test in your in your in your test suite for this new feature so we added one um and we're like oh clearly people like our test Suite because because they're using it for their own systems um there's security features of the sort that you would expect um you can do encryption and compression these are features of of S3 but I think we also pile our own on top for some specific use cases um it supports static website hosting in the S3 way um oh one thing S3 doesn't do is we integrate with elastic search to do certain kinds of metadata searching um and you can get event streams out of it to do processing on data stream or on the newly uploaded objects and things but it's pretty cool yeah it is a a thoroughly battle tested solution yeah no compl point the other thing I can tell you is that like almost everything that goes in VN S3 interface is going to be encrypted and encrypted data is basically uncompressible or and and and obviously you can't DD it so like DD is a feature in boxes it's not a feature in like scale systems and someone in the back yes okay so the question is if we have different networks for different purposes and stuff and the answer is that you you can configure a public and private Network in St and the public one is what clients talk to each other over and to monitors and repli and the replication is what osds do replication on um so the at the software layer those exist um but you know you're buying the hardware and the networking so you can configure it behind that however you want those could be the same network it could be you know that you have four four ethernet cables that you bonded together you can do all you can do whatever you can do with a Linux networking stack um underneath stuff yep yep all oh and another one sorry do you mean IP 4 and six or yes yeah anyway open source version totally supports this you can you can do it I'm I'm sorry can you speak up that's fine yeah it so so SEF consumes Linux block devices and if it looks like a Linux block device will consume it happily and do the right things um federo mentioned we like raid cards and that's true for some some use applications because raid cards if you can figure them correctly we still use them in a jbod array but um you can do faster commits because raid cards Enterprise raid cards have caches that are battery backs so it's just about dis dis a latencies and if you have you know drives that are have battery back caches then the raid card doesn't add anything better than that okay so that's the rgw gate the rados Gateway that speaks as three now let's talk about block storage um this was actually the first stable offering of the Seth project um because it turns out the block devices when you have a reliable autonomic distributed Object Store pretty simple to layer on top at least for basic features we spent you know a lot of time adding more complicated and interesting features but just having a block device is pretty straightforward um so RBD support is um a user space Library live RBD and also in the Upstream Linux kernel for 14 years now um so you can just you know take a fog standard Linux kernel and mount and mount an RBD image on it um and when you create an RBD object or an RBD image you you know it's a it's a CLI tool or available through a through a gooey website that the manager hosts or whatever um you specify a size for it and it creates a little tiny index index header object that specifies what the block device looks like and what's what its sort of object naming scheme is um and then if you want you know or sorry and then it just chunks up a block device into lots of little objects usually of size four megabytes so if you write it index zero then you'll get the first object and if you write it index you know one gigabyte you'll get the 256th yes 256th object in that in that RVD image um but it's just it's a thin wrapper around Ros to do things to translate to do that translation mapping from Individual objects up into a block device address space um the storage is you know out in the rados cluster so this is a great fit for anything you're running in Virtual machines where you want to be able to migrate them around arbitrarily which is why open stack loves SEF we are like the perfect this is the perfect storage syst for open stack or for you know any kind of private cloud like that um and it is integrated with everything um I am sure there's an open source project out there that provides storage that is not integrated with RBD but I have no idea what it is um RBD built a lot of more interesting features on besides just a block device you can take snapshots of an RBD volume and they are instantaneous um Ros provides a per object snapshot functionality and Li RVD is again a thin wrapper around that um snapshots are readon instances um that are copy on right so and you know belong to a particular image with point in time consistency you can also create clones of an RBD volume you can you know install a new auntu OS on a on a form I don't know a 100 gigabyte RBD volume and take a snapshot of it and then whenever you have a customer come along and say I want a new new new new BTU volume or VM then you just like you take a clone which is pretty much instantaneous because all you're doing is tweaking those RBD header objects of the snapshot of the golden image it's a writable overlay um so if you know the new object in the Clone exists then it reads that one otherwise it calls back and reads the snapshot of the underlying image um and those clones can themselves you know be SN shoted and resized and renamed and cloned again yada y y this is all as I said pretty much Point instantaneous constant creation time it's all copy on right so only change data uses any space whatsoever um and it works great um oh yeah okay so we got a picture the data layout here so on top you know you got your virtual block device you got the little header which says here's the name here's how big it is um we've been talking about chunking data up into for blocks you can do more complicated things but rarely care to but that's the striping um when you take a snapshot you give it a name the heart header says here's all the snapshots and what their names are um you know different options um and this is where we coordinate if you want if you want to do exclusive locking on an RBD volume which you can do you might care about that for you know failover scenarios in your in your cloud computing environment or whatever the Locking happens at the header and then the data objects are again just little 4 megabyte objects that map to a 4 megabyte space of the address range um the objects only exist once they're written to the space the the space within an object is only consumed once that space within the object is written to and usually RBD volume or RBD pools are replicated but you can eras your code them as well with the you know subsequent change in the performance characteristics um RBD also supports oh yes the default for everything in stuff is 4 megabytes you can configure that at the various layers if um within certain bounds that go up to depending on the layer 100 or Megs or a gigabyte but there's not a great reason to change it unless you know a lot about the about the system and your workload yep okay so r supports asynchronous replication in two in two modes one of them is you can do data journaling um in this CA in this case in this mode when you do a write to the volume it writes to the end of a journal I am writing these four kilobytes to this to this block offset and then it also goes off and writes that into the actual normal RBD image um and then that journal is consumed by an RBD mirror demon and it reads that journal and then it writes it out as a rados client to a different SE cluster so you know it is behind in some fashion and if the cluster a dies while without while the mirror demon hasn't fully replicated the entire Journal out into cluster B you will be behind by some amount of time but you will have exactly the im an exact crash coherent copy of what the image looked like at some point in the past um and you know you can bound that with your networking and your and your iio capacity and everything else um the RVD Mir to scale out in ha um but also yes this is ludicrously expensive because you're doing a full data Journal of everything you write and then reading it out and writing it out to Across the network um so if you have you know rapidly changing systems maybe this is not the right solution for you so you can also did you make a new slide for this hey you did you can also um take snapshots of the RBD volume in an interval and the RBD mirror demon can consume those snapshots and write them out to the to the other cluster um asynchronously and it you and rados has interfaces for it'll go have to go pery every object but Ros has interfaces for clearing what are the snapshots on this object what's the bite ranges in which those snapshots diers so it goes off and does efficient reads to only transmit the change data in that 30 second or five minute or hour long interval RBD has other nice features besides integrating with every storage consuming open source system on the planet and mirroring um we've got an RBD top tool that shows you the io activity happening within RBD andsf cluster um you can set quotas on users and volumes and yada yada um you you can specify that RBD volumes live inside of a rados name space and restrict users of RVD to particular name spaces um so that you know you can give direct access to VMS or Linux hosts that you don't control completely but that you sort of trust and prevent them from going on trampling from accessing or trampling on other people's data um you can Import and Export these these images to both RBD formats and and like Kimu or or no not Kimu um well yeah anyway whatever the whatever the block device compressed format is ow thank you you can import into Q from qow 2 to this um you can do incremental di between snapshots and Shi them off to some other backup solution of your choice instead of using RBD mirror um and there's a little Trash feature because it turns out that even in systems like this sometimes people delete things they don't mean to and all right as I said before there's Linux kernel client we also have an icei Gateway there's a coming out imminently you can go look at the PO requests nvme or fabric Gateway um it's it's integrated into the NBD block device in in Linux and then you know lib RBD the user space implementation of this can be linked directly into your library this is how KVM works with it and various other things and now we get to seph this is my project I mean I've worked on most of these but this is mine this is my baby um and sees is actually the reason that seph exists um SE was created initially as a PhD project after luster started getting rolling rolled out National Labs to fix all the problems with luster and it turns out that a file system is way harder to write than a block device is but it is stable now and we and there are customers from lots of companies who use it or sorry there are customers of many companies using seph ofes so seph ofs is a distributed Network file system it looks once you mount it in you know a posix environment like a perfectly ordinary posix file system it follows the PX semantics it does real file system things it is not dressed up the way like hdfs is or the file system that makes your your S3 Bucket look sort of like a file system as long as you don't do anything too weird like try and overwrite part of a file so it supports files and directories inside of a tree and you can put and you can hardlink them to each other and you can do renames and it all works and if you have three clients reading and writing to the same file then that you get coherent answers out that reflect the real POS requirements of doing that um so it is fully coherent and cache consistent um if I do a write from client a and then a client B does a read it sees that data like as soon as like the read will will serve that actual just written data from the other node whether you've done communication between those nodes on your side or not um and sep ofs is really cool because it runs on top of Ros um all of the data in the SS file system is stored inside of rados whether it's the file data or the metadata that belongs to the file or the metadata that you know a file system needs to keep track of all of it all of the files in the system um so the ability to do fast wrs of existing files is controlled by the characteristics of your rados cluster if you want it to be faster in terms of throughput you just add more rados nodes and your file system is now faster hooray um and if you know and can grab water for me um the number of files um is control scales with the number of metadata servers and this is actually not the number of files in the file system but the number of files which are actually used by a client if you have you can have a trillion files in this file system and if you only ever use 20 of them at a time you can have a teeny tiny little MDS server because all it ever needs to deal with is like 20 I notes plus the directories that that lead to them um and so MDS servers are basically big memory caches of stuff that already lives in Ros um and that you know handle the coordination with the clients and the like okay there are 37 clients with this file open and here's the permissions I've granted each of them to keep it possible for 37 clients to coherently access the same set of data all at the same time um so the question was about the performance impact you like the performance impact of multiple clients or okay so yeah so do we catch anything to the client itself and the answer is that depends on whether we can do so coherently um if there is one client to a file it will get buffer and cash capabilities that allow it to read data and remember it or to write data but buffer it before it sends it out to the osds because maybe it's going to be overwritten again um but once you have multiple clients asking a file obviously they cannot read each other's caches and so that that point we move into a synchronous mode where iio goes directly to the OSD you're responsible for that range of a file and to serve a read you go and read it from the OSD to Ser a right you write it to the OSD and then respond back once the write is complete so it depends on you know how it's being used if all the clients are readers then you can cash it all you want it's great yeah so so the important thing there is that the MDS needs to be sized with the amount of metadata activity that's happening so pedabytes of data doesn't actually mean very much from the MDS metric because if you have very large files that could be like 10 gigabytes of metadata but if you know it's all four kilobyte files then that's an insane metadata workload um so when you're sizing the MDS you really think about the number of files that are in use at a given moment not the total Storage storage of the system no the the Linux Upstream kernels supports ffs and you can mount it there but there's also a fuse client and um there's there there are various there are backends for systems like NFS gesha that speaks ffs out their back end and for NFS out the front um yeah yeah I mean it doesn't have to like like Linux supports it lots of okay yeah so that's yes that's that's the benefit is that when you have a seph ofes system the client speak for metadata requests if a client wants to create a file it says to the MDS hey make make a file for me and give me back the iode um I want to rename it or put it into a directory do that for me but when you do the file IO to the to like the data to the file you just go out and speak to the OSD that hosts those objects that are backing that file directly so so if you're using Linux then you get to then yes there is a kernel like you need you need some interface that presents a v system and that can be yes the Linux kernel client you can also do a fuse Mount if you if you can't modify your kernel but can run fuse you can do that um you if you're building your own custom applications you can link to lib sefs um yeah there's Community Windows drivers that I think I don't I don't know what one those permissions you need for that actually it's a little that's a little sloppier um yes yes and it is very complicated because it depends on what you're doing in general the kernel fine is going to be faster than the fuse Mount but not but but for but the user space if you're using it directly from a process is faster in some cases and slow others that you know they're different implementation so they have different strengths and weaknesses okay um we're getting there um let's see so I think I went through all these things already um I guess we didn't talk about scale yeah so I think we've run tests up to 128 MDS sir servers um it scales pretty well um a single MDS has a can can only do depending on the operation between 10,000 and 30,000 operations like file operations per second um but that doesn't mean that your cluster is limited to that capacity because the MDS aggressively spreads out authority to clients so if you do like a listing on a client 20 times then you're going to do the listing once to the MDS and then satisfy it out of the local CL cat the subsequent 19 times um similarly if you do like do a file create and then you know start changing the mode and obviously updating the the time stamps and you know whatever else like that's going to happen on the client and then it'll just sort of flush those out to the MDS in the background so it's pure memory operations a lot most of the time unless there's conflicting access so I alluded to this before but metadata fors is stored in rados um the client talks to the metadata pool the metadata pool has objects in in the rados cluster for every directory it has a journal of you know the updat it's performing which are again objects inside of the rados cluster that are separate from the directories there's a few other special objects for doing random things like tracking snapshots um but this has some really fun properties like if an MDS server dies you turn on a the MDS demon somewhere anywhere they can talk to your radios cluster and it comes up and it does a and it replays the journal and and it gives the clients a time to reconnect so that it can reestablish coherent state with them that was that might have been lost um and when I say dat coherent state that might have been lost I mean the permission that clients have to do things to files that has that may be timing out um not like you know file data or committed updates or like anything that's been up sync is definitely for sure save but you can just turn on the MDS server in a new location or in the same location or whatever and it just goes off to the rados cluster as a normal rados client and reads that data back and then life goes back to normal and there's a me the monitors maintain a map of the MDF servers and so the clients see hey there's this new MDF server that I should be talking to now because I guess my old one died it also has this fun property do we Okay we don't have there that um you can scale the number of metadata servers more or less arbitrarily um logical metadata servers exists because of the journal um but if you say hey I have one MDS server and now I want three then you turn on two more mds's and you tell the system to actually use them as actives instead of as hot instead of as hot or cold standbys and then depending on how you consider the system there's an MDS balancer that will try and balance the workload across all those servers or you can do static or configurable pinning of your own preferences to to spit to split the workload out among them and that all happens you know like the system manages that it happens more or less behind the scenes and you don't have to worry about it too much sefs also supports snapshots um you can go to a directory and you and you make her inside of a special do snap namespace in folder in that directory and then then if you go look at that new newly created folder you created inside of the snaper it is a snapshot of everything in that directory and the directories underneath it and you can do this in arbitrary locations these snapshots can be you know you can snapshot you know like the slome Greg folder and then you can s later snapshot the slome folder and then you can later snap snapshot the slash like the root of the file system um and this all is totally fine um I will know there's one one exception to that which is that we will later talk about sub volumes and sub volumes must be snapshotted at the root but that's a special case um to support mostly those specific requirements of the system ah yes no when you create a snapshot as a unit and you create or delete that snapshot all all at once yeah yeah yeah yeah you can't delete a portion of a snapshot unfortunately the the way yeah yeah unfortunately not um these snapshots are point in time consistent at a per client level so if you have 20 clients working Direction shot it and then you might get the data from one client being a little later than the other um that is about to change for squid release where we have a where we have the ability to qu iio um though those are you know quing iio means stopping it so maybe you don't want to do that anyway no one's actually complained about this to us over the last 10 or 15 years but but we realized that we cared about it it's you know oh yeah the snapshots are easy to use as we talked about it's just Mak I mean there are other interfaces if you prefer them but this one um there's a you when you mount the file system you need a you need a key to access the cluster and that key specifies if you're allowed to create snapshots and in what folders or or rados pools or whatever you can do so um but as long as you have the permissions you can just do it from you know the posix UI um and again these are pretty efficient um snapshots are built on top of the rados indiv object snapshotting layer so they only consume space when changes are made after the snapshot is created snapshots are read only um and you know in the MDS server it like you know it does some metadata twiddling to create the snapshot and then in the future as you make changes then it does a copy on right of the metadata as well as the as well as the file data inside of rados yes I'm sorry ah okay so that's a good question he asked he asked if Snapchats are counted under the same user space quota and the answer is that se does have quotas quotas are enforced entirely client side and are and and and they're lazy quotas they're not they're not up to the up to the millisecond accurate in terms of data used and whether you're against the quota and snap CH quotas do not interact in any meaningful sense um which is why you need permissions to create snapshots but yes snapshots do exist they're good for you know an environment where you control the clients um if you don't control the clients we recommend that you go through a samb or an or or an NFS or something Gateway because SEF ofs clients as we said everything's fully coherent so if you have a malicious client that like gets permissions to read a file then until they then if then it can prevent anyone from ever writing to that file Again by just holding on to those permissions super bad want to do that um we will eventually evict them but it'll be like five minutes later and unless you do something specific to deny access from that key then like they can just do it again so you don't want that um but SE vest um you know it consists of metadata servers talking to Ros pools you can create multiple seph of's inside of a rados cluster because you can give them their own pools and their own metadata servers um well it has X address because that's part of PHX supports flock and fun file locking in the way that you would expect because that's part of posix it has quotas because people ask us for them and it satisfies many use cases though not all of them um you can and this is the way that that um Brook with kubernetes and open shift tend to use it and a lot of a lot of the open stack users via Manila um there's a there's a sub volume library that lets you that creates you know folders inside of a special directory and generates access permissions for only those folders and make sure those folders are isolated from each other that you can give out to end users um inside of a you know semi-trusted environment or that you can you know provide to your NFS ganesa server to provide out to actually untrusted users um but while maintaining security as much as possible across the whole system um ah yes okay so sefs also supports storing data in different pools um you can have and and you map that data at the directory level and then everything underneath it inherits it until you over until you until you provide a different policy these these policies are accessed and manipulated there's other interfaces but mostly by talking to Virtual ex adds on the directory that you know you can say hey there's a um data layout thing layout exat and you can set the pool that data inside of that directory will be created inside of um that doesn't move data for you that already exists but any newly created file inside of it will write to the place that you tell it to go to so you can have you know bulk data that lives on hard drives and fast data that lives on nvme devices or you know some data that is replicated and some that is eratio coded or whatever because you can just have an arbitrary number of rados pools that are used to store file data um if you are in the HBC world you may be interested in lazy IO which lets you turn off the coherent caching and you manage it yourself um let's see what else do we have here um oh yeah okay so as we talked about earlier Seth best has been in the Linux kernel since 2010 or 11 or something um you want a pretty upto-date one though because you know you want to support the newest features and you want to get all the newest bug fixes um there is a seuse binary that you know uses the file system in user space interface um we have NFS and the the samb VFS s is has existed for a long time and is getting dramatically improved this year like as we speak and lib sefs is you know a library that you can link in so we have in the past supported people using sefs instead of hdfs inside of a dupe or in various other places and that way you can talk directly in your user space application out over the network to get fast access that doesn't require colel updates or upgrades or anything and that is an awful lot about most of the seph project though there are a few things we haven't touched on so seph ofs this is actually I don't know how long the dashboard's been around it's been around for a while um it's part of it started out as a you know third party think on top of it and got integrated into the entire Upstream project so the dashboard is a you know website goey that runs inside of the SEF manager process and lets you manipulate your St cluster and do management and day two operations and look at metrics and get alerts and all kinds of cool things um that's built on top of a lot of apis and monitoring that exists in the CLI interface and that you can plug in directly yourself VI other mechanisms if you prefer so we have warning States if you lose an OSD then we'll say hey this proportion of your data is degraded because it only has two copies and you can you know watch it and get updates on that um there's integrated configuration management you can say I want all of my osds to get this new parameter or all the osds inside of this Rack or this one particular OSD to get the to get these parameter changes um you can I mean you can read these things is any of it particularly that you want to particularly call out I think yeah yeah um I guess the one I will talk about is telemetry um it is 100% opt in but we have joined the is asking for Telemetry on on how the system is being used because we're a distributed storage system and we really want to know what interfaces people are using so we can know if we should be devoting more effort to it or you know what features there well we don't do a lot of feature reporting but um at least one interface is and and the big one's crash reports um if you turn on Telemetry and your and and some and a demon inside of your cluster crashes then a report will automatically get generated inside of our tracker and you know we scrub those periodically and can Cor them and be like Oh everyone's seeing this one what's happening as opposed to that's your hard drive your hard drive is bad fix your hard drive you have a number of installation options um the the SEF Upstream generates and maintains SEF ADM which is an orchestration interface around containers that um and and SSH um Rook is sort of a sister project of ours which is the kubernetes interface for installing and running and operating Seth um there's also sort of the well one of the previous ones was SE anible and that's still alive though gets a lot less attention um their puppet I don't remember what those are called are there recipes it's been a while manifests that's okay so there's puppet manifests um canonical maintains micr with SNAP and some other stuff around it deep sea comes from soua okay soua deep sea um and you know we do explain how to do your own installation which used to be what we expected everyone who installed seff to do but it turned out that was wrong so now we have these other more opinionated installers but you can definitely go still install it yourself we have like three more ways of running SE that developers use and the seph community seph is open source it's mostly lgpl version 2.1 um there's some bits that are a pachy licensed or whatever based off of other the projects that we're integrating with or that we pulled from for things um you can go look at the Upstream GitHub and tracker tracker's red mine this is all public like we we don't do anything I mean people have conversations in hallways sometimes but like the like the PO requests are public you can go look at them you can go read them we've had like four in our history that were private because they were security related and we had embargos um you can see the community meetings and the stand-ups that the de that you know the developers are pretty much all in at least one standup every day you can we have mailing list for developers and for users and we have a we're more on slack now but it's still bridged to our old IRC rooms if you prefer that access meth mechanism um we do releases generally a major release every year and Point releases for the last two or three mainor releases that on an ongoing basis for useful or important backports to fix bugs and everything else um we're distributed in a wide variety of distributions and other alternative installation mechanisms Pro MOX will will deploy Stu for you and we integrate with Cloud ecosystems open stack like I said loves us we're basically the reference storage installation for everything that open stack does at this Point um we're big in kubernetes though a lot less dominant there we've been in KVM Bri ages obviously we have clients inside of the Linux Upstream um and seph has a a foundation it's a with lives inside the Linux foundation and owns the sort of we have a big Upstream testing and CI infrastructure and you know run the sephon Community Conference events and sep days and things and we have many EV and many many people participating that that way addition to just technical interactions sorry I spoiled seon we also have sep days those tend to be smaller and Regional websites check out if you're curious oh we forgot to update this slide seong hun 2024 is happening um and here's more links and I think that's what we got presentation wise and JC is going to come and actually show you Seth any questions before that happens do we have any use cases in HPC specifically and then and then I lost I heard you say lusters so uh what are the challenges of breaking in ag e e all right technically compared to luster there are some pretty key differences um first of all luster is definitely definitely faster if I IO them is um and partly that may always be true but also we just like there aren't enough people who are focused on that in the SE community so that could get better um and that and that's file and I mean like because we do three repli because we expect our data to be safe if you work with luster you have definitely definitely definitely lost a a luster file system in your in your working career um and people have had SE crashes but they have not lost a whole file system um and so that's just a sort of philosophical difference between them but it does you know change what what you're going to be going for um the the so Seth FS was initially conceived to the the resiliency problem and to deal with metadata scaling um luster has in the intervening 20 years gotten a lot better at metadata scaling and I can't really speak to the details of that because it's been a long time since I looked and I was never a luster expert um but they have gotten better at that um but also sefs is for oh and and there are some specific features that luster users really expect that have never been implemented in Seth ofs and it's because there's never been like enough momentum to make it happen um the the big one is that seph ofs the coherent caching is all file based so we don't do the bite range access locking and that's a key part of you know not just luster provides it but that's a key part of many HPC applications um and you can blame the whatever the lust the not quite luster Foundation was in 2012 for the fact that Seth never implemented that open SFS that's it because we asked them for money and and oid told us we would get it and then they turned us down um but we are faster for some kinds of work cases I went to oakd Once because they ran some experiments and the like you know HBC storage lead wanted to see how fast something was so I like ran a command and it was done and they were like oh that was F we expected so he ran up and put it in a loop run 10,000 times and I was like that'll take a while to to enter and then it was done because it ran once and was then was in the client and was entirely satisfied out of memory which you know blew their mind because luster doesn't work that way um but all that said there are HPC users of sefs um Stern is one of our biggest users and that started in definitely not their HPC but in their it infrastructure but every year it pushes a little closer and a little like it's it's never going to be the the dump for their like actual sensors but they do run their like some of their aggregation on seven now I'm pretty sure um like their second or third tier CERN sorry CERN yeah and um yeah how many admins do you need so okay yeah I mean it's the thing is that like people who are used to being storage box administrators find stuff hard to use but people who are like Linux administrators or open stack administrators or open or or HBC administrators are like this is so easy because yeah and that's is it's basically a set of like like it's a set of Linux systems and it works the way you expect Linux systems that are doing this to work um from from an administrative perspective I mean I yeah I don't have any idea about the administrative overhead yeah he yeah I mean he does the io 500 every few years com and I'm trying to think who actually does HPC workloads because we do have people in the community but we sort of see them at conferences and then they mostly don't talk to us they like yeah it works um yeah yeah the the National Labs we hear from a lot less now I know there I think there's still some at Sandia though we haven't heard from them in a while um I mean so where did I oh we don't have these logos up anymore yeah I mean there's University of Michigan is one of our um was one of our board whatever members for for academic institutions and we're in their H I can't remember what they're called put their HPC HPC Center um that's not HBC though at least yeah and then there's a few a few medical foundations in like the east coast and and England and I can't remember the names of the ones that I can say but they do talks if if you search for seph in like a field you're interested in you'll probably find something CERN hosts SEF days and Computing days that talk about SEF pretty frequent like you know every year or two so you can find those archives and see sort of the HBC people talking about what they like and dislike about it that's a change oh yeah I mean if you went from HBC to cloud and are coming back we're definitely SE is definitely the thing to look at yes okay okay okay okay okay so so where an object is stored is not in a map it's it's very important that it's not in a map there's a map that describes the shape of the cluster and then you use that map as an input to a calculation and say if the object Fu exists where does it live and then you go off and say to say like okay this OSD is would be responsible for object Fu and tell me about object Fu on the OSD it might say I've never heard of object Fu or it might say here's the object um but it's it's a calculation based off of the the name of the thing you're trying to find and what the cluster looks like the place that's respons that like that calculation spits out a set of osds and you say hey o first OSD in this list tell like I want to write this object and you know ah okay okay so so seph has its own custom system called Blue store that that speaks a block device a Linux block device protocol out out its backside so it lives inside of on you know a block a Linux block device somewhere that has you know a heie of software called Blue Store managing it to lose that like yeah yeah the disc can die so that's why we have three copies or eight shards and six recovery shards or with the Eraser coding of the data yeah oh well well again so well I mean so it is it's always synchronized when you do a write it gets written down to all three places it's supposed to live before the right is before you get the right back is done so that's always always done and and safe so there are people you can run SEF on all the SEF demons on a Linux host a Naas would be a really small one it has been done and people do it for fun I would not recommend it for anything meaningful but it'll work um at least you know if the Nas is actually a Linux system you can do this with um there are a lot of home users you can if you go to like the reddits for home Labs or Stu then you can see people talking about it there for like you know on disperate it's a lot more common to be on disperate pieces but yeah like you can get you can get a long way you can get a long way on an xfs rate array and it's gonna cost a lot less in the green and then so a client can mount I mean it looks like a file system so you can mount an arbitrary number of them but there's no like it's just different file systems yeah so so a so a given client can yeah yeah if you're if you're talking to different file systems in different clusters you they're targeted at different monitors in within the appropriate cluster so it's just they're just wholly independent from each other but there's nothing that restricts that from working so seph can run on commodity hardware and does and whether you and you could install it on a Linux server that gives you Linux server access and I have no idea what your you know expire support expired Hardware box looks like or can do technically yeah so I mean seph is not going to do anything that would prevent that but you know I have no idea what what Hardware restrictions the boxes are enforcing on themselves I have no idea if it's like a real Linux system so that's so I mean so I mean seph definitely doesn't care seph like if you can if you can boot the demons on it seph will do it but yeah and I mean there are there are many people who who run seph on sort of expired or previously used systems there was a talk at sephon last year about the environmental Joy of doing that because it was in the Netherlands and one of the local supporters was like the Netherlands has has pretty strict climate reporting rules now and so they were like look how great this is for your for your for your legally mandated reporting systems yes in the back I'm don't think that's quite translating in my head sorry ah okay literal uid translation you're asking about okay sorry okay um seph is not well that's complicated mostly seph doesn't but like there is the um uid mapper is integrated in the kernel now and um you can set up seph so that a user can only act as a particular uid and do mapping inside of lip SEF fs and the other layers don't really speak uids the same way so mostly but it's it very much depends on the details because you know uid mapping is the whole thing so much of it but what it works for your use case I have no idea yeah but I mean like you know that's that's the file system like block devices and uids or object stores and uids don't they're they're totally unrelated so you know more about this when you think about an out PVC that would be on a block device for example right when when you're going to attach first the block device then you're going to mount the file system so cube is going to lay the file system and basically it's going to use the well the characteristics of the part that is starting to lay down what user ID and G ID you're going to use on that particular file system so if someone was trying to remount that if he doesn't have the same user user ID and I'm talking about RBD but for sefs would be the same because For sefs What we do is that we create a sub volume for that particular PVC so your part in your namespace are also going to use the uid and the g that you use so the sub volume will use that inside the fast system that you lay on top of that directory which is but from a part perspective it looks like my file system so then comes the problem is that if you want to share a volume and if you want to share between two parts that are inside the same name space well you're good to go right you're in the same name space so you're going to use the same characteristics unless you override in an actual part it becomes more complicated when you want to share that PVC across multiple name spaces and then you come into tricky situations and that's why a lot of sites when they have to share an ux PVC across multiple name spaces what do they result to the good old NFS right because it's easier and I don't have to worry so if you wanted to do it with Native the the the native CFS CSI driver you would have to make sure that the two name spaces use the same uh security setting FS Group uid G so do you really want to do that then you get into those Corner cases where it's acceptable when it comes to security and other Corner cases where security is going to say no no you can't do that so there are a lot of areas that we would have to discuss and inspect to figure out exactly what you want to do but that's basically What's happen what's happening inside so we're now going to go to the live part yeah I'm the one taking the risk that's what he's not telling you but um we didn't introduce was trainer for has literally trained more anybody else plan including and um he has pretty much won every recognition that bread training is capable of giving somebody so thank you well yeah I've been around quite some time uh just for the fun and and it relates to what Greg was saying uh when he was talking about the different deployment tool that you have with Seth right my first encounter with seph and I remember the date is actually February 2013 that was my first uh my first encounter with SEF and back then the tool was mkf FS all right and remember when uh Greg said that the original module the reason of the SEF project is the sefs module right and that's why the tool was named MK sefs right although the the name of the project is SEF but the original modu ffs anyway so I've been around since uh February 2013 I've been doing SEF uh my first prototype was uh two months after that basically we we introduce that to a customer in Europe where I was working uh back then I used to have my own company it was a contractor and then in November of that year I became an intank employees and since then I've been doing SEF almost every day uh SEF Standalone uh Seth in kubernetes environment with rook and I'm presenting next week at cucon in Paris I've been doing SEF on openstack in hyper conver environments non hyper converg environments and I've been doing open shift with the open shift data foundation and IBM Fusion which is is basically another uh renaming of it so what I did is that uh we had to make a choice we were limited in time I was told two days ago that I would be here so I come from Vegas I'm not that far uh so I basically created like three VMS on my laptop um and those VM for practical reasons um I did a training internal to IBM last week so basically I reused one of the VMS that I use for my internal training so it doesn't use the community set version it uses a downstream uh set version so that and you will see the colors of the dashboard look very blue right but that's fine it's the same it's the same code so we have three notes I mean I can be quite simple uh so scale node one scale scale hopefully it's going to work and what we're going to do is that we're going to build a tiny three node cluster each of the VMS um basically have three disc devices well four one being the boot drive three being drives that are empty that we will use as zds remember when we're discussing that one of the benefits of SE is that it's it's extremely flexible and designed originally to consume like you know stuff you have on the shelf or that you have left over basically to that extent you can actually do it on a VM it doesn't care as long as you have enough course and enough RAM on your laptop you can do it you won't do performance benching with it but it will work just fine so one of the things that we're going to do and and I copy pasted some of the commands that I'm going to use so I can them the steps is to do what we call bootstrap the cluster so bootstrap the cluster is to basically stop the minimal configuration with so and responsibility that mod to basically help using e to willas we the cluster when we the the cler what weate ke so we that key to the other that we have so that basically my single manag can actually start toing to these other nodes in the cluster from there we're going to assign labels and you're going to see that basically the cluster comes to life as we put the labels on the nodes the uh automatically the orchestrator module is going to some new monitors because we need three months at least in a production cluster then we're going to assign labels for the osds and we deploy osds you will modu as soon as you say add some osds is going to ask all the nodes tell me about all the devices you have and we'll do something simple uh which is basically consume all the drives available on the no make it simple now you can do some more refine you know things say only consume the ssds only consume hdds there are plenty of things that you can do including consume only the spindle that are exactly 16ab right all you can do do when you create the OSD put the D part of the OSD on a spindle but put the metadata part of the OSD on a flash drive so you can do all these refinements and when I send you the instructions the instructions will be with that and we Te You the format of howy create what service that can what want we're going to do all these little steps one at a time but we're going to start basically first uh with the bootstrap forgot to mention one important thing so mine is going to be this one that's my registry on1 on that's of image that's act tagur us passw access a cat on that file when the time comes so what we're going to do first is that we're going to do a f ADM bootstrap so the uh bootstrap is going to create one monitor create one manager like specific remember when the questions about how many networks do you have we have a public network trust Network then when you do A3 you obviously have the network where's the point actually L that can be different from the and the cluster Network so you have a whole bunch of networks in a envirment on mypop I just IP Fusion this is done and now what we're going to do is that we're going to do an SSH copy ID so we're going to copy that set up so we're going to copy the public key to one of the nodes the se03 I'm going to copy to se02 just so that it's just like right here and available and what I'm going to do I'm going to this I don't remember in what order I put them and now we can go here and if we refresh say visit my old eyes are not that good so by default uh you choose uh what password that you want so the default is I set admin and change me obviously because it's the first time we get connected is asking you to change the password now and now I can reconnect here we are so this is the uh this is our very cluster so it's aot now what we're going to do is that we're going to add the nodes so we're going to do a fun part and we're going to do like an ad using the CLI we're going to do a mix of CLI and UI so that you can see uh so what I'm doing is that I'm adding one node right so that's my SEF 02 node and I'm adding another node which is going to be the SEF 03 node which is another IP address and what I do and if we go back to the UI if everything went fine we see that as it comes we should start seeing the monitors the number is actually increasing if I did it right here we are right so the first one the first whole yes we still Zer now is that by default when you don't specify anything it's just like just want to see why is the cluster that's an interesting question so let's see what it says oh we have a clock Cube damn that's the part I forgot to do okay um so something remember we said it's a environment so we need the clock I didn't have that problem when I was home anyway um so everything we do is time stamp right and because I forgot to configure basically crony or ntp on those noes and uh and seph is extremely sensitive to the uh amount of uh time difference between two nodes and uh that's fine it should you know it should leave us alone for what we have to do so now what we're going to do is that we're going to get in there so we go into the cluster view we go to the OSD views and obviously we have zero SDS and what we can do is say Okay I want to make it quick I want to consume basically that's the default query I want to do AAC optim based uh the reason is it's a demo environment I have only one type of device it's all virtual drives you just do that you keep everything you see that at the bottom if you want you can select encryption so you can do data at rest encryption so when you deploy the osds we're going to use locks to create uh lockbox key partitions and we're going to store the keys to open the lock box the in the monitors so when the OSD start if you do that we'll use the key that is in the monitor to unlock the lockbox key so that we can read the key that will allow us to unlock the real data partition for the OSD this way if you lose the key inside the cluster inside the monitor well basically goodbye you can't read the data right so we're just going to make it simple and we're going to do okay just deploy osds everything works Ail on all the that are in the cluster right now we have and [Applause] eaches OPP of PG grouting layer where is a to here that well that guy was so that's the basic and we have like a functional cluster right so we have monitors we have managers we have osds if we have osds we can store data so one of the uh because I have three nodes and I I told the the demon to basically consume all the drives on all the nodes and I have three empty drives on each node so three * Three N so what we're going to do one of the you can deploy air rados Gateway so we're gonna small so I'm going to call it at three I'm going to say Deploy on one1 and one2 so you can also deploy by labels remember when we deployed the the monitors it was by label here I'm showing you that you can actually deploy a demon not by labels but by selecting exactly the name of the host that you want so it really depends what you want to do so we're going to do uh one we're going to do two and we're going to do Port not going to do SSL it's a so the Reas I'm doing uh po 80 is because on that VM that I use for my training I pre I preconfigured the the the of us CLI to go to Port 80 80 and to use a specific access key and secret key I'm going to show you also how you create an access key and a secret key on the rados Gateway but basically that's why I chose for at80 and why I'm going to create one specific access ke so that it matches the VM size we're good so we basically deoy one R get way one on set 01 one on set 02 I got my two then I go into the object Gateway I'm going to create a user so this dashboard user is uh the user that is used by the manager module that is the dashboard so that you can get statistics inside the rados Gateway so that's the dashboard user that automatically gets created when you uh install but we're going to create another one that is going to be just for USA address and that's where it becomes out I said don't generate the keys I'm going to show you the keys you're going to see they're super complicated very secure right no one yeah the first one that grasses I'll go after you okay no I don't want to store that so we created that and and I'm going to show you I also prepare some set of instructions that we're going to demo and of where we are so what we're going to do I'm going to get all these instructions I'm just going to do like so I can recall them from here so if I do an aw3 LS right obviously says no bucket right we created the user but so far we haven't created a bucket so what I'm going to do now is that I'm going to create a bucket so I'm going to use thew I the one that you can basically download from the AWS website that is available on many DRS where you just say dnf or yum up your yum install whatever right and we're going to create a bucket and now if I lease my bucket again well surprise surprise now I do have a bucket oh it's it's like the standard a us uh so it's like a so the way the a works so you can either pass a parameter on the command line or you can use configuration file that's what I'm using or you can use environment variables me what I did because was EAS because I wanted to sh that stuff to my students basically right config and credential so config is where you define the end point so in my case 701 colum 8080 right and credentials is access key 1 2 3 4 5 secret key 6 7 8 90 that's how I got it to work so now what I'm going to do is that I'm going to create uh a temp file which is that I'm going to copy that temp fall into the bucket as the I okay so interesting to know it looks like thatand's reason uses when you don't pre-re the pools the r will create the pools dynamically for you so the first time in my Cas the pools didn't exist so the get the pools so the time to number pgss that's in the beginning if you were to the command second the CP work now already created so how do we look at the pools so well we can do that so you can do from the CLI that's all the pools we have in the cluster and the one the last one we created to receive the data defab and if you want you can actually go back to go it looks so small right so you can actually see that exactly the same uh from that end now what is pretty cool is that we can do something funny we can actually do that we say edit it doesn't want it doesn't want to let me do it so you can actually modify the uh the number of copies that you do on a pool there's one rule is that you cannot change an ER an erasor coding profile for a pool because you decide the number of data chunks and parity trunks so obviously you cannot say like on the Fly change that but when you use replicated on the Fly you can actually go from two copies to three copies from three to four from four to two or from four to three and the cluster will dynamically remove the copies that you don't need if you went from four to three for example or if you go from three to four or two to three you will create additional copies automatically for all the objects that are actually stored in the pool you just have nothing to do you just let it you know just look at the the thing just like doing the job what copies so that's uh you know one one of the things that you can do is like a three uh we're going to show you also just because of Greg I made a few uh I made a few things here I'm gonna be the nasty guy I'm taking risk I'm telling you so I'm doing here is that the first thing I'm going to do if I'm using the wrong keys so what I'm going to do here is that from the C I'm going to tell you know the orror module to basically create one file system remember he said that a file system is a volume that's another name that you can give to a f system so I'm going to tell the guy here just like please create me a volume so a file system called CFS andss if want to be able to make a f system because that's a we need to Ander is now going to have a file system right so we have one file Systema that's the pool that we have okay now we're going to do some more fun stuff is that we're GNA take another risk and we're going to tell the orchestrator module please deploy me an NFS Gateway on top of sefs so that I can access my file system either using the sefs native protocol or the NFS protocol so I'm doing that and what I'm doing after is that I'm going to do a little watch because sometimes you know I'm on VM so the stuff is not like super fast is is going to take a little bit of time to deploy the NFS Gateway so I just added to my instructions a watch that basically monitors the service that we create I can see just like get to one one which means I asked for one and uh I'm going to see it coming once we have the NFS gway running then we're going to be able to do an NFS a mount D tnfs type NFS right so that we can actually Mount the F system and what we'll do is that we'll store something in a file using NFS then we'll Mount the file system using sefs and we'll add something to the file to the very same file that we'll see we'll have like when we do a DF we'll have one Mount point that is with fs and the other Mount point to is ffs no you don't have that so what where was saying is that sefs when you create a f system is able to consume the entire space that you have on the osds if you want to limit the size of someone you use user quoters and the quoter will say that guy you can do more than that with the snapshot questions obviously interesting enough when you do uh SEF in a containerized environment use using the CSI driver that's exactly what we use to when when you trade the PVC and you say I want a PVC one gig right what we do is that we actually assign a quarter to the sub volume say that sub volume is one gab so when you reach one gig G over okay we're one one so looks like we're in business and now that the Gateway is up I'm going to ask to actually export the file system the entire file system I'm not making it complicated after all it's just a demo right to say export the entire file system so I can access it through NFS so that's one command I know yeah NFS export right now I don't think we can still do them in six because that's a version six in six we can do it from the UI in 71 will be the U is like the latest basically I cre systems I just like taking from the root of the file system so my export let me actually see the entire file system the entire SE file system okay and R W readr so I can do something once I've created the export I'm going to create like a mount point and I'm G to mount that dude using an FS hopefully it works that's you see I told you I'm the one that's taking the RIS these guys are just doing slides no host name is the uh the vitual machine SE 701 looks like it doesn't like it unless something is wrong there's no reason I would have forgotten something yes is weird there's no reason it wouldn't work was working the last time okay let's try something else let's see if if we can uh I'm going to create a mount point just want to see if I have the same problem so it's just NFS that seems to ad because I just wanted to make it easier for me uh and make it work is it's not aall no is just listening on the default ports for NFS and U it was working on a single VM on my single VM I haven't retried on the mulm I'll take that another time um so basically could be let's where oh it's running on Z2 okay that's the why Che that okay that's stuff I forgot you see okay so uh we're going to do a touch because we want to show that we can just like touch that file through an FS right uh funny enough if we do a DF right so we have we create like a dummy pool so that we can store nort information and so that's object so far that's what we have now what we can do is that I'm just too lazy to the command been a long day I'm going to store something in the file right so that's using uh NFS so if I just like do that this is my now what I can do is that remember that if I look at my system right I have two Mount points it's the same thing that I'm mounting one through an FS one through sefs right so what I'm going to do the first thing I added content through NFS using the NFS Mount point and now what I do is that I'm adding content to the very same part with this one using the sefs mount and if I look at the content of the file right my first line is the first one that I added us that's kind of of thats have directories that using NL devices or is these that use NL devices it's more like kind of an archive workload you don't care about the performance perance also all the stuff we also you can actually on the directory not only say I want I want this directory to leave on thats right that's say you can also say I want to change the way I'm going to do striping I'm going to use radios object size that are different with a specific stripe unit and a specific stripe length so depending on the workload in some cases you can actually adapt what you're going to do not only the type of devices you use under how you basically and lay the DAT on top of the objects that another thing we can do we can also show you I'm trying to be on time uh to do kind of everything uh block devices so we can create a block device so we're going to call it my image uh we're going to get the auto correct of thatu crazy P scale I'm going to use the name RBD I know up plac group I'm going to do 32 I'm going to do replica applications I'm going to say it's going to be RBD CR rules I don't care and use the default I do not do compression so I create a pool so that's a pool that is going to receive some rbds if everything goes fine so my pool is at the bottom it's empty so far so then what I can do is that I can do images and I'm going to create an image I'm going to call it my image I'm going to put it in full RBD size I'm going to do I don't know 32 well one gigabyte I never remember how you spell that know I'm going to leave the default Flags okay if I list that's the one and I can do an RBD map um and I say my image so the first car uh parameter is the name of the pool I'm cheating here that's why I call the name of the pool RBD because if if the name of the pool is RBD that's the name we use as a default pool name so I I put RBD slash to show you that the name of the pool could be anything different yeah and RBD so we now have one device RBD zero and what can you do mkfs does xd3 RBD Z and we can actually create a file system on top of an RBD and then you can like MK RBD and you do a mount RBD my RBD it's Allin provision yes okay and now I have my RBD right so I can do an LS MNT my RBD and that's actually a file system that on of an RBD you use it like if it was like a regular file system on your Linux system have been have to wrap up that's because those guys do too much time okay but that's all the cool stuff you do ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "GEeodHEOmvA": {"video_title": "Visualizing Kubernetes with Generated Diagrams", "video_description": "Talk by Kevin Howell\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/visualizing-kubernetes-generated-diagrams\n\nUnderstanding the relationships between various resource types in Kubernetes is crucial to understanding Kubernetes in general. It's unintuitive and takes many queries to gather owner/dependent hierarchies and find indirect references. Tools such as k8sviz can help visualize real Kubernetes resources, so we can quickly understand and communicate about the overall state of a Kubernetes-based service. This session is suitable for those with basic Kubernetes knowledge. We'll demo tools, look at some example diagrams, and discuss opportunities for improving diagram generators.", "transcript": "thank you thank you louder how about now all right so uh first I'm going layout um sort of the flow of this talk uh first thing I'm going to do is bit of a kubernetes uh primer uh just to make sure we're sort of on the same page about you know some very fundamental kubernetes uh Concepts then um I'm going to frame why diagrams are a useful tool in actually understanding kubernetes and then we'll look at a couple of different tools look at some examples of um the diagrams these tools produce uh and then just talk through a little bit of general advice and we'll wrap up so first off uh a little bit about me I'm a principal software engineer at Red Hat uh I work on console. redhat.com which is uh sort of management plane for a lot of red hat Services uh and and specifically I work on subscriptions on Console Dot and and I'm I'm the tech lead for uh some of the subscription analytics uh services that we have there I'm based out of Raleigh North Carolina uh this is actually first time I've traveled to the West Coast um so so the weather's been fun for me uh and if you want to get in touch with me um I am on Mastadon as Khal madono and uh my my own personal website is k.net um and just to give another little bit of context uh some of the Technologies I work with day-to-day at on Console dot are open shift you know which which is uh kubernetes and uh Java corcus and a little bit of tecton pipelines all right so I mentioned we're going to do a little bit of a kubernetes primer so uh first uh concept I want to remind folks of that is that kubernetes has a declarative interface and um what I mean by that is is that when you're working with kubernetes you give kubernetes the desired State and you're essentially asking kubernetes to to sort of make it so uh second thing is that um the language that you're doing that in is is this a is declaring API resources so there are lots of different types of resources and we'll get into some of that in a moment but um that is a key you know concept another important facet is is that you know when you're working with kubernetes you have uh distributed workloads so um the biggest consequence of that is that the cardinality of some of these resources grows pretty pretty quickly and then lastly uh kubernetes is container Centric and I say container Centric because there's nothing that prevents you from working with resources that have nothing thing to do with containers even though that is the most um common thing to do and um you know point I wanted to make about distributed workloads and container Centric is is that you know between those two um facets it kind of really increases the cognitive complexity when you're thinking about an application that's running in kubernetes so uh what is also interesting is that there are lots of levels of abstractions and those abstractions have relationships so uh here I show if you have a deployment uh deployment implies that you're going to have one or more replica sets uh a replica set implies that you're generally G to have one or more pods and then pods imply you're going to have one or more containers and so you can kind of see how pretty quickly you know just looking at a deployment you're already talking about four different types of resources and you're talking about you might be talking about a single deployment but you might look at uh you know multiple replica sets and multiple pods and multiple containers so um you can navigate some of these relationships between the various resources using uh the API or you know if you're on the CLI you can use uh Cube cuddle uh so uh here's a here's a quick example I threw together of of uh how to get the owner of a given object so so generally most of these relationships are expressed with an owner reference and so you can kind of walk from the bottom of that hierarchy upwards so just uh to go back for a second if you start with a container and you ask for its owner you're generally going to get a pod if you ask for a pods owner you're going to get one of several things one of which is a replica set if you ask for a replica set owner you're going to get a deployment generally and uh the big thing there you can do that over and over in sort of like an iterative or recursive fashion you can kind of walk the tree that way and that's that's a way if you don't have any other tools at your disposal you can sort of navigate that hierarchy uh if you are using an interface to kubernetes that has a guey uh oftentimes they will expose the owner reference field with a hyperlink uh so here is a quick example of um an open shift uh if you pull up I think in this case I was looking at the the Pod uh you can see that on the Pod it gives a a reference to its owner and you can sometimes also go down a level so um you might be thinking to yourself okay well there is this graph-like uh representation that this sort of apparent in the fact that you have these these relationships that you're navigating and uh hopefully it's obvious that you know when you're looking at a graph that is something you can see visually so there's a pattern that I've seen implemented in a few different tools and it's it's a quite useful pattern and the way the pattern goes is is first you query some resources and sort of gather uh data you're specifically looking for resources in their relationships and then you you use that information to draw a graph and so you're essentially converting resources and relationships into graphs nodes and edges so uh First Tool I'd like to to talk about a little bit here is uh one called uh Kates fiz uh katees viz is is a standalone tool um written in goang uh easiest way to to get it up and running is clone the repo do a build and then um there's an awkward little bit where it uses some icons and if you want to use the icons you need to manually copy those over into the the build directory using um Kate's viz is is pretty straightforward so you you invoke katees viz you give it a name space to look at and then you give it a type and an output file name uh and Kate's viz is really sort of geared towards uh graph viz so you can either output the graph viz source which is that- T dot or uh you can generate a PNG file and all it's really doing is is asking graphis to do the conversion to a PNG file underneath and I did an example with these um you'll notice I I put in my example uh command keycloak system I took the keycloak operator I deployed that and then I asked uh Kate's viz to give us a graph of what that looks like and here are the results this works but it's it's kind of hard to read especially for a presentation because it's tall and you know the screen is wide so there we go so so what I did there uh the magic behind the curtain is I took that source and I I tweaked it a little bit I changed it so that the layout would be left to right and I also increased the the font a little bit so that that's hopefully readable uh and and you can kind of see pretty quickly at a glance here for for a deployed uh keycloak instance you've got a deployment for the keycloak operator uh that has um replica set and then that has pods and then what is also interesting you can see sort of how the network plays into into things and that you'll see like a service resource and you can see what that points back to and seeing this all visually at a glance is is is pretty nice um you'll also see here that there is a stateful set called example- casc that's that's just the sort of bog standard demo uh keycloak deployment created that um one thing that that is missing here uh one resource is not represented is is there's a custom resource for um for uh keycloak instance in general but but just in talking in terms of like raw kubernetes resources this is a good way to to just see at a glance you know what you're working with uh you also see here like the the Ingress that gets created for the um the keylo instance itself so so again you get both the resource relationships in terms of uh workload and then also you get some of the network resources and how those relate back to those uh compute resources so I I quite like this I this is one of the first things that I'll do now if I am working on an application I'm not familiar with I'll run you know this tool or I'll show another one in a moment uh as I mentioned uh it doesn't support custom resources that's this one thing that I think could be contributed back to this particular tool um packaging is another area uh I mentioned before it's a goang project not ideal to have to make and build a tool you want to use regularly um so you know I think it'd be ideal if uh somebody packaged this for Fedora Oru uh you I mentioned before that I edited the uh graph that it generated in order to get something that was a little bit easier to present um layout and font options would would be neat I think there's there's definitely room for for someone to take a tool like this and contribute that back or to build a tool that uses the same Concepts and uh gives a little bit more flexibility um filters would be nice because uh sometimes you're looking at a namespace with a lot of stuff in it and you really only care about um a narrower range of of objects and then uh like I mentioned before custom resources aren't something that this Tool uh supports another thing that's not well supported is um this requires you to actually point the tool at like a live uh kubernetes instance and a live namespace uh it it'd be nice to take a tool like this and teach it to uh look at Helm charts or customize so that you don't actually have to deploy the thing that you're trying to visualize so I went searching uh and found another Tool uh so this idea is is actually pretty straightforward so lots of folks have kind of gone out and they've said I'd like to implement you know quick little hobby project so another one I'm going to show you is uh Cube CTL graph one notable thing about it is that it's distributed as a crew plugin so if you already use uh crew you can actually just very quickly use Cube cuddle crew install graph and you're good to go and uh it works a little bit differently uh the way you invoke it is to ask it to graph a a set of resources so you're actually telling it specifically what resource types you want to see and you may notice if you're paying attention that it does actually support custom resources so I asked uh Cube cutle graph to show me keycloak deployments replica sets and stateful sets and and here is is what you get uh if you ask it to Output graph Biz uh so it's a little tricky to read but um it starts with the cluster so this node is actually the root uh this this is api. cc. testing which is the name of my uh open shift cluster and then you get the name space and and they're using here I thought this was a quirky sort of um convention but they're using uh the edges they're labeling those with the types that uh that the the nodes are naming so so it's a little awkward to kind of walk through but sorry about that okay so uh the other thing that this one will do is it will actually emit uh mermaid diagrams as well and uh I tweaked this one again to do the Left Right layout because it defaults to top down as well um but again um you can see pretty quickly you can get an idea of what is actually present in the Nam space that I asked it to graph so you've got the namespace itself it has a deployment for the operator has uh replica set for that which has a pod which has container here you see like the custom resource uh so I have a keycloak named example casee and that uh itself has a state full set so on and so forth so uh had a few ideas there um I mentioned packaging again I'm I'm big on installing packages at the stro provides uh when that's an option I think it'd be nice to be able to get this into a package that way as well though I will say the the crew thing is not bad either uh you'll notice that these graphs don't use any of the icons um so so it's a little harder to tell at a glance what types you're looking at uh again layout and font options because you know I've I've edited those diagrams and that was pretty fast to do but it would be nice to be able to generate some of these uh in a way that's usable like out of the box and I found that choice of edges uh to to be a little bit Goofy and it'd be nice to have like an option there where you could say I don't want to to label the edges uh with the types I'd rather put the types maybe in the the node labels uh and then you know some general advice uh regardless of of which of these tools you want to try or if you decide you want to find other tools or you want to write your own and sort of follow the same pattern um first one is is don't be afraid to edit the diagrams so so as I as I showed the default output it's useful but it's not necessarily like ideal for presentation um so so if you're willing to just go in and tweak the the diagrams afterwards that's extremely helpful uh I I'd recommend actually trying multiple tools or formats so you you noticed before um especially with the cubec cube cuttle graph that it supports both graphes and mermaid I find the mermaid diagram little bit more readable out of the box so that was nice little surprise for me and uh want to do quick plug here um red at developer specifically I showed open shift earlier if you're interested in trying open shift and haven't yet um you can go developers. r.com and check that out and with that uh I'll open it up for for questions all right so we going to walk the microphone back and forth between questions just for the recording as well yeah I was just kind of curious like if this was a fairly exhausting search for the options that this is what pretty much what you found is that sort of the state of things yeah so I I'll say that wasn't an exhaustive search by any means uh I took essentially and I looked on on GitHub and looked for projects that had uh the most number of stars and I picked you know top two options so so the the Kates viz and cctl graph at least at the time that I looked were were in the top two um I did see there were plenty of other examples out there um again it's a pretty straightforward pattern you're you're basically just querying some data and then graphing it and it's all you know well formed data for that so there's there's definitely a lot else out there um and it's definitely worth continuing to to look and see if you find a better tool and if anybody has has used a better Tool uh I'd love to hear about it don't be shy more questions I can run it's fine all right going once twice all all right thank you everyone for being here please give a big hand to Kevin thank you all and um if you have any feedback for me on on the slides or you know again I'm interested if anybody finds any better tools i' I'd love to hear about it um and there's a couple ways to get into contact with me uh there are other red Hatters giving presentations uh that QR code and URL uh have more details and uh thank you for your time thanks for coming ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "N5UYCkMnQ60": {"video_title": "Power Up With Podman", "video_description": "Talk by Paige Cruz\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/power-podman\n\nCurious about containers beyond Docker? There\u2019s a new generation of containers on the scene, Podman! Supporting secure, rootless containers for Kubernetes microservices, it was designed and built with the cloud in mind. Benefitting from the lessons learned out in the open from Docker, this next generation of containers will quickly become a trusted daily driver in your dev workflow.", "transcript": "good morning scale I am so happy to be here welcome to kubernetes community day today I'm going to talk about powering up with podman I am paig Cruz I work at a company called chronosphere that does open source compatible observability think your Prometheus metrics and your open Telemetry traces and metrics if you are that sophisticated as a part of my work um some of the things I love getting to do is put together workshops and help people build um their first steps and their first experiences with some of these open- Source projects um and as you can see one of the prerequisites on there is installing podman desktop I'm actually surprised I specified desktop instead of podman um but my counterpart Eric worked at red hat and so when we were setting up um all of the workshops for our team he said I'm going to use podman you're free to use whatever you like but like maybe you should use podman so we can have some standard ation I said like I've been using Docker my entire career Docker kind of Docker and I kind of came up at the same time in technology and I thought why would I want to use some other container system that I just don't know anything about when Docker works just fine for me um but I'm one for standardization so I said I will give it a try and I found out that it actually was not that painful to make the switch from Docker to pod man and that's why I wanted to bring this talk um out to this conference because I don't think um enough people know how easy the path is today compared to maybe a few years ago so what is podman um and to be honest I spend so much of my time thinking about observability tracing ecosystems metrics all of that stuff I've really taken containers for granted so I want to start kind of at the beginning with podman um looking at the logo who are they what does this mean how did this project get started well these are not actually seals they are celis which are a mythological Irish creature that can turn from woman into seal um they are shown always as a pod of seals perhaps because pod man is short for pod manager um I really love um I just really love that little um bit of culture there they also have names we have catlen Merr and roshen may not have pronounced all those rights but they have names suies so we can already get the sense that pod man's about running groups of things together as one unit kind of from the GetGo and pod man is not actually new once I started looking into this I saw the first release that I could find um publicly as 2018 so about like six years ago now um podman was kind of released um to the world from there a fast follow a year later with a V1 uh and then you know the pandemic and I'm sure some important things happened but our next next big milestone that I saw um really when I started to to learn oh there's other container run times is when kubernetes removed support for Docker shim and my day-to-day didn't change I continued to use Docker containers locally but I was like kind of given a peek into this world that there was a lot more going on in this whole container space than um me or a lot of my developer friends even knew so following that podman desktop V1 was released last last year which brings us today so excitingly literally looked at the podman blog yesterday which dropped that they are planning to launch podman 5.0 they're doing a major rev from four to 5 so now is actually a really great time to get started particularly if you're a Mac User which is not super the vibe of scale um but I particularly um had some trouble with the VM um with qemu quu I do not know how we say that but I basically had a switch to the Apple's hypervisor it's having a lot of issues with the VM and the good news is they fixed all of that stuff in podman 5.0 as well as made the boot time faster increased reliability and efficiency all great things that we care about cool so the official definition of podman like what is it you will find that it is a dless container engine for developing managing and running oci compliant containers on your or Linux system asterisk podman supports other types of systems but of course it first started with Linux so we're going to take this term by term container of course I like to think of it as sort of a terrarium that is um encapsulated a whole little ecosystem within there there's bugs there's air there's plants growing everything's going you can take that container put it in your kitchen put it in your bedroom um but it's got everything it needs no matter where you take it that's kind of what we do when we containerize or package up our apps with their dependencies and all of that stuff cool so what is a container engine this was the this was news to me um I didn't really think about it because Docker of course kind of presents itself as an all-in-one package to developers so you don't really think about different components apart of containerization but what I learned is basically the container engine is what containerize meaning um it needs to turn that image which is like a tarball of all of the files and stuff that you need into running process that is what the engine part does so it takes your user input it pulls images from Registries it expands or compresses because sometimes these images get beefy um prepares Mount points and manages metadata so that is like the engine part all of the things that we need for this uh to go from a set of files to a running process and now we'll come back to Damon list what does that mean Damons are kind of continuously running programs that go on in the background um Docker uses this to manage container engine duties like launching new containers so Docker has a Damon podman does not have a Damon we'll learn um what podman does is it launches new containers by forking and running separate child processes and managing them via system D not via uh like pod man Damon which is nice because it's one less thing to run one less security risk um and kind of just kind of simplifies things and that brings us to the acronym um open container initiative what is this well this is the group that enforces or that puts together the standards which is what powers the portability of containers containers on their own would not be portable if we did not have the ocii to define a runtime spec an image spec and a distribution spec this is what lets us run containers on all sorts of different machines and um different systems so I did not know about oci I'm very happy they exist um I had kind of just been a little user um just taking my Docker containers from my local laptop and putting them into prod not really thinking about how that all happened so now we can kind of have a little bit more insight into what it means that podman is a dless container engine for running these oci containers but when we talk about podman kind of similarly to how when you talk about open Telemetry you're actually talking about a few different things and podman sort of this umbrella project for a couple friends namely builda and scopio builda builds oci images and you might think isn't that what pod man's doing like what why would you have this separation of concerns there what's the difference but really it's that podman is creating containers for you to to have long lived for your services to run more durable containers say where builda is about that intermediary building step um and I ran across the docs that said think about um build a run just like on your Docker file a run command to kind of create a new layer um and then that that whatever was doing that goes away um where when you're doing a podman run you actually care about that thing running long term and sticking around and then scopio is the piece that talks to the image Registries it gets images um it does the signing it does uh yeah it it retrieves the info it does all of the talking to that external sources for where those images are hosted but I think with everything in life and Technology it's important to talk and ask the question why why does this exist why do we have podman why should I consider changing from what I know with Docker to podman um and really to me it boils down to open source enabling Choice um it is not fun when you only have one dominant player in a space um it is great to have the ability of choice and freedom as a user and as developers it's really nice to have things that are open source that you can extend modify and build on top of but also I like to think about it as um not every use case is supported by Docker what I learned about the history of podman is that there are banks or like really highly secure systems that have not been able to use Docker and get the advantages of containerization that the rest of us have enjoyed um and part of it is due to that security issue with that root full Damon that Docker runs so we have left some people out of the containerization race podman is here um to kind of invite them to the party um and I kind of think about it like you know we have Prometheus for metrics open Telemetry has metrics why would why would we waste time in open Telemetry like creating a whole metric spec and instrumentation and then processing in The Collector well some people may be totally happy using prom the whole way and otel just for traces some people may need the standardization across an organization for all those signals um and want to go all in on Hotel so everybody's got different constraints that they need to work within and so I think having more choices is awesome the second thing is uh podman was able to learn from maybe some of the earlier design mistakes of Docker was able to design container engine in the world of kubernetes within the context of kubernetes it's right in the name pod man um so these days uh it's very nice to get started with podman if you're running kubernetes in your later environment or production because it is what you're running locally is going to match a lot more than say like a Docker compose but we will get into that so what are some benefits of pod man what might what tickle your fancy what might make you interested in giving it a world um obviously the first big one is security and I really wanted to look into this like what is okay what is the issue like what can you do when your brute why is it so bad to have a root full Damon and I was going to list on here all of the capabilities and things you can do when you run as root but I would have run out of space because it's basically uh there's like two things you can't do as root but everything else you can um so that is pretty scary um on the face of it but it wasn't until I read Alex Hope's blog post hiding malware in Docker desktop it's from the atasan security team blog two years ago highly recommend it um it really showed the danger and the invisibility that comes with running a root full Damon so um Alex had said okay we've got bad malware Dosh if we run it um as our normal user we can see our user we see our ID and we see the command that run commit cyber crime. py oh my God that would probably raise some flags at your security uh monitoring however running that same malware script within a Docker container within Docker desktop locally um it actually just looks like everything's running fine we just see com. doer do hyperkit and then just the normal Docker run command so that was the thing that really flipped the switch in my mind of like oh wow that there's a big difference uh between visibility um what's going on between those two options there so really the security posture comes from the combination of dless and rootless containers um and kind of an exercise you can do for yourself is to run a pod man image or run a container and you can see that the root user with inside of there is your non-root user outside of the container so yeah really the the difference is when you launch a container with Docker the docker client communicates to the docker Damon which communicates to The Container D Damon which then launches an oci runtime like run C and then launches PID one of the container I did not know all of that stuff was happening when I did Docker run when you do podman run um the podman uh podman pulls the container images configures the storage launches the oci runtime and then starts the container but as a child of the container and the original container engine command so that's what you get that visibility into so security that's that's a pretty good benefit I um don't have a security background so I love when things are designed with security from the get-go I feel a little bit better a second benefit which you might say is a double-edged sword is that is going to be familiar is to you if you are a Docker user if you've been using Docker for years it's in your muscle memory it's sprinkled across um a lot of people joke that you can just Alias Docker to pod man and that is how some people get started and then they run into the bumps along the way um but that is how compatible they are specifically podman CLI to Docker CLI commands let's see yeah so also just like Docker you will forget to run podman system prune Dy and you will have all of these images stacking up so um not only the compatibility with Docker CLI but kubernetes asterisk um we will get into what kubernetes objects podman supports but it is nice to have podman pods running locally which is going to be a lot closer to what you're running in later environments then say Docker compos locally then at some point we turn it into a pod before staging and now Ops has to do all of this configuration and management of that translation and apds live within the docker compose world I have been there it is very frustrating it just rebuilds that same wall so it's great to have pods locally getting developers used to looking at pod yaml um and kind of those Concepts so of course pods are treated as a first class citizen within podman it is really really nice not over state that I have tossed all of my Docker compose away I'm very happy um pods locally however it's important to note like podman is great locally it is not a container orchestrator kubernetes you are still going to want kubernetes in your life um but you're going to want to introduce podm locally the rough edges here is depending on how complex your Helm templates are or some of the other things that you're doing with your kubernetes configuration um you are likely not writing raw yo I hope not um podman doesn't support all of those things so it's not a onetoone match it is just that it is a little bit better for running a few things um so if you happen to have your hel templates or lots of automation that spits out at the end raw kubernetes AML and that lives in repos that are separate from application repos you're going to want to bring those in bring all that stuff closer so that the opds can get to the raw as close as they can to the raw Amo that is going to be used later just really minimizing the differences between local Dev configuration and later on um I find to be helpful so this section is all about power up the first time that you interact with a new technology is really important um if you have a great experience you're going to keep going you're going to tell your friends you're going to read the docs if you have a bad experience you are probably going to read the docs in Anger you are still probably going to tell your friends not to use this um and that's why I wanted to talk about a really nice easy path for you to get started um specifically from the point if if you are um already a Docker user so obviously install pod man the docs are great um just go ahead and do that um our hello world of course we see our beautiful suies up that's just going to be a podman run hello-world and once you've got that going go ahead and bookmark the troubleshooting page and this is not that podman is super buggy um it is that I think the Linux Community RIT large but also specifically podman has a great community support because this is relatively newer than Docker there are going to be new issues and bugs to run into across a variety of devices and instead of being on your own you're actually a part of this big community that has put at the time of this talk I think 42 different entry level scenarios you could run into with their so yeah can't say that enough bookmark the troubleshooting page and maybe the create an issue page as well and for those that live in the CLI I will just have a small plug for podman desktop it is really nice to be one click away froming in a shell into a running container to you know run some Comm man check something see what's going on um versus having to remember all of yourself so not only can you look at um podman pods but there's also a lot of support for looking at containers within podman Docker Lima kind and obviously red hot open shift so podman has gone a little bit beyond podman and so this might be helpful if you're working with a variety of Technologies and want one kind of interface for it all um yep now you've got Docker containers on your system let's import them into podman um you can just do a regular Docker export name whatever you want the tar to be and do a podman import I would say that's probably the lowest effort way to get started just bringing what you have into podman and seeing how it goes pretty low effort three commands install export import okay we do have to talk about Docker compos and podman you have two options here neither of which I personally recommend um but if you wanted to just um swap out your Docker host variable you can continue to use Docker up and Docker down um but it's going to run through podman comma not everything is supported second thing you could do is install this project called podman compose which the community has brought to life this is not something that the podman team is like managing steering taking taking into design considerations this is something the community demanded because we could not let go of Docker compose I understand I understand but um the problem is while podman supports most of the compos spec it does not support all of it so for those really gnarly nasty complicated Docker compos files you might not be able to just run them out of the gate like this and I think you're honestly better off translating it into a pod yo because podman was designed around pods it was not designed around supporting Docker compose forever or the compose spec I like to use technology the way it's designed if at all possible and so this is an option for you it might ease the transition it might let you experiment without doing a ton of leg work but just know um there be dragons with this approach so let's talk about what you could do instead which is create pods incrementally um so you can just run a pod man pod create and then you can add do pod man runs and add containers as you go as you work on things as you need to rebuild and adjust your code um you can just kind of grab a container take it out of the pod put it back in um everything's hunky dory from there the magic comes in podman generate Cube um and then go ahead and name it whatever you want so podman will take whatever all that info that's running and spit it out into a pod yaml that you can then use going on so if you're able to get the ER compose up and running with podman compose I would recommend just going ahead to generate the cube um the Pod from there and then just use that going forward um but it's super great um because the frustration that I had with Docker is that that workflow of I changed something I need to rebuild and then I got to deploy and I got to check the logs and then I got to bring it down and it's just this really inefficient clunky workflow so I really liked this approach of the the iteratively adding so of course you can run pods without kubernetes that is podman play Cube and then pass in your yaml like I said it is not a one toone with uh the compost spec and it is not a oneto one with kubernetes the docs say that we only support pod deployment PVCs and config maps that can get you pretty far um however I did run into a blog post that said they were able to generate a service. EML from this but it only was using node ports I did not find anything in the docs about that and so again I like to use what's published open and like said to be supported so for all intensive purposes this is um all that I use um within podman and so I talked a little bit about this disconnect of if you've got a lot of devs running Docker composes locally SRE Ops infra devops um sometimes has to do this translation work to get it into kubernetes manifests um well you can run pods locally with kubernetes and that might be a nice happy path middle ground for folks um so you kind was the first thing that was supported that probably has the best Integrations but there's also support for running podman pods within mini Cube as well um something also that they bundled in was the functionality to push images that you build right to those um local clusters which is just one of those nice to have things again like really thinking about like optimizing these workflows that we go through so once you've got all of that going maybe you go to your team and you say I want to try pod man in production I would be curious to see who's running podman pods in kubernetes in production I've not come across too many folks um but I would recommend joining the community sharing telling me about it you've got two places that you can connect if you run into issues you want to share your use case you want to maybe build some other tool or or what have you the first is the podman community meaning um what is lovely is they record it and they have the meeting notes up so even if you miss something it's easy to see what is the latest going on I could watch a video I could skim the notes um kind of whatever modality works for you or of course joining live the second is the podman community kapal um which is more about like steering of pod man designs and any outstanding issues so if you had a super big Bugaboo might be nice to join a cabal and get a quick answer um that might not get lost in all the IM system systems we have these days and I think the most important quote that I came across in the docs was that no contribution is too small even if it is a spelling mistake or a full-blown feature um like I said podman is about six years old if we're going to count from the first public release docker's been around for a lot more than that Docker seen some things we're going to run into issues with podman it is not um perfect we have not found all the bugs we'll say um so it's really incumbent that you become a good open source citizen you tell um you do bug reports or even just like Plus oneing on bug reports um that match your use case um or updating the documentation because when you make it better for yourself you're making it better for all of us so reporting issues and I will say they as far as open source projects go podman has it down pretty well they've got um good first issues they've got a very like streamlined workflow to check if the issue that you're reporting is already existing or if there's being done on it so it's not like your complaint is going to go into the suggestion box that gets thrown in the trash like there are many many avenues for you to um check in if you want more direct communication IRC Matrix Discord slack so so many IMS and um the email the mailing list which is a great one to be on Just for kind of um those occasional updates they've got a beautiful guide to contributing if you um so happen to need to contribute something or contribute a fix um really detailed documentation really have have laid out the red carpet um for people who are not on the podman team to come join and contribute if you don't have a use case for podman today um you can give it a whirl um with my open Telemetry Workshop um there's also I believe a Prometheus and a fluent bit Workshop um that'll just get you running pod man pods like in a normal context and you know also learning bit about observability which is never a bad thing um so definitely check that out if you want the deeper dive you're like how is the networking happening like I need to know more pod man and action is a really great book I actually have a copy here that um I can give to somebody I picked up a signed copy At cubec Con uh so I'd love uh for someone else to take all of the learnings um from that book really really recommend it walks through not only just the how to do stuff which is important part of tech documentation but the why why did we make this decision why did we make this design decision how did we think about what Docker did and think about what we wanted to do differently all of those wise at some point um you will run into like why does this work this way and it's helpful just to know that background so with that um thank you you can find me um I have a Blog I write sometimes but I'm all over the web as pag your duty with an I um and I work at chronosphere and I picked out this really cute tiny tiny TV that you could win um if you enter it is uh from the tiny circuits company who I absolutely adore um and I really think that um somebody's gonna have a a really fun time with this so with that I do want to ask who is using pod man today oh my goodness who is using it in production today do you want to share a little more how long it's that's just the one part that I couldn't I couldn't give to you so yeah i' love to unexpected to be up here uh we use it in production for we we release open source software that uh is monitoring software called pmm and we use podman when we deliver it for our managed Services customers so we have I can't give it specific numbers but somewhere around a 100 okay and so this is for Fairly large deployment so we're very happy with it and it works great and it's open source yes okay thank you podman and prod ction I knew it I knew we'd have somebody um well with that I am happy to take questions you'll have to come up and grab the mic um or if whoever wants the book I'll I'll hang around for a few minutes otherwise please enjoy scale and I will see you around ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "RQWFwZBfGv8": {"video_title": "Terrible Ideas in Kubernetes", "video_description": "Talk by Corey Quinn\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/terrible-ideas-kubernetes\n\nIn the spirit of the presenter's previous \"Terrible Ideas in Git\" talk at SCaLE14x, Corey once again takes to the stage to demonstrate an increasingly prevalent technology via counterexample. There's no better way to learn a technology than to implement it hilariously incorrectly\u2013and thanks to this this talk, you don't even have to do it in your own production environment by mistake.", "transcript": "once upon a time before a douchebag bought it there was a website called Twitter I took to it to make a prediction that 5 years later no basically nobody would care about kubernetes that prediction was 5 years and 38 days ago because the timing gets weird due to pandemic I didn't predict that one as I explained back in 2019 and every year since then my prediction wasn't that kubernetes was going to magically go away I'm not that lucky of a man but instead that it would be irrelevant to most people's understanding of their environments since it would have slipped below our surface level of awareness like the Linux virtual memory management subsystem a couple people really need to care the rest of us absolutely don't and we're all better off for it now given that you're all here to attend a talk with the word kubernetes in the title I think it's safe to say that I was a little wrong or completely wrong with my prediction and so as Penance for messing up the prediction after many years of very studiously ignoring the damn thing the time has come for me to build a kubernetes of my very own and then make fun of it on stage which I've spent the last two months doing uh Pro tip if you ever want to learn something new a great way to do that is to publicly shoot your mouth off about it and then get a conference talk accepted that's called a forcing function they won't move the conference for you I know because I checked but first who here has never used kubernetes themselves okay who here has never heard of kubernetes okay not a strong Amish showing good good okay so what is kubernetes uh for those of us who are oh so blissfully unaware of kubernetes is history it started its life as a practical joke played upon the rest of us by Google's clown division once upon a time they actually would go to market for Google cloud and ask companies hey would you like to run your stuff on Google Cloud they take a look at the application and say wow this won't work very well in our Cloud probably because your engineers didn't build it the right way the way that we build things at Google surprisingly being incredibly rude and condescending was not an effective sales pitch so they stopped telling that story but they also put kubernetes out into the world so that we could start writing things like googlers might uh there were some very smart people there we know this because for a while Google would never stop telling us just how smart they were that's because they absolutely did not have some of the most humble engineers in the world that's a different skill set as it turns out but as a as a compensation for us if you wanted to work there but couldn't get past the incredibly rude and offensive hazing process known as their technical interview the constellation prize that we all get is to pretend that we work at a cloud provider by working on kubernetes in the various companies we work at it it's a practical joke that they put out the rest of us took way too far like that time that they put out a map reduce paper which was a hilarious joke until the other Jokers at Yahoo saw it and came up with Hadoop and then we've had to deal with that ever since same principle applies now I originally when I was imagining this talk I set out to start doing some hilariously bad things inside of kubernetes as a joke like using the etcd control panel to for your production database or using ephemeral nodes to host the control plane itself uh using one giant namespace for everything running it at all on IBM Cloud you know the typical things you never want to do but as I set up my own cluster at home I realized that I have been beaten to the punch by years because the terrible ideas were in many cases expressed in the actual experience of running kubernetes now I am sort of a special unicorn on this because I've been intentionally ignoring it but I'm not a complete idiot when it comes to computers because some parts are missing and I've found that I'm usually not the dumbest person to ever try to do a thing and increasingly when I ask why is it like this the response is it's a really good point yeah why is it like this it's a weird skill set uh for those who have not heard of me before I'm Corey Quinn I write the last week in AWS newsletter that makes fun of Amazon for a living because I have no sense of self-preservation my day job though is I'm the chief Cloud Economist at the duck bill group we're a consultancy that fixes the horrifying AWS bill because it's fun uh that means that okay I know AWS super well let me figure out what it'll cost me to do this experiment on top of AWS because I am legitimately gifted at figuring out how much things on AWS is going to cost so I set out to do the prediction and it turns out that that is freaking impossible now I can tell you what the control plane is going to cost uh 10 cents an hour while the cluster is running now after a year if you don't update it it goes to 60 cents an hour for extended support this is a good thing because otherwise you're not going to update or patch your production infrastructure for years at a time that causes a cost to the rest of us you should bear some of that keep your stuff patched if you want to be on Taylor Swift's Internet it's not that hard I can also tell you what the worker nodes are going to cost that's your choice really however many you want to have choose your own poison Etc but I could not For Love or Money tell you what the data transfer is going to cost maybe it was going to be nothing maybe it was going to be $5,000 maybe I should go to hell and that's a bit of a problem because I have a budget this is one of my favorite conferences and I need to make sure that I budget appropriately for it so I can actually go uh so a new approach was needed because I didn't have a cloud dependency also can someone help me budget my family is dying so I went back in time to my data center days and I bought 10 raspberries pie of varying types that I could Cobble together into a cluster and that was a horrible reminder of of the bad old days uh I had to deal with all kinds of things such as ordering parts the parts don't show up the part shows up and three work in the fourth is defective uh finicky connections that don't quite work having to think about a network fabric uh the small fire it started in a cabinet etc etc uh all kinds of fun things um and there was no Cloud involved in any of this and it been a long time since I'd built anything like that and there was something that struck me about this entire process as I was doing it and comparing this to what my my clients had been seeing and I realize something that that makes sense when you start to look at it from a certain point of view and cuts against a whole bunch of people who have something they very much want to sell you and that I was building a cloud fundamentally in trying to get kubernetes to work yes a small shitty one insert your joke here of your choice about a vendor yeah but it's true because cloud is a the term that is both basically meaningless in many cases means oh it runs on kubernetes well what doesn't these days really because people won't stop talking about it and in this case what I was doing was getting kubernetes to turn a pile of Hardware into something that could be reasoned about in something approaching a cohesive sense and Abstract away all of the hardware that lived underneath it uh kind of like open stack was trying to do if it didn't absolutely suck or vmware's Cloud nonsense back in the good old days I mean remember how when we were all younger we might have pirated VMware and then the Pirates bought VMware and then they're cranking the cost into the stratosphere and no one wants to use it yeah now kubernetes serves a function and it is an important one it abstracts your application away from the underlying Hardware that it runs on well guess what jackhole so does any reasonable cloud provider ec2 will live migrate instances off of failing servers they don't make you do it yourself anymore a failed hard drive in AWS manifest just a very brief latency Spike and it lasts less than a second these companies do their jobs super well I can't outrun AWS when it comes to doing those things well and also other things like giving Services bad names I digress but you start off with all of this Hardware that you have to do with and then clouds can sort of abstract that away with their own version of cloud and then they wind up pouring the kubernetes as another abstraction layer on top of it and you're left there being extremely confused nice vest by the way so when you slap kubernetes into your existing Cloud environment you are introducing a black box of abstraction uh to the perspective of the cloud provider you're running a single application with some really weird Behavior patterns and for all intents and purposes that behavior remains completely non-deterministic uh and oh my sweet stars that love to be chatty so you don't really know what it's doing well why does that matter if it's chatty because that's where I see it in my client accounts cross a data transfer generally costs about 2 cents a gigabyte and kubernetes itself is no sense of Zone Affinity so it'll cheerfully throw things across the expensive link instead of the free thing right next to it and despite me whining about this for the better part of a decade they still haven't fixed that out of the box so it chatter away on your dime I get questions then like so okay why is this bill so high well okay what's moving two pedabytes every night between these two availability zones and the answer is only found through oral freaking tradition unless you're really good at interpreting VPC flow logs which you're not because no one is good at that nonsense it's Arcane it's it's reading tea leaves so okay it it's going from this worker node to this other worker node but what was running on those both at the same time I must consult the bone and here we are so yeah by running on kubernetes you're separating out the infrastructure in such a way from the very fine people at the cloud providers who understand how that infrastructure interacts with different things and you're baking in a lot of complexity it's a choice I am not saying it is a bad choice I don't have the context to tell you whether it is or not but it is a decision that you're making you're giving up something when you make it make sure that what you're getting in return for that decision justifies it now I'd be a little less concerned about this if kubernetes actually did what it says it does on the tin but I assure you it absolutely does not uh I used to be a Cadman before I became a devop which is a Cadman who gets paid more money I have heard the howling of the data center server fans and my hands bear the scars of the rack nuts the lessons of that part of my career are Graven upon my bones I say this because it teaches you things when you go through that level of pain and one of those lessons is you don't trust anything in production until you not understand not only how it works but how it breaks so I did the responsible Thing Once I had my kubernetes to see what happens well okay but who would actually do something like that in production meet Dewey he's the remote hands employee at your friendly local data center uh do he tries hard but he's unfortunately accident prone and he doesn't always understand your company's naming conventions among many other failings of doy uh he assumes you properly label your test nodes with clear descriptions of what they are like this one for example good job Dewey the production database label is how we identify our test nodes you're batting a thousand my boss would like to introduce you to an actual bat but that's neither here nor there so sometimes nodes for whatever reason suddenly all off the network so at 2: in the morning when Dewey rips the network or power out the back of the server like he's rip starting a stubborn lawnmower great kubernetes should probably understand that a node is gone non-responsive and spin the things workloads back up on a different node so I did that and the workloads didn't reschedule elsewhere from that unplugged Noe for several days so okay I reached out to the kubernetes Enterprise support desk Twitter now you see the 46 responses to that I learned some things I learned that kubernetes doesn't work that way I also learned that kubernetes does work that way I also learned you have to do a whole bunch of other configuration to get failover to work properly I also learned that no you freaking don't idiot and the longer the responses went on the more it seemed like maybe nobody actually deeply understands this thing that still gets pride of place in conference talks like this and on resum\u00e9s like yours but I do know that if one of my servers drops off the network at some point and the application is that is running is part of production the odds are apparently terrific that I'm about to get paged because we have apparently learned freaking nothing as an industry about graceful failure modes in the near 30 years we've all been doing this something else I learned as I went is that kubernetes does more than schedule containers it is also a jobs program for an entire ecosystem of companies and Engineers that despite raising billions of dollars in Venture Capital apparently can't afford enough vowels to go around in their names but that's neither here nor there so almost anything you want kubernetes to do it doesn't actually do at least not on its own that's when we turn to the cncf projects it tags in one of these other things to do them for you [Applause] I uh that is the downloadable PNG of the cloud native landscape for those who haven't seen that particular monstrosity an earlier version was turned into a thousand piece puzzle that my business partner got me so I burned his house down okay now I understand and I appreciate taking a modular approach to systems design but this this doesn't just approach absurd it volts over it into clown shoes territory now many of these things themselves by the way are also fractally designed because they themselves are composed of a ridiculous combination of microservices that nobody fully understands uh let's talk about one that I encountered almost right off the bat storage in the old days you would have a big Central hideously expensive multirack file storage system or honestly volume storage system called a sand that Dewey would of course mistake for a urinal and attempt to pee in because again he's not really topped here in employee material these days a bunch of systems pulling together their local storage together to handle this and distribute it out is much more common and a few things provide this functionality I went one that's called Longhorn because I am a sucker for cow themes uh it's kind of like EBS inside of AWS for those coming from the cloud world uh complete with its own terrible naming conventions uh it seems to have give oh that's a volume let me give it an ID that's about as long as the system's firmware dump awesome uh it adds additional layers of abstra in the form of nodes discs volumes replicas volume claims attachments and a pretty user interface so you can manage your storage the way that God intended with click Ops that's the theory so I spun it up and it got right to work doing apparently what it does best suddenly and vi vi itself to death in a constant Loop now it turns out that this was not just me being cursed there there was a low undercurrent of people reporting stories about this over the years oh so opened some issues okay great you're running it on Raspberry's pie yeah those aren't powerful enough closed won't fix okay but the metrics of the node show that these things are all sitting around bored it's not a resource starvation issue well you're going to want the volumes to live on ssds closed won't fix yeah well they already do see oh then then you're probably going crazy closed won't fix and so on and so on helpful group of people over there I turned down the generous suggestion to go buy a quar million doll sand since I was going to use that money to run an AWS managed Nat Gateway for almost a week so a bit of experimentation combined with an awful lot of digging turned out that there are all kinds of interesting other closed won't fix go away issues over the years uh that longhorn is Battle tested and completely B bulletproof until and unless you commit the closest thing our industry has to a war crime and restart the projector all right it had to stop and think for a minute I same projector same yeah and ask it to follow a Sim link or something and then it falls completely to pieces now when I replace the SIM links that I had mounted to the storage pool on these things with bind mounts the problems evaporate as if by Magic well everyone knows that no Professor they don't I promise I I mean I get it though Sim links are relatively recent enhancements of the Linux kernel uh support was added just a few years ago in 1992 so I got it up and running unlike so many people who give conference talks on kubernetes now it was time for me to actually use the thing so did you know that kubernetes has a command line tool uh in a classic example of ing the forest for the trees instead of focusing on the big problems with this tool such as you know Engineers getting repetitive strain injuries from having to type out all the goddamn flags and options it needs to do freaking anything uh the universe instead spends its precious development time arguing about the other most important thing how to pronounce the Tool's name kuyl kubectl Cub cuddle Cub CTL and that damn thing yeah uh I'm increasingly of the opinion that the actual uh cu hctl developers probably don't actually have any friends therefore they don't ever have to say the term out loud so it never occurred to them to figure out the pronunciation guide uh this also it shouldn't really be one massive tool with this many options it really does present as more of an operating system by gmax so tell me what happened in my cluster I ran kubec get events and for those who can't see because of course it doesn't know how to say things concisely uh the events show up from oh there's one that happened 3 minutes 5S seconds ago a bunch 37 days ago one 2 minutes 59 seconds ago 1 47 minutes ago um do you know that it doesn't return events chronologically by default because that's apparently Hard maybe I should check and see if anyone else has ever noticed this Behavior well wouldn't you know they did eight years ago and it's still not fixed um you see that there are 46 comments there on that issue guess what fools those comments are in chronological order I wonder why imagine that the fix by the way which doesn't actually work most of the time is to use a apparently a completely different subcommand and it still doesn't always work it has a different weird order but it's still not the one you'd want and what is this I am this ancient dinosaur who grew up in a time where we used text files to record logs and you would append the latest event onto the end of the text file uh these things will return in any order they want that is not chronological okay I'm done I'm done no I'm not okay when challenged on this the community's response by and large has been event ordering is hard do you want a recurring event to show up the first time it showed up or the last time it showed up in the ordering well I don't know jimothy but I do know that events that happened 4 seconds ago probably shouldn't get massed by things that happened a month and a half back but what do I know I'm not a kubernetes engineer don't worry though there's an easy way to get it to tell you to spit things out in order uh and I'd love to know why that monstrosity is not the default why do I have to remember whatever god-forsaken Arcane syntax that thing is using to spit out the events in order okay now I'm actually done on that but unlike every other talk that I've seen about kubernetes which largely seems to be purely in service of running kubernetes itself let's turn our attention for a hot second in the direction of the putative reason that we're running this thing the actual applications that live on top of it I've got a bunch of them one that I've been looking for for years is called change detection the the doio is the actual official domain it's great it checks web pages periodically and it alerts me if they've changed it comes prepackaged as a container but getting it into kubernetes was quite the treat as it turns out uh here's the config they're broken into five files which you can just decide to do arbitrarily or not but there are five separate components I as a human have to manage for this you've got the deployment which means the container in question the service so I can hit it with a web browser and there's a dependency as well on a volume for the thing that needs persistent disc cool you could put them all in one file and it would be 111 lines of yaml which is apparently what senior engineer means you write yaml great um there are no comments in there because yaml and all those lines are needed they they interrelate with each other but you do have to manually specify it every time you build something like this now to make changes to this service I can either edit one of those files and then apply them to the cluster or I can lose my goddamn mind and YOLO slam that  into place assuming I got the brackets correctly balanced and now the files that I have describing the state of the deployment are no longer accurate the next time they will get applied by an automated system like Argo CD or GitHub actions or whatnot what happens my change gets stomped the thing that this command is R got running will get wrecked and we figured this problem out many years ago with configuration management kubernetes wants to discover everything from first principles so we get to learn it again um speaking of Argo CD for those who haven't played with it it is pretty great if a little bit wordy it's cicd for applications deployed onto kubernetes uh the website says Argo CD is cicd for kubernetes some people like to say it can be used elsewhere but okay those people are are excited maybe the project team lacks imagination for example I've not seen a single thing come out of them called Json and the Argonauts it's a lost opportunity ah there's my Classics friends okay let's talk about Helm which is in theory a package manager for kubernetes in theory Penguins can fly if you launch them hard enough it's very tricky to mod to modify anything that Helm Provisions into your account or your CL cluster and maintain Upstream compatibility with the helm chart which is their term for package great whoever made this chart can set up a list of values that you can override or set to customize the parts that they think should be customizable in practice it means that they lack all imagination like someone naming a service at AWS or ability to step outside of their own use cases it because gets on full display because why would someone want to run any of these applications on I don't know a Raspberry Pi that would be ridiculous sounds like something a lunatic might do let's also talk about the lack of care clearly evidenced by the way some people actually deploy kubernetes in the real world let's say that I'm running a DNS server inside of my cluster bam because I need a database now py hole specifically is what I'm using DNS is a database I will fight you if you disagree on that it's true best database in the world now I use P hole because I don't want my iot network to phone home reporting on things that are frankly none of the rest the world's business like how many times I do in fact open my refrigerator late at night it's remember it's not a dad bot it's a father figure thank you now many production environments also run their DNS servers inside of kubernetes clusters the difference between some of those production environments and my own crappy one is unfortunately that I have triple checked that the DNS server itself is not needed to boot the entire cluster from a cold start now clusters rarely get turned off completely I get it that makes sense sense which has the unfortunate side effect that people sometimes lose sight of the reality that when it finally does happen to get it back up you absolutely cannot have that process depend on anything living inside of the cluster well okay it's not like some of the companies I've seen do this are uh doing that important work I mean one of them was only a bank let's talk about bad job titles for a second there's devops as we said it's like assisted men only you know pays better you've got uh SRE which means that you are a devop in the Mountain View region of California otherwise you're just a sparkling Dev up you've got Dev SE Ops because hey security is tired of being the department of no but security doesn't know how to not butt into things and here we have them there and you have platform Ops which means hello I have something to sell you this week but the absolute worst job title that you absolutely don't want is of course load balancer that's just a thing for Pro for computers not for people please don't make me do that by hand now kubernetes supports multiple Network load balancers itself awesome makes sense most of them expect you to be in a cloud provider but there's one that doesn't and it's called metal lb first released in 2017 accepted in the cncf in 20121 and the first thing on its website of course is a protestation that despite it still being in beta after 7 years like a godforsaken Google product it is totally stable and you should trust it in production at the bank cool it's been stable for me but okay fine it it touches on some yet another underlying problem in kubernetes here specifically that every every load balancer out there is a little bit different in how it thinks about things and how it interacts with the environment and the things you ask it to do it's a classic thing I like to tell people about the dangers of multicloud where you expect that oh everyone has a load balancer yeah but the way that they work the way you provision them the way you call them does change the way they break changes and you have to start planning for that which is why on some level when someone goes multicloud hard enough they build their own load balancer inside of a cluster okay and its interfaces as a result are all relatively undefined and that means that the behavior of each one of these is different and they can only really address in kubernetes the common cases uh Pop Quiz uh DNS servers listen on Port 53 is it TCP or UDP yes exactly the answer is yes because UDP is what thing you query it with and it will respond TCP is needed for Zone transfers also known as database replication or if there's a truncated response over UDP the client has the option to retry over TCP so I tried this to turn both on and it turned out that it just didn't start a UDP listener on the service just TCP I removed the TCP option and then UDP started what the hell turns out if you want both on the same IP address imagine that you need to come up with a specific config for the load balancer you are using here is metal lbs in this case you have to say that it is allowed to share an IP with an arbitrary string that they use as a key on this now if I want to move this service to another cluster with a different load balancer well none of that is going to work and I have to program defense ly around it and it's hardcoded too for what IP address that's going to be so it's it's not portable maybe I'm missing something and how to get this to work properly but this example is copy pasted from metal lb's documentation so if I am wrong on any of this please share it with those folks too it is an issue in kubernetes uh don't worry though this one has only been on the road map to fix since 2016 uh they said okay we got it fixed and having torn through it it's clear the fix did not take for all edge cases like you know a non-cloud load balancer closed won't fix there's a lot of that that goes around okay I'd be remiss if I didn't talk a little bit about observability uh just a smidge in here now what observability is it started off as monitoring and then they decided to fix the biggest problem with monitoring in that it wasn't expensive enough so in seriousness there are some other changes too like okay you like this this container stopped existing 20 minutes ago you better have enough dmetry to figure out why an error happened it became a murder mystery it was glorious welcome to microservices so I instrumented my kubernetes with a few things in this space I use axium for logs uh honeycomb as an otel metric Target that I can just send all those things to grafana or in Prometheus within the cluster itself so I could get a pretty dashboards of my nodes overheating because of see previous comment about small fire great and something I noticed is that since I began this experiment in late January it's thrown a quar terabyte of logs despite not doing a hell of a lot that's almost 130 million events that it's submitted despite again not doing a hell of a lot so that's something that I don't really understand because it never basically never shuts up but also never does a damn thing that's useful raise a hand if you have a coworker like that yeah yeah yeah I've been that coworker for some of you yeah now I before I wind up wrapping I want to make sure here that I don't sound like a total Downer on kubernetes so let me highlight the actual good parts and there are some they're there they're just generally a lot less funny uh for example it's basically the closest thing we have to a multicloud API uh that last load balancer problem aside it works reasonably well at it uh I've been saying for a long time that multicloud is not a thing as imagined by people who espouse it the most but the reality is everyone's multicloud uh people are going to run infrastructure on AWS but if you're doing that you're not going to use their code commit product for git because you have a small problem called still having self-respect so you're going to use something like GitHub okay great you're going to use G Suite or something like that over in Google land and these days you'll send something anywhere that can sell you a uh Nvidia GPU for the right price we've seen that everywhere so that's been that is the way that the world works so every Everything has to interoperate on some level data transfer remains prohibitive egress remains obnoxious but you do have workloads that will need to be deployed in multiple places and kubernetes makes that a heck of a lot simpler uh if any of you have snuck in from Amazon's branding group I know that's not your actual logo in fact all four of these are using older versions of their logos because I'm a small petty man like that I know they replace the box logo with the the a smirking at you that wasn't that wasn't for telling anything um K9s anyone use it oh my God you all need to use it uh it is a text based user interface that has a psychotic enough logo that I didn't have to edit a damn thing to put it on the slide it's great the actual interface is awesome too uh you can drill down into all the different things deployments containers uh pods look at the logs from anything you hit a button it'll automatically do a port forward to your local machine you can hit whatever service that exposes on Local Host it it is effectively an awesome way of just reasoning about all these things and oh my God is there a lot of typing and tabbing between all these different things because nothing is Unified because we aren't allowed to be happy anymore and it is it does segregate optionally by namespace but doesn't require that so you can actually see things cohesively as a unified whole which is a big step because Nam spaces are incredibly important within kubernetes as it turns out because if you if you're in the wrong name space or forget the namespace option it does its level best to Gaslight the living out of you of what pods are you talking about there have never been pods here are you feeling okay yeah it's great it doesn't actually ask you if you're feeling okay because I assure you it does not care it really doesn't it's also and look at the other options too I mean you can let's do a car analogy because every engineer loves those uh you can view Docker as a bicycle it gets you from A to B but there's an awful lot of work involved with doing it uh there's a Docker swarm which much like the Pontiac GTO hasn't been really Advanced at all or made in 20 years uh you've got you've got then on top of that you have Docker what is it Docker sorry Docker compos the compose is still with us swarm is the Pontiac yeah then you've got the Honda Civic hanging out with uh that is Docker composed it's reliable it works it gets you from A to B and everyone uses it for something and then you've got basically kubernetes which is like the F35 uh it's incredibly complicated it can't fly in the rain and it's estimated to cost $1.7 trillion to develop and run I mean I kid but there is another value here too which is none of you would be here if this were all about terrible ideas in Docker terrible ideas in Docker swarm you're here because it's an exciting technology I made a joke earlier about the resume side of it but it's true I was talking to a friend who runs a company where all of their customers are cloud-based but because of the nature of what they've been they run it on Prem and they absolutely call out that hiring is a problem because they're not they're finding people who are excited to work in on Prem environments the same can be said of kubernetes people want to have the thing on their resume ideally that is portable that you can pick up and take with you to another job instead of I got very good at this one company's specific applications and processes around it plus with kubernetes let's be serious here everyone's using something a little bit different you're using different load balancers different storage systems different ingresses uh different cicd pipelines different everything so one person's kubernetes usually looks nothing like someone else's we're all building unicorns on some level I keep waiting for a bit of a thinning of the cncf landscape it hasn't happened yet but who knows maybe they'll just start I don't know taking heads or something over at cubon euu this weekend who knows and there's one more benefit too which is that nobody else knows how to use it either because every person I've really sat down with over the last couple months when I start really asking the tough questions it didn't take long at all to get to oh I don't actually know the answer to that yeah none of us do we're figuring it out as we go along and there's solidarity in that there's there's a sense of Peace in that oh thank God I don't I'm not a complete idiot or at least if I am I'm in very broad very good company this stuff is complicated it has introduced a lot lot of complexity and that is a collectively a decision that we have all decided that we're okay with maybe it works out maybe it doesn't I just get to sit up here and talk because my primary skill remains wearing a suit now later I intend to do other talks in this series uh terrible ideas in eks for the AWS folks terrible ideas in gke for Google Cloud uh terrible ideas in banging Stones together for IBM's version of it the usual stuff uh I'm also going to be here giving an ignite talk about 90 minutes where I wind up talking about generative AI with just glowing things to say about it and of course sign up for my newsletter at last week in aws.com to get my sense of humor like this in your inbox every week because if there's one thing people love it's a sarcastic jackass with opinions thanks for listening you're free to go you're also free to basically assault me with questions but the answer is going to be I don't know I have gen now to wind up confidently being completely wrong I I don't have to deal with that anymore I've been replaced awesome what do you got uh yeah is is there actually a need for kubernetes or is it a giant scam run by devops people it's a fair question the the reason I the reason I believe there is an actual need for this is that I don't know if you met a lot of devops people but they're not exactly what would call persuasive um there are reasons to do it it solves problems it again before this separating out the application from the underlying Hardware was hard to do virtualization got us part of the way there containers made it easier but scheduling them is important needing some sort of overriding supervisor system to do that is something that was no longer optional um I've talked in seriousness with Folks at Google and they said that their Borg system internally is what they use for this and when they started developing kubernetes internally they basically had two two problems one well we can't use all of the other dependencies uh that went into Borg so we have to build things like this for the rest of the world and two yes we can redo the system and not make the same mistakes so instead they made different ones uh that's always the way of second systems the problem that you'll see though is that it's being used in for cases that I find relatively unconvincing or uncompelling uh everything that I'm running on that tiny little cluster would be just as well served as individual Docker containers running on a spare host uh I still have a single power supply into this thing the small to midsize fire is still going to take out the whole thing I'm not I'm not saving myself anything particularly useful there I was mostly able to solve the I can restart individual nodes for patching and whatnot none of the applications seem to notice or care but it took some it took some work and some testing and some tweaking of those individual workloads um I will say to kubernetes benefit there's a strong demand for operational experience in the industry and if you've worked on it in one place you've learned a lot of transferable things not necessarily tied to exact implementations but it shows you know how to think about things like okay maybe it's not all just one giant monolith maybe you have a microservices story maybe you have to think about distributed storage maybe you shouldn't use etcd as your primary production data store etc etc uh whether or not that is the subject of a conspiracy I it's comforting to believe that it is just because it would be nice to think that someone was that persuasive and had the ability to see far enough ahead for that kind of outcome but I don't buy it I really don't sure that's when I pay you to handle running kubernetes for me the question is what does a train look like on that projection yeah I pay someone else to make it their problem the joy of public transit honestly that's kind of what you do with Cloud providers on some level I promise they are way better than fixing failed hard drives than you are and Lightyear is better than Dewey of course de's an action of malgam of like three or four people I used to know at various data centers who uh were not the sharpest on the third shift when it came time to now make sure you restart the proper server or don't like there's a reason that we hang fiber in loose bundles tightening that down for us uh was surprisingly unhelpful now you're going to help us rerun that all the original Call of Duty at 3 A.M that's right it's nagios but yeah now gives it a big problem wasn't expensive enough I have a question no so as a cloud Economist do you think kubernetes is a good tool for Cost Containment since three time do I think it's a good tool for for cost reasons no I think every time you're running it you're seeing that it winds up causing driving additional cost unless you approach it from a the position of cost toare architecture um it is is as deployed in many cases not optimized at all because and to People's Credit when you're deploying an application and building and scaling it you're not sitting there thinking now how can I do this for the least possible amount of money uh at least not the good places and because you don't really see success from that you build it make it work first there's opportunity to optimize later but data transfer remains incredibly poorly understood there's a lot of questions about how densely you pack these things do we go with a few big instances or a whole bunch more smaller ones God forbid you're running anything that license on a perk core count because that becomes nightmarish uh there's benefit yeah yeah or anything that's on a host basis yeah the okay so we're it'll save us money if we make bigger instances yes but it will increase your blast radius should one of them go down how long does it take to get a new new capacity added do you like Auto scaling is great it gives you the capacity you need 20 minutes after you need it so you've got a plan ahead for some of those things Cory what do you got thanks for coming to Pasadena this is my first scale and um thank you yay um what type of companies normally call a cloud Economist like number of people work there and then what's the biggest cost savings you've ever gotten oh geez that's a bit off the beaten path on some of these things um honestly it's more about large it's about large companies spending you know significant piles but it's less about saving money which is what I think something that throws people for a loop it's about understanding and predicting it because otherwise if it just comes down to saving money it's not that hard to do it irresponsibly like backups what are you some kind of a coward or yeah overhead room like anyone go to visit your website it's fine turn it off thank you of course other questions concerns criticism feel free to call on everything I just said by the way yeah the the question was more around challenges about managed services and when they don't work how do you avoid the uh being uh having being finger pointed over to the shared responsibility model that's always the problem whenever there's interoperability anywhere you see it internally in the uh in in companies if you don't believe me go and report a problem with the network to your network team and see how quick for e e e e e e have I tried using gener AI to fix any of this stuff well uh I did ask it a couple of fun questions that were interesting and uh it gave the answer of oh yeah that's a neat part you don't great not particularly helpful uh gen is fun just because it's very good at giving a surface level overview of something but if you start pressing it on something you know about it turns out it doesn't know much and then you start to realize wait a second no one seems to know the answer to that oh dear then you look you forget that you look asking for stuff on something else like well okay so it's terrible at getting kubernetes up and running H now it should really tell me how to perform CPR I need to know that in a hurry yes hit me with it uh who should be making the decision ah dear Lord yeah who should be making the decision to implement kubernetes there um ideally not you and I say that with all sincerity in that you probably don't want to be the person holding the bag when you implement something and it doesn't work out super well spoiler at infrastructure level everything is going to have problems uh usually I've seen kubernetes rolled out because of a new CIO in town trying to drive change in practice the actual thing that drove more uh change in any other six C in a company's history was in fact covid-19 that's your digital transformation that hit but by and large I see it coming to solve specific problems I also see teams going ahead and implementing it because it is the shiny hotness it's a decent way of running things in a repeatable declarative way and that's important I don't have a whole lot of better answers I mean ECS on AWS is great but I'm also not necessarily going to propose things that have no equivalent in other providers so you don't want to have a strategic lock in in that particular sense I get it uh so I feel like the answer is often fine kubernetes I guess uh I don't see people super excited to be rolling it out and deploying it except the technologists that want to play with it but I don't generally want people who are really excited about the technology because they heard good things about it to necessarily do the entire technology selection process it depends other questions comments concerns criticisms um so you were mentioning the data transfer costs being a wild card basically uh have you heard about eks being a hosted kubernetes platform being any better at that since theoretically Amazon should put some magic on top that handles the replication traffic between and the control plane you're right it doesn't charge for any of that but I'm talking where I see it is not the control plane itself being chatty but rather the individual workloads that live on top of it where okay I have you've built this you've broke this thing into microservices because every outage needs to be a murder mystery so I have I have the uh application want to talk to redus that just happens to now be located somewhere else instead of right next to it or multiple things in service Discovery uh R Shambo decides is going to talk to that one instead but the hardest part with things like that is just tracking it down and being aware that it's happening the way to avoid it is multiple clusters one per availability Zone but it runs counter to what most people instinctively think should be done like wait single availability Zone that that could cause problems if AWS loses an a yeah but how often does that happen on a global basis not as often as you're going to trip over yourself because you misunderstood something there again it all depends like in my tiny Twitter for pet scale startups thing I enjoy having uh like I don't need to be multi-az if it can be down for an hour most of my  if my s's down for an hour the internet's a better place for it debatable debatable there's also this idea that a downtime is incredibly expensive for some companies sure but for other companies it doesn't really matter if you're if you're displaying adds to people great uh then yeah downtime has an actual real cost if you're selling something that you're the manufacturer for and your website's down for an hour well a lot of your customers are going to potentially come back after the end of that hour if you're always down they won't come back eventually so it's a sliding scale it's what are the what are the contexts of your business a lot of back office stuff where their only customers are large entities and they're doing batch processing and their responses measured in a calendar still have to sign ridiculous vendor assessments that demand five nines of up time it's really because that's not actually how this relationship needs to work at all what template did you steal this from so I have a question over here me please so I've been working with kubernetes roughly about a week actually so less than two months than you but I I found it that it's that it's a when you use the kubernetes operator for example like the kubernetes Kafka operator I found that it greatly simplifies the deployment of something as complex as that I've done that manually before um um component by component and it it sucked but with the kubernetes operator an install of of a Kafka cluster and all its necessary components was a breeze so I think that's one of the things I'll call you out on it it's actually simplifying the deployment for these type of you're absolutely right I have a tail scale operator in my cluster to put all of the some of these workloads onto my tailet which is great the and in this case it works super well the Counterpoint is is that if I need to start modifying how I want these things Exposed on the the tailet uh I will no longer be able to use the operator past a certain point of configurability I don't I'm not basically a network terrorist anymore but the but for things like with Kafka I could absolutely see a story where for certain tuning reasons I might need to do something that the operator as written doesn't necessarily work for so then I'm left with either doing it by hand or patching the operator like honestly on some point when you start seeing things at significant scale almost everything winds up being patched locally think yeah the scary part is that the big companies they have every team implementing their own operators because they're not allowed to talk to each other but yeah that's a fun what do we got speaking of network terrorists yes um what are your thoughts on service meshes service MCH is how it pluralizes for starters and it's I haven't gone deep with them lately it just isn't something that I want at this scale I don't really have problems that look a lot like that yet and when I looked into it briefly it seemed that everyone was more concerned with talking about how the others suck than talking about the specific benefits that they offer and if I want to wind up seeing that kind of uh kitten based slap fight I'll just go and talk to more observability vendors anything else I should I let you all out a couple minutes early okay if there's no further questions why don't you join me in thanking Corey for this great present ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "TT3POS35dDw": {"video_title": "Vending Machine for Accelerating Data Science Experiments", "video_description": "Talk by Christina Andonov, Apoorva Kulkarni\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/vending-machine-accelerating-data-science-experiments\n\nLearn how platform engineers can leverage Kubernetes, JupyterHub, Ray, and Crossplane to provide a robust data science platform for machine learning experimentation that fosters innovation and drives business value.", "transcript": "hi everyone my name is Christina andonov I'm Solutions architect at AWS and I help customers um build platforms on kubernetes hello everyone my name is Aura Kar I'm also a specialist sa soltions architect for containers I help customers of AWS adopt machine learning workloads on communities let me take you back in time to the beginning of 2022 when all almost nobody knew what chat GPT was and AIML was on no Executives priority list and that time business applications were the priority and organizations had a platform team that created kubernetes clusters and application developers installed application on those clusters and at that time if you got hired as a data science engineer and you went to that platform team and asked can I have a kubernetes cluster to run my ML workloads on the answer usually was sorry can do we have business applications to run instead here are the keys to the Kingdom and other words the keys to an AWS account I'm sure you'll figure it out and those keys don't look um as nice as this one they actually look like this and at that time if you're that data science engineer you would start wondering and reconsidering your life choices and wondering was that PhD really worth it a few months pass by and the company hires a few more data scientists and they also hire an embedded devops engineer for your team and life is better with company and with everybody's help you're able to run a couple of machine learning workloads in that account but it's not easy and still the question like was it worth it and then one day in November that same year everything changed and now priority number one on every executive um was AIML and now that no can do turned into let's see what we can do so let's see what the platform team can do they already build kubernetes clusters um for developers and they also set up the uh add-ons on those clusters including the tooling for developers to deploy business applications to those clusters so now they would not start building clusters from scratch they would actually re-evaluate this platform that they already built most likely in terraform to see how they can repurpose it for data scientists and the first thing they should do in this case is go to the data scientist and ask hey what are you trying to do what do you want to run and the data scientist will say well actually it's not one thing I want to be able to run a variety of experiments I want to be able to uh do data transformation and processing uh inference um fine-tuning large language models and I want to use this open source uh framework called Ray that is a Swiss Army knife when it comes to running python workloads also I want to be able to run multiple experiments at one time so as a platform engineer you can certainly install Ray operator and you can set up the tooling that the developers install the business application with for the data scientist a little differently but they can uh you can set it up so it installs a name space for each experiment and stands up Ray cluster and Jupiter Hub and then they can set as many name spaces as they need that way um so I know I brushed up on Ray operator and Ray cluster and a PVA is going to tell us a little bit more about Ray so uh show fans how many of you have heard of Ray or use Ray in some shape or form to go uh so for the rest of you who are not familiar with Ray Ray is a distributed is a python framework for running distributed applications at its core and it has uh some higher level libraries for uh you to be able to run uh training jobs or to be able to do fine-tuning hyper parameter optimization it has uh an inference uh capability through Library called R serve and uh because it was kind of born after the communities era or you know it was kind of B during the same time out of UCB they built it up ground up to be able to run on kubernetes which is great for us right so now we can run these Ray jobs and Ray clusters on top of kuties and the way it runs is there is a a project called the CU operator which is in the open source a bunch of people from the ray Community came together to write it and CU operator deploys uh three custom resources on communities cluster so Ray cluster itself then there is the R job to run Emeral Ray clusters and then there is Ray serve to be able to create an inference Endo uh for serving real-time uh machine learning so and all of this obviously works across uh any kubernetes environment pretty much and it's pretty easy to get it up and running uh in a quick start way so you can just it's a standard H chart deployment you add the helm repo uh and then Helm install the Cub operator that deploys the the C C RDS on the cluster and then after that you can use the H uh the H chart for the r cluster or you can create a yaml file either way uh that basically Provisions that rate cluster on top of uh on top of kubernetes the nice thing about that is these rate clusters can live in individual name spaces these are namespaced uh scoped so you can create multiple rate clusters uh across multiple new spaces so you have the isolation boundary for that cluster to be able to consume uh resources within a specific name space be uh you know adher to the resource quoda limitations and limit ranges and any security and governance policies that you have within the the confines of that names space so uh a very convenient way to run these on top of commies back okay thanks so now as a data scientist you have the means to uh get yourself a nam space and get R cluster and Jupiter notebook in that Nam space but remember remember that platform was said to run business applications and business applications usually run on CPUs but to run ml workloads you actually can't use CPUs you need gpus for that and that just reminds me about the time when I spilled coffee on my keyboard and I had my computer but I couldn't really use it I went down to the IT department and asked them hey can I have a keyboard and they said well we don't have any in stock right now but you can go search online find one that you like come back tell us and we'll get it to you in about two weeks so I went online and I searched for a keyboard and there was an overwhelming number of um 698 choices and I don't know about you but when I see that my brain goes well maybe I should get a really nice one you know to serve me for the long run so I checked the really nice ones but then I don't want to lose my job either um so maybe I should settle for something in the middle and then still remember like even if I get one I still need two week wait two weeks to actually get it wouldn't it be just nice if I could walk down the hole swipe my badge and get a keyboard of precur rated number of choices that are safe for me for the most part uh and access my computer so if you notice here the precured number of choices well sometimes I would actually need that really nice keyboard but I don't need it for the whole entire month I just might need it for an hour or two so now I have to start adding features to this vending machine also while I'm at it and adding features maybe I should expand the the medium siiz keyboards to have more options and choices there and just pick one that's available or one that fits the best so what would that look like in terms of the platform um how many of you here are using cluster outter scaler okay uh one one two people okay so cluster AOS scaler is how kubernetes is um it's an open source project that can uh you can add manage node groups and it can scale those manage node groups and it does Pi CPU you can also add manage node groups uh for gpus to Cluster out to scaler however what cluster AOS scaler is not aware of is pricing and here remember that price tag we need something that is very aware of the pricing so uh to do that we can replace cluster autoscaler with carpenter carpenter is an AWS um Pro it's a project that came from AWS it is open source it has been open source from the beginning and it recently got donated to cncf sck out of scaling so Carpenter the differentiator factor here why organizations choose Carpenter for running machine learning workloads is two things first Carpenter can bring bring up an instance in matter of seconds versus a few minutes for cluster autoscaler and as I mentioned the other one is definitely price Carpenter is aware of pricing and I'll show you in a little bit how that works uh also to make that cluster speak gpus you you would need the Nvidia device plugin cuz the scheduler doesn't recognize the gpus so you install uh the Nvidia device plug-in and it registers on Nvidia do um com/ GPU type with the scheduler so it starts recognizing the gpus all right so we set up Carpenter we give it some gpus uh if you are looking to move from cluster out to scaler to Carpenter we're not going to cover that here there is this talk happening at C cucon EU next week on Tuesday uh if you're going attend it it's going to be a great one if not uh watch the recording after it's going to be okay so let's go back to our vending machine and see how we can make Carpenter provide these sizes for us so uh before we talk about sizes let's talk about why we separated them in these categories because you could tell me like well right now I can give all the instances to Carpenter I can just have one size and Carpenter should figure out the best price right it's aware of the pricing so it should figure out the best price well the difference why we separated in in um medium large an extra large is because the Nvidia type so data scientists would use a different Nvidia type to run different workloads for example um you can run like data transformation experiments or on a1g uh you can run inter in inference on h100 and maybe fine tune large language models on A1 100s okay so we know now why we have three different categories here so for Carpenter we're going to create three node pools and you don't have to read all that I'll explain it don't worry I know the back of the room can't see this here so we'll have a note pool for each of these categories and um for the category for the medium we have three instance sizes so Carpenter will select the best price depending on what we ask it to do the other categories however we have one instance only but if you notice here on the last line we have a spot and on demand and usually the spot price is better but sometimes times the spot price will Spike above the on demand price and that's when Carpenter would know it will pick the cheaper one so uh we set it we set three note pools but we actually don't apply this to our cluster what we're going to do with this yo files is we're going to put them in a Helm chart and we're going to put like a little IFL statement here with Helm uh the top line is like if experiment size is medium or large or extra large don't worry it will all make sense in just a moment okay so now if I'm that data scientist that yo file that I'll create my namespace with looks like this and it has an experiment size so if I give it an experiment size my tooling under the hood will know that I need to it will create for me a note pool of size medium okay you with me so far all right so while we're talking here and we're building this platform the company hired a few more data scientists and now we have a team be of data scientists as well and they decide that at the exact same time we're running our experiment they will also run an experiment of size medium so now via the tooling they'll have another node pool of the same size so how would the ray cluster in team A's namespace node to schedule that experiment on the first one and Team B is on their node pool well uh there is actually three ways you can do that with Carpenter it's by matching the labels by matching the annotations or the good old tains and tolerations so in this case we're going to use tains and tolerations to do that but you might you can use labels and annotations so in this case we put a taint uh on on those and we said uh the taint is experiment name team- experiment that's an arbitrary name that the team set and then Ray cluster in the configuration will give it a toleration for that taint and it will um run the workload there okay so now the screen is getting really crowded here so we're going to send send team be on vacation for the time being uh and just to re reiterate so now team a gives this very nice snippet of yo I just want my experiment size here's the experiment name uh and here's my team rooll and it will create the name space with the carpenter node pool of the proper size in this case medium so let's use now that uh setup to actually run to see what happens when we run some some workloads okay so I'm going to uh I can access the Jupiter notebook and I will give it um in this case I'll request uh four gpus for my experiment and let's take a look at those instances two of those instances have one GPU each and the 12 extra large has four gpus so also uh one thing to mention here in the ray configuration I have set resources uh I I have set limits and how I want to set those limits is that this part that will come up I wanted to fit in the smallest size in my group but I want to leave some room in there for the demon sets and things like that so I I I'll kind of push it to the max almost to the maximum memory and CPU but not to the max so that's why these settings so now if I run this notebook Carpenter CU I requested for GPU use has a choice to uh like it has many options but one option would be to bring me four uh four instances for two extra larges and fit one pot each that's four gpus or it will have a choice to bring a 12 extra large and to fit all four pots on that one and it will do again that that calculation based on pricing Spot On Demand all the good things we talked about so the great thing here is also that when we created the namespace and we put Jupiter notebook and Ray cluster into that namespace those are actually run on CPUs only the workloads when when I provide an experiment of size when I run a notebook then it brings up the pods it runs its job and then the pods go down and even though I still have my note pull there carpent will take those instances down so I will not be paying for those gpus it's just they they as as the job finishes Carpenter take down those pricey instances all right okay so let's go back to the vending machine and now I have these uh oh sorry I forgot to tell you one thing the maximum replicas uh if you set up uh Carpenter so maximum minimum is zero of course it can scale down to zero but I should put some upper bound here so I don't do you know guard rails good idea in this case 10 okay so let's go back to the vending machine now so we have now all these nice features we can uh use gpus we can bring up name spaces it's going to select the proper uh cost efficient uh way to um to do that well that's great but what if I need a USB drive can I get that well let's go back now to the platform and see um I have this nice way of creating the namespace and Carpenter not pool let's see what it would take the current platform to create an S3 bucket um um so first and foremost that S3 bucket would need a few more resources than just an S3 bucket because we need fre to be able to access it and to set up access today um developers would just use terraform they would either open a ticket for the platform team which would take another two weeks to turn around or they would just go straight to terraform and let's see what uh that process would look look like in terraform well they'll have to write something like this and you don't have to read all that but it is uh not straightforward so some platform Engineers have uh put all of these complexity in the module and even if it's in the module you still have to go and apply it to the account wait for those resources to get created grab the bucket name which is straight forward but then also get the role which is not so straightforward if you're a developer or data scientist pass that into your git repo possibly you know the proper location and the proper value and your ammo file to pass it and redeploy your application so that process is a lot different than what we're trying to get as a vending machine that nice yo if I'm a data scientist I don't even want to know that this bucket exists I just want to use this remember that this is my interface and I kind of like it so far so what if I install a controller to extend the kubernetes API in order to be able to create AWS resources currently we have two options for that one is called AK which stands for a WS controllers for kubernetes and the other one is crossplay so I can install a controller to extend my API uh actually the platform engineer will install that controller you don't have to worry about it and with that controller um with the Nam space creation you can set everything up for that S3 bucket to come and the proper permission so you don't have to worry about like getting an S3 bucket it would just happen now automatically and uh get connected to your rate cluster great so we can get an USB drive now from the vending machine but we've done all this work so far so how about we put a cherry on top of it what that means is the data scientist now has to use a python Library called Botto 3 to access that S3 bucket and Botto 3 is okay but it writes files and read files it's not the best experience there so we can make it a little nicer for developers and we can actually Mount that S3 bucket as a file system while we're at De uh and we can do that with S3 Mount point that came out November of last year at reinvent they announced it okay so now we have the vending machine we have the carry on top and now we're actually going to show you how that works in practice uh this is a recorded demo so I'm going to be pausing it if I need to explain something or otherwise I'll keep it good so just bear with me here all right what where I'm starting from is a a kubernetes cluster uh it's an e cluster cluster but it is kubernetes uh it's got a you know some basic platform components already deployed on it right so if you look at the list right now it's rcd uh which is going to be a way for me to uh uh pull down get manifest and apply those communities manifests uh on my communties cluster uh and there's a bunch of other demon sets uh but the one to notice is Carpenter uh the Nvidia device plugin the S3 m point which chrisa talked about and the cubay operator uh so all of these is are the like the building blocks of my data science platform all right it is I think uh a two node cluster so all of these components right now are running uh across two uh uh no two two instances in the node group right so now let's take a look at uh uh going back to my UI right now Aro CD is running but there it's empty there's nothing running on right it's uh waiting for applications to be uh provisioned uh it's actually monitoring this repo uh this is my vending machine right this is the repo where I will be asking my internal team members data scientists to open a pull request with that yo file that christas should and they will uh open a PO request with with that yo I as a platform engineer uh can approve and mo that P request or decline it uh hopefully approve it the idea is that this is the only interface that the data scientists have to learn they just have to know how to open a p request and ask for what they want right so oh look we already have two data scientists asking for uh uh J notebook and R clusters so we're going to merge a pull request from Team a uh they want to be they want to onboard themselves onto the platform they give me uh an IM AR which is the way they uh access AWS right they that's the role that was given to them by the cloud governance team or the cloud uh CCO or whatever uh they use that role to access a accounts they give me that role and they tell me my experiment size is a medium I'm not doing much but I still need an environment to do cool let's merge that pull request and now when that request gets merged Argo CD notices that there is a new environment that needs to be provisioned uh for this particular team so in a few seconds Argo City will reconcile and start deploying that team's resources and here we see we see Aro CD uh creating that application this is the app of apps so to speak so under this app we have multiple hel charts being deployed onto the cluster and we'll get into that a little bit later but you can see a bunch of resources are being provisioned and all of those resources maap to what Christina showed earlier with the diagram it's the Jupiter Hub notebook the a cluster uh the S3 bucket the IM roles that need to be provisioned to you know access that S3 bucket the all of that you know complexity is completely abstracted away from the data centers they don't need to know all this is happening behind the scenes right they just open a pool request and they mer and we merged cool so now that you know uh the these resources have been provisioned remember there's Carpenter running on this node and we need to Pro provision additional E2 instances to be able to accommodate all those incoming pods so carpenter has noticed that these pods are are need to be provisioned it goes ahead and and Provisions an E2 instance type and then once those instances are ready uh we can see that in the team A's namespace uh they will be scheduled on those newly provisioned instances so all of this wiring is kind of what's this this vending machine is doing right so now that we have the environment up and running we can now give the URL for Jupiter Hub to the data scientist and they can can log into their uh notebook instance they didn't know what just happened for all of this to basically you know just to give you a notebook right so they spin up the notebook and we have some precured notebook containers that they can use these are approved notebook containers from it and appsc and whatever security governance controls you have they use that notebook to be able to run their experiments for my example uh I'm running a simple data transformation job within the notebook if you uh our data scientist or have worked with notebooks before this is probably going to look super trivial to you for me it was a lot of work to actually get it to work but I'm pretty proud of it uh pip installs a bunch of dependencies that I'm going to need to to be able to run this experiment uh after that I'm going to be like just creating some staging area for my data set to land right so I I'm going to be pulling in a sample data set from the internet in from the public domain to run my experiment and uh remember we have S3 Mount points now so this mountpoint is being used as a file system within this notebook so whenever I'm pulling down a large data set it's going straight to my S3 bucket I don't have to worry about AWS S3 CP star. CV to like a pocket URL right I'm writing it to my local file system I'm using p Arrow to read that irs. CSV data set it's a very like commonly used data set it has data about flowers their different lengths and widths but what I want to do in my transformation is to calculate the area of those flowers or something so I have the length and the width I do a product of those and that's the area I want to add that area as a field inside that data set the newly created transform data set so this is what's happening in my transformation script but I want to run this across a set of in a distributed way using Ray cluster uh I want to create a batch of this Chunk Up the data and have these Ray workers uh simultaneously work on these chunks of data and then in my notebook I'll gather all that data back and concatenate all that all of that into you know my transform data set so that's the ray uh script that basically does that job as submit the job to the ray cluster uh the nice thing about Ray is that any dependencies that you need during the runtime of that script can be dynamically installed onto the ray cluster using just pass in an array of your pip dependencies and Ray will dynamically just install those uh as a Spenser uh I've also copied my transformation uh the python script to an S3 bucket which gets mounted uh inside the a cluster as well and remember I'm just writing everything locally I don't know that it's Landing in an S3 bucket it's just ra it's just picking up because it's the same bucket that's mounted inside the r cluster as well I submit the job the job's going on and now I'm just monitoring the progress of the job with the simple loop I can just keep pulling what's going on uh it also uh forwards me the logs from like the jobs that are running so if there's something going on that I need to take a look at I have the logs in the notebook but if I want to dive deeper Ray dashboard is always handy so I can open up the r dashboard within the browser and if I want to Deep dive into like if something went wrong or if some logs are not showing up in the notebook I can always do that remember Carpenter Ray worker needs a node an easy to instance to be able to run on Carpenter has noticed that Ray worker needs to be provisioned it needs an E2 instance it's a medium sized experiment which means it's going to run on CPUs and so it's it's figured out that I can do this on a C6 a a large x large instance and that's what it thinks is the our best choice at the moment so it goes ahead and Provisions that and the a worker uh the a autoscaler brings up the worker pod the worker P gets scheduled on that instance it's up and running and it's going to go ahead and finish that transformation job across those Ray workers that got provision so the job succeeded I can check uh the logs if I want to if not I can go back to my notebook instance and I can test uh whether the job uh the transformation uh actually happen the way I wanted it to happen or do I need to run it again so I need to can like concatenate all of those chunks that were uh you know performed by those individual workers into a single CSV file and let's check so yeah it works so the the pedal area uh ctim squares field was added to my newly transformed data set so now that I am able to oh yeah this basically is again to to call out that the the newly trans data set is also Landing inside the S3 bucket as well but I'm runting it locally on a file system just to prove that I'm not lying it's inside the S bucket so you can see the uh the Rob data set and the transform data set and all the partitions everything is inside the so now if there are other jobs that need to if there's a glue job that needs to take a look at it or there's something else some other team that needs to access that data it's already there right we have one more team that we need to land onto the platform and this time this team B wants to do a batch uh entrance uh using gpus so they have asked for GPU medium again same configo with the principal AR and the uh size of the experiment uh merge that put request now Team B uh resources are going to get provisioned onto the onto the cluster again Argo CD controller reconciling notices Team B has landed and starts provisioning uh the resources that Team B needs inside teams B's resources team A's resources are still there on the cluster but they are left untouched team a is Off to the Races Team B is spinning up uh same same deal it's the same cycle repeating itself right so we have pods Landing into the cluster Carpenter noticing that needs these in these pods needs e instances let me go figure out the best instance that I can get at this point in time spot or on demand and it's going to once the instance is provisioned kuet scheduler Provisions those pods onto the newly created instance for Team B this time carpenter has chosen to do an M5 ax large uh because that's what it thinks is going to be the best choice uh remember the the the taints that was disc discussing earlier uh this is the team B staint so no other workload that has uh that doesn't have the Toleration for the staint uh can land inside uh inside this note uh Team B's jupyter notebook Ray cluster spinning up uh it's the exact copy of Team a but a a more larger size because they have asked for a larger size and they're going to be charged for it looking inside the ray cluster uh just just a sidebar here uh to to to see what R cluster looks like there's a head group node and a worker pod the headp really doesn't do much other than uh maintaining the cluster membership and uh and directing like the the the jobs themselves is the workers that do all the work so we have in this case we have uh configured a Max of 10 replicas main of zero so it has the ability to scale down to zero if you need to keep a worker alive just bump up uh the the main replicas the replica desired replicas to whatever size you need and we have resource limits defined per worker so every worker gets a Max of seven CPUs 30 gig of memory and one GPU each so if the data scientists need uh more than that then they need to ask for a larger experiment size that they can work with all right and now the ray a scale comes up we can the data scientist can again from theme B can go into the jupter notebook this time is Christina uh notebook spins up one thing to remember is we asked for a GPU uh instance steps and we're going to see like how that's being uh handled here I'm using another notebook here for for prod type uh this time uh similar to the previous case I need to install a bunch of dependencies so same deal like it's it's the a package um creating a staging area from my data set uh to Landon uh again it's the S3 file system that's getting mounted inside the notebook maybe skip a little all right so in this case the the the ray job that gets run is an image classifying job so we're using uh an ml model that can classify uh a data set of images and add the score and the label on on what it thinks is inside those images uh but this job uh according to this data scientist according to chrisa needs uh at least four workers uh with a GPU each which means they needs total of four gpus uh for it to run efficiently after the predictions are being made the batch predictions are done I want to take a sample of those and and just experiment with them inside my notebook right so this is what's happening basically take uh a sample create a data frame out of it uh stick that Pand data frame inside an S3 bucket which I can then load up inside my notebook again uh submit the job uh my code is in an S3 bucket again which gets picked up by the r cluster and uh dependencies as well for the for the script to run I submit the job job runs skip ahead it's running now that the job has finished uh I can test whether it it's successful or not so I I can install pandas locally in my notebook and and uh oops check on it remember we said Carpenter takes Those sensors away my experiment is done I'm done with testing my data and so I don't need those gpus anymore I don't need to manually shut them down carpenter knows there's no more workers running on those gpus and it's going to spin down those instances it's similar to Cluster autoscaler but it's a little bit more quicker and it's it's more smarter about how it's doing this uh and with that again to prove that if I was lying there's an S3 bucket with Team B and all of that data is in there all right I'll hand it back to Christina you watch that thank yay the demo worked was it's okay that was an awesome demo and if you notice the first experiment running CPUs to be even more cost effective which we didn't cover in the slides and that's what the risk is when your Co co- speaker really likes to play with that stuff uh so I think the first demo there was a smaller uh model that would fit CPUs so we added even like a smaller subset okay let's get back to this so now we have our vending machine we know how it works we have all the bells and whistles and cherry on top here and now if I'm D that data scientist and if you ask me that PhD totally worth it all right so if you want to find out more uh we have data on eks that has a lot of examples uh for Ray spark and more go check it out Trum and thank you so much I'm question here we go all right I'm gonna steal this and run it for questions so we can get it on stream anyone have questions any comments my experience with Jupiter Hub and python modules has been pretty timec consuming getting all the dependencies and um people I work with have resorted to venv for uh compartmentalizing all the necessary modules they need how does this deal with a dependency stream that might take 10 or 15 minutes or may even be impossible to resolve when you spin those up uh what was the tool you said V andv virtual environments command to okay I'm not familiar with that uh I don't have a good answer for you I just say that most of the customers that we work with are in the camp of Jupiter Hub or Jupiter notebooks so your question is how do you handle python dependencies how does the product hand so this is not a product we are suggesting a pattern uh that customers uh or you know any any platform Engineers can use to deploy uh data science related components on top of kubernetes uh to make it easier for data scientists to be able to do their jobs without dealing with the complexities of kubernetes or a cloud provider uh if there is some other tool that deals better with it this pattern will work hopefully with that tool as well this is not about handling software dependencies at the data science level like we're not dealing with that issue at all um I'm not too familiar with the S3 mounts but I'm wondering how you handle um bucket policies when let's say somebody wants to write data to a prefix they have no business writing data to how do you handle um I guess this permissioning when you mount that data locally so the question was with that S3 bucket we mounted regardless of the S3 Mount point with or without how do we hand them permissions to to that bucket in this case because uh in the example we showed um that data scientist from the name space would have access to everything in that bucket because it's a it's a bucket per experiment so we're spinning a new bucket per experiment it could be um set up to where you have a prefix per experiment that would work too for a single bucket in this case we use the controller uh either AK or crossplane that wires up the Ursa which stands for IM roll for service account or it can also do uh the new uh way of doing permissions which is B identity in order to access the bucket or a prefix in the bucket so the controller wouldn't just create a bucket it would create the role the policy and it can also link like it can take the bucket and put the bucket into the the the bucket there in and like if you design your controller API to do a prefix you can even pass it a prefix and then let me just go back to that slide real quick here it's going to make more sense let's see all right there we go so the controller will set this up for you so it will set all the permissioning it's going to put it's going to annotate the service account with the proper annotation so it has access to the role that will then assume the policy and then we'll have access to the S3 bucket but your question is that you want them to have only some access to S3 right like here's yeah all all of the teams are going to use the same bucket or something and they all get their own prefix under that yeah that that Mak feel safer yes yeah so if you want that you can set up the controller so the controller has options uh and depending on which controller you use like for example let's go with the example of crossplane crossplane uh you have the like when you install crossplane it comes with the S3 API but then it has a construct called uh compositions and xrds with which you can build a on top of that S3 API you can build another SPI API on top of your API if does that make sense and then that your API can be specific you can say I can only pass in a roll and I can hardcode my bucket name for this cluster and um I can pass it a prefix per experiment and then it's going to come here and it's going to set up your policy only for that bucket only for that prefix do you know how the new um S3 volume Mounts work with principles because at some level my principal won't let me write if I say I don't get to write to this prefix and then I mount the file system with the S3 file system Mount does it show up for me does like is it just a readon file system I I haven't played with the file system yeah if your principle does not have access to the S3 bucket then you're not going to get it I can still I can still Mount the file system I just won't be able to right to it or maybe not even see it and so that might be the best way is just in your principal when you mount that file system just say hey this principal can't map to this prefix yeah yeah does that answer your question yes okay any other questions all right and I'm sure you'll both be around at least for a little while I know you have to get out to Paris uh sometime soon we'll be around for for the yeah thank you so much ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "btpq6brm13g": {"video_title": "Back to basics, getting traffic into your Kubernetes cluster", "video_description": "Talk by Nicolas Fr\u00e4nkel\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/back-basics-getting-traffic-your-kubernetes-cluster\n\nHowever you're using your Kubernetes cluster, you'll sooner or later need to direct traffic into it.You're spoiled with choice. Kubernetes provides no less than 3 different objects: NodePort, Ingress, and LoadBalancer. But LoadBalancer requires a dedicated implementation provided by Cloud Platforms. Moreover, Kubernetes is introducing a new Gateway API, adding one more way to direct traffic to the cluster.In this talk, I'd like to offer an overview of all options, with their pros and cons, and a more in-depth explanation of the new Gateway API.", "transcript": "hi everybody thanks to be here for this talk um this is part of a tour so since three weeks I'm touring North America if you want to tweet or to use mastor blue sky or whatever I would ask you to use this hashtag um there is a trick among speakers that normally you would like tell an anecdote to connect with the attendees at the beginning of the talk I generally don't do that because nobody cares I mean you are here to listen to the technical stuff but today I want to make an exception this is my second time at scale yeah thanks but that's not what I want to tell you the the the first time I want to tell you about my first time at scale it was part again of couple of like presentations in California I was supposed to do two talks in Los Angeles one to at scale then in the same week I would fly to Romania and then on set Friday to fly to Turkey and then Monday afterwards to fly to Australia which was pretty stupid but hey and it it was the week before covid so of the two talks that I supposed to take place in San Francisco one of them was cancelled the other I did it online in my room I was in the other building there were 10 people then Romania called and said it's cancelled so I rebooked to Turkey and then they cancelled too I was able to fly back to my airport on Saturday and on Monday everything was closed the whole world closed uh I live in France so nly I didn't leave my place for two years in France the lockdown was very strongly inforced you could go outside for like I think there were five reasons one of them were groceries but he just wanted to go outside to take a walk it was 1 hour and not 1 kilometer further away from your place so for me scell is like reminds me this so I want I'm very happy to be here again uh because I want to exse this memory from me so I hope that it will end up much better I'm Nicola Frankl I've been consultant for 17 years I was in projects I was in different positions and then if you are doing project as well you know about deadlines changing requirements sometimes toxic management so I decided to live this life and now I'm a developer advocate so as I mentioned it's a basic talk so I want to bring you back to how does it work at the most basic level so this is kubernetes this is uh the definition of a pod object and the beginning I will go through a demo at the end unfortunately I have tried it and it takes too long so I will do the slides unfortunately but let's start with our cluster since I don't want to do like to to depend on any cloud provider I'm using kind so Docker and kubernetes and here I want to create the kind cluster so I just copy paste everything and now that's the beginning of the problem yeah okay so my ID I want to show you the config my idea is to have two nodes I want to have a kubes cluster with two nodes so I cannot use the default Docker one for reason that I will show you afterwards so there should be something like this I should have two notes first thing first I will deploy my first node amazing before doing that I need to preload the images that's how it works and because I'm super lazy I will replace Cube CTL with K because so I create the deployments so randomly because I have no Affinity or whatever kubernetes will assign my P to one of the nodes and then I can query the nodes and I can ask about its IP ah yeah yeah yeah yeah sorry you that's very good feedback sorry so I have asked thanks thanks for your feedback [Music] um sorry if you are in the back you cannot see everything here it should be better um so I ask about its IP and it returns me its IP okay because I don't trust it perhaps I can ask not kubernetes but the Pod it's itself about its IP so I go into the Pod I exact and give me your host name it's still the same so now I'm pretty sure it's is IP so great the problem is I cannot rely on this IP because in kues as you know the pods they are they are kettle they are not pets they come and they go so if I delete the p and I make sure that I have no pods no I still have a pod it's just that kubernetes because I used a deployment recreated the Pod automatically but it's not the same pod if I ask about its IP the previous IP was 1.2 this IP is 2.2 so the problem is that pods we cannot rely on the IP of PODS they are ephemeral and the IP as well they will be assigned randomly so in order to compensate we could ourselves by the way we could ourselves like register some controller that listen to pods coming and going and then keeping the list but in order to do that that would be a lot of issues and kues allows us to do that through the service so there is service object and now we can get the service and the service itself called Enix here as a stable IP and this IP regardless of the Pod is stable across all deployments so this is good unfortunately this IP is internal so we we solve the problem of stable IP but it's internal so let's delete everything and now I need to deploy something with a deployment just checking that it's on the right Branch yes so here I will have two pods I will have one uh sorry I will have one pod and it will return me it's an engine exp spot it will return me the host name and the server and the IP through the service and now I change the service to a not Port so if I apply it and I curl it that's pretty good so now I have a stable IP to call I can Cur the stuff because I'm using kind I have mapped this port it goes to the service and it goes to the pod that's pretty good if I remove the Pod just like as before kuties will notice that the Pod is missing and because I have a deployment underneath it will create a pod to replace it and if I curl it again I I have a different IP but it's still there so because I'm using a service I don't care which pod it is I'm just accessing whatever the service gives me that's good we are accessing our pod from outside the cluster now we will scale it normally because I have sorry because I have two nodes I should get one of each of them so one pod should get should go to one of the nodes which is the case here and the other should be to the other noes so if you have any experience with kubes what do you think you should happen if I curl repeatedly the service how which pod should be should the traffic be forwarded to yeah it's about half half right good now I want to talk to you about the next service the next sorry the next object so this was our thing the service the not Port does load balancing but you know probably there is an object called load balancer which is funny and the idea of load balancer is not that its feature its main feature is not about Lo balancing it's a very badly named object and it was to compensate at the time that cloud providers wanted to give you more features and there was nothing at the time so the idea is there is something called this load balancer and you should replace load balancer by specific cloud provider stuff as I mentioned I don't want to um depend on any cloud provider so I'm using something called metal lb which allows me to install a load balancer object on my machine normally if I go there I should have something here so I can get some crds out from the internet and now I can install metal lb itself we will wait to bit until it's considered okay normally 20 seconds time out should be okay I need to check if the range of Ip is the same as in my so it should be yes okay that seems to be good timed out let's see I'm still waiting for one of them okay so this metal lby stuff is very specific that is not important this is just for this kind of load balancer what is interesting is this one now I can replace the no port with load balancer and normally I should have the same things as before so yes we also doing load balancing also through a service but now we replace not port with load balance what do we need to do more the thing is at the moment all of our pods are the same what if not the the the the pods they are not the same we need to somehow route some requests to some pods and some other requests to other pods and that's the reason why kubernetes provides the Ingress object that's the idea to do some routing there are lots and lots of implementation of Ingress on the market so I'm using api6 in general because I work for AP ii6 but there are lots of them probably I mean I I couldn't list all of them um just two slides about ap6 you wanted the previous slide so I give you the previous slide yeah that's customer service um you also wanted the previous slide okay that's again customer service everybody has taken their picture now good um so ap6 is uh an API gateaway I I assume you are familiar with the apesi foundation right um You you might not know that uh in order to give a project with the apesi foundation is a multi-step process so first you give the project you give the IP you give everything and that means at that time you you join the incubator and in order to be considered a top level project to be like full fully mature fully adult you need to fulfill a couple of steps one among them is there must be committers from different companies so um it's a pretty like old project not 20 years but already like four four years old uh it's based on engine X so very stable very major project very good reverse proxy who knows about engine X yeah um the biggest flow about engine X in its open source version if you need to change the configuration you need to switch it off and on again which is not super great if it sits at the entry point of your information system so for that there is something called open resty open resty allows you to change the configuration through Lua code and so you have solved the dynamic uh reload problem thing is open resty configuration Maps like quite one to one to engine X configuration so for big project it's not that great ap6 provides you with like abstractions such as what is a service what is a route what is an upstream and everything is plug-in based that were my only two marketing slides um so I want to talk to you about Ingress when you deploy an Ingress actually you do deploy two pods and two Services the first one is sorry yes um so here you you deploy the Pod of the getaway itself and you deploy your own pod and so you will have a couple of additional steps so when a request enters it you will first hit the Ingress service that will direct forwards the request to the Ingress pods and the Ingress pod will'll probably forward it to uh the service of the real pod that you want to hit and goes like this so for this I unfortunately won't do any demo because I've checked and it takes a bit too much time so unfortunately I I won't do the demo but the idea is you are able to actually do some routing you can and you probably will use different Ingress instances inside your cluster each instance can actually use a specific implementation that's pretty good but there are issues with ingresses I will use the simple use case of API versioning so you probably are using some kind of apis there are three ways to version your apis through like a prefix path-based through a query parameter to be honest I've never seen it but well you can do it or through a header that's a very good use case for routing right we can want we want to like forward some V1 to a specific service and V2 to another service but the problem is imagine we want to have path based routing which Ingress controller supports how do we remove the prefix because if we forward V1 SL whatever to the Upstream then the Upstream will receive V1 whatever and we just want to receive whatever the Ingress controller by default doesn't offer anything to remove the prefix okay what can we do well it depends on your implementation so this abstraction actually is very lightweight in order to kick in the features of your implementation you can use annotations so here I'm using ap6 and you see that we can rewrite some stuff problem solved good but unfortunately it's not an abstraction anymore since the idea behind an abstraction is I could easily move from one Ingress controller specific implementation to another one but here I cannot anymore second issue it only supports path based routing if I want to use header based routing which is not a bad IDE I cannot anymore that's a second problem third problem imagine that here you have your Ingress and now your developers they want to update this and your company is big enough you have different teams that run to route to different Services they are all working on the same file they are all working on the same object from an organizational point of view how long do you think it will take until somebody overwrites something that another team wrote well probably the biggest the organization the more people are working on the same file the less time so for that we have the solution you have custom crds you have something for example called api6 route so you install the CRS you use the namespace and then every every team can use use their own routes or even better you can have one route per file or one file per team and then you have all routes inside what I told you about migrating from one Ingress implementation to another it was hard before now it's even worse so that's not great to compensate with all those issues we have the getaway API who was in the the other room at the previous talk okay I will be even I I I I will only talk about a little of what was talk in the other room the others talk was much more advanced on this the IDE is to fix the problems of the Ingress controller not all the problemss but most of the problems for example we will split the Ingress into multiple objects we won't have one single controller with all the routes beneath we will have different objects we will widen the features offered so that we can do query based routing or header based routing and lots of other stuff what I like in this getaway API is that it tries to model how people are actually working how teams are organized in general teams are organized there is one team that provides the whole cluster and that they like these guys they configure the Gateway or the Ingress and then you have apps team that manage their own stuff and so for every level of responsibility there is one object so there is one getaway Clause to be honest there was also an Ingress Clause before but now it's modeled and there is the Gateway that you can let your team dedicated to configuring the Gateway handle the Gateway and finally we have the HTTP route so previously sorry this was a crd that was specific for here API 6 others might have the same object or not but now we have dedicated HTTP route object and then I was telling you about routing so now we have path matching we have header matching and we have query param matching so every single issues that I mentioned before the use case about versioning where previously you had no choice but to use path based now you can use whatever you want we have the regular name space so at the moment we still need to install crds but those crds they are part of the like kes. namespace they are not custom to one product and HTP routes here does the exact same that I did before here I just for fun I use the query params so now if I have the query parm with name V and value one it directs me to the backend V1 but we can do better no rewriting before unless you used custom annotations now we have filters so we can do Ur rewriting either pause based or fly rewriting I told you about the multiple Ingress controllers it's like the same so the idea is it becomes closer to R abstraction where you can actually use an object use the specification and then you can more or less change the implementation but be be careful it's still not all implementations are the same at the moment because it's still a new specification so for example here I won't lie to you API ip6 is lacking in some ways the reite rule is not supported you cannot at the moment specify the Gateway or the Gateway class you must be very careful with the Gateway IPI it looks super shiny it looks super nice um it has been released November last year finally so it was first Alpha Beta now it has been released officially but I wouldn't say the paint is the paint is a bit fresh let's say worse the implementation they are not at the all at the same level of implementation so depending on your implementation depending on the provider that you are using you must be careful the getaway API aims to be a wider abstraction the Ingress controller was very limited as an abstraction you still had to use a lot of custom annotation or crds the Gateway API aims to widen it but widening doesn't mean that it it's actually 100% so you might find yourself in a situation where hey you want to use the gateaway API you are using the gateaway class and the Gateway and the routes and you still need to use some implementation specific trick to achieve what you want so this is a bit too optimistic but it's still a good thing thing I was a bit faster than I expected I'm sorry for that on the good side that you will have a longer break for yeah you are yeah so I see that it makes you happy um in case uh we will still have uh time for questions in case you you want to follow me on Twitter on masteron on Blue Sky and um if I got you interested in ap6 and now we have some time for questions yes uh for the api6 Ingress uh does it support other protocols or is it still just uh HTTP https uh no you you can use uh TCP and UDP as well great thank you thanks uh so the the question was about api6 if the question is the routes of the getaway API they also I I know there is a TCP route and I think there is a UDP route but I cannot I won't put my hand on fire just for that but uh you can check that's a good question just it widens the scope but you might find yourself find a it doesn't support it but TCP I'm pretty sure I I read it yeah another question for the same price that's amazing can you speak to um uh TLS and http2 support is there anything special that one would have to there is TS root if I remember well the specification as a TS route okay and http2 is there anything special or is that generally handled honestly I don't know sorry I reserve myself the right to say I don't know but if I know I will try to answer anyone else so can you tell us a little bit about the load balancer you use for your on your local machine sorry again yeah could you tell us a little bit about the load balancer use for your local machine yeah so I I use something called metal lb so normally you would use some kind of virtual load balance answer provided by your um by your cloud provider but since I want it to be self-contained I use something called metal lb which you can install directly on your Hardware so no need for no cloud provider which is great amazing no lock in the follow up with that have you had any issues with that it's fine uh I am a developer Advocate nowadays so I'm the king of hello worlds okay so um production experience I'm afraid that I'm not the good person to ask thank is there any native Integrations with sorry could you speak louder at my age I cannot hear anything is there any native Integrations with let encrypt or like C manager to manage integration with sorry let's encrypt or like C manager to do you come closer sorry I I really cannot understand you sorry about that are there any native Integrations with like let's encrypt or C uh cbot to manage certificates let's encrypt uh let's encrypt uh at which point uh at the AP api6 level at the Ingress control level or the getaway API level uh Gateway API or Ingress I have no clue uh to my to to the extent of my knowledge no but I at least on the AP uhi api6 side it's completely orthogonal so probably what I assume is you would have another system to manage the certificates and then you just like just use them in your getaway API or Ingress controller implementation but apart from that no any other questions enjoy your lunch thanks a lot okay join me in thanking Nicola for a great [Applause] presentation ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "kTJJbEWeAz0": {"video_title": "What\u2019s New in OpenTelemetry", "video_description": "Talk by Dotan Horovits\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/what%E2%80%99s-new-opentelemetry\n\nOpenTelemetry is the most active CNCF project after Kubernetes and is progressing at an immense pace. The core project is expanding beyond the \u201cthree pillars\u201d into new signals, and beyond backend into client side telemetry and real user monitoring, eBPF, and much more. It can be difficult to keep track of all these updates. If you liked Horovits\u2019s KubeCon talk last year on OpenTelemetry, you won\u2019t want to miss this sequel. In this talk Horovits will go over some of the notable project updates you should keep an eye on, as well as useful guidance on how to get started in a pragmatic fashion.", "transcript": "and today I'm going to talk to you about what's new in open Telemetry but before we start I want to just uh get to know you so how many of you have heard of open Telemetry with a show of hands okay that's amazing Almost 100% and how many of you use open Telemetry okay interesting so about 50% I think or a bit less so uh that's nice and it it shows the growth I've been giving it talks about open telemetry the world and it's nice to see how the adoption growth and the familiarity carries on and one last question uh this will be a a more complex question than yes no but the question is sorry about that I thought I'm missing a slide here but I wanted to ask you one more question about that uh let me see just a second so we'll do that without the slide I thought I prepared the slide but how many of you uh how many tools do you use for your observability in your companies and we'll do that according to numbers how many of you use and I'm talking about logs metrics traces uh app infra uh uh cloud services everything all together how many uh use H up to three tools or three or more tools with a show of hands okay how many of you five or more more let's see how many hands stay up look around you and how many of you 10 or more okay okay I think some of you will discover that there are other tools that they're not even aware of but uh we we'll discover that later but so this is going to be the topic today and uh my name is Doan horvitz I'm the principal developer Advocate at logs. logio provides a cloud native observability platform that's uh built on popular open source tools such as Prometheus open Telemetry yaa open search and others uh and provide it as a managed platform I'm also a cncf Ambassador the cloud native Computing Foundation that's behind open Telemetry and many of these other projects and kubernetes of course um and I'm also I also host a a podcast called open observability talks uh if you're here it might be very relevant for you so if you like podcasts do check it out on your favorite podcast apps I'm also one of the organizers of the kcd the uh fellow kcd in Tel Aviv so if you're around do check this one out and many others and you can find me everywhere at horovitz as you can see so if you're twitting or toting or posting anything from the St do uh do tag me so I can see that so the topic today is going to be about open Telemetry just a second I want to check because for some reason I'm missing a slide uh and I want to check that I'm on the right uh deck I suspect that I'm not uh so just a second here apologies for this uh and I want to have just a second okay let's try this one uh see if this is what we're looking for and let's do this magic and let's write this so I want to to talk about uh the vision for observability we all know that observability is uh the way that we understand the state of our system based on the Telemetry that it emits and the vision talks about unified observability across the different signals so l metrics traces and uh as we know there as we'll see there are even others and also across different sources so think about your app your infrastructure open source tools that you use kubernetes and others cloud services whatnot and essentially fusing all of these together to gain observability into what's going on in our system that's the vision but the uh reality is much more fragmented and the reason is exactly what we asked before about the increasing number of tools the different tools that we use for our observability uh different tools and each each vendor in each tool and each open source has its own way to instrument the application and then its own way to agents demons collectors to then collect the uh the data and to Aggregate and to do all sorts of sampling and processing and whatnot and then its own way to uh transmit the data and model the data and all of that first of all it's an operational headache I'm sure that you'll agree but the main pain going back to the vision of observability that we talked about is that it creates silos it creates silos that makes it very difficult for us to uh cross ask questions that are cross cutting across the different silos that across the different signals across the different sources okay and it also creates tight coupling between the Telemetry uh collection and the backend storage and analytics and that prevents the uh unified observability we talked about before and this is what open Telemetry comes to solve so otel if I call it otel that's a short for that the same thing is a framework as you can see for generating and collecting Telemetry data across the different signals logs metric traces and across the different sources so if you if you will one um one framework to rule them all and uh it's an incubating project under the cncf proud to say as a cncf Ambassador it started off as a merge in fact of two uh different projects or specifications under the cncf open tracing and open sensus uh back in 2019 it got accepted into the cncf sand box uh and then in 2021 uh into the cncf incubation and now to the first big news of what's new with open Telemetry open Telemetry just last week fresh hot off the press has applied for cncf graduation so first of all I want to applaud to the cncf let's show the support in getting this done and this is in fact a major Milestone so just to make sure that you understand what it means the cncf defines um an open source project maturity model uh and it goes from as you can see here from sandbox to incubating here at the uh lower part and into uh graduation uh and then the to the technical oversight committee evaluates the project at each Milestone to uh approve its uh maturation into the next uh Next Level so this is the process uh and otel as I mentioned has been accepted to the sandbox back in 2019 then into incubation 2021 and 2024 should be The Big Year where otel will graduate get into graduation and very important to say this uh maturity model is about the open source maturity not about the um maturity of the software for delivering to production I'll talk about that in a minute but this is about the open source maturity we're here at at an open source conference at scale and open source signals such as how many uh active maintainers there are uh what's the diversity of organizations how many different organizations are involved um what's the release process what the fact that there are H regular releases of the of the software and so on so all these signals that indicate the health and maturity as an open- source project this is the model that is modeled here uh so this is the the first update uh but H now I want to talk about uh this activity that merits graduation and H talk about how what's new in the project in terms of the adoption and activity and the first thing that I again very proud to say this is open Telemetry is today the second most active project in the cncf second only to kubernetes itself and this is taken from the uh cncf Dev stats it's an open Source uh open grafana dashboard you can see that this is in the span of 30 days but it's been consistent for uh for over a year so it's pretty consistent the second second project uh and let's delve into the uh this activity let's break it down you can see that in the numbers here um 55,000 code commits 67,000 uh pull requests 466k contributions over 9100 um individuals contributing code and over 1100 uh companies uh on the contribution side so this is really active this is taken from the uh uh cncf report that we published recently uh a project Journey report for otel all the way from its acceptance into the CCF and until uh the report end of 2023 and again let's break it down amazing to see uh the growth in cont in contributors over the years which is just actually accelerated if you can see the slope there in the last year and a half uh leading to as I said over 9100 probably by now it's closer to 10,000 uh even more impressive to see the number of contributing companies that has reason uh every quarter in an impressive rate uh up to today is over 11 1100 um and also good to see the diversity we can see here that all the major Cloud providers are involved Amazon Microsoft Google all the major monitoring observability uh players are here which is good to see we can see the largest tech companies also on the end user part Facebook Shopify Uber you can see them here uh and this and obviously also smaller players like MailChimp and merari and others and and this diversity of code contribution as I mentioned before uh both the diversity between vendors and end users large companies and smaller startups is shows a very health good health indicator for the open source and uh in my belief is a good signal for for uh to gra for graduation um we can also see maybe last stat about the geographical uh distribution we can see contributors from all around the world uh leading the pack with here the us we're based in the US so uh a big round of applause for all the maintainers and contributors in the US uh contributing to open Telemetry that's that's really amazing to see uh and as you can see here also Al uh with many from Europe India Australia Russia and many more I I'm actually personally uh uh proud that my local community in Israel is listed here among the top 15 you know Israel tiny with 9 million people being able to squeeze in here with the contribution that's that's a great to see and otel is also on the radar in terms of analysts outside of the open source and Cloud native realm we can see an example here from Gartner that started covering otel uh on its hype cycle for emerging Technologies back in 2022 already uh so it's definitely on the radar for major players and and you see Gartner expected it to reach the plateau of productivity within two to five years so even analysts believe that it's really uh about to mature uh in the coming years so that's just a bit to give you a taste of the activity the level of activity and maturity of the project and now I want to uh quickly go over the main component of open Telemetry to level set the knowledge I know that many here already started playing with that but I want to set level set so that we can then then talk about the updates uh to the project with a different component so open Telemetry essentially is a unified set of apis sdks and tools for generating Telemetry data logs metrics traces and others from our application that's the the bluish part in the on the left hand side and then a standard way for collecting the data and processing it this is the green part in the middle and then exporting it uh to any backend uh of your choice open Telemetry doesn't have any stake in it so whichever back end you you work with you have the relevant way to export to it and we'll see that in a second so let's go and look into each one of these and before going on the components I want to say a word about the open Telemetry specification that is not a component H by itself but it actually governs all the compon components and all the implementations uh of uh the uh open Telemetry across the languages so the cross language requirements and expectations and it includes the API spec the SDK spec and the data spec the semantic conventions and so on across logs metrics and traces uh and that comes to solve of course the problem we talked about before that we have too many specifications and each tool and each vendor has its own this comes to solve it with the unified uh unified um uh set to avoid this fragmentation and also to create semantic conventions that will enable correlation across the signals okay so uh definitely a win and then there's the client libraries the client libraries essentially open Telemetry provides one API and one SDK per programming language that of course according to the uh unified specification I talked about before so we have the Java the net the python the and so on and so forth then it offers language specific integration with popular Frameworks so you know RPC uh uh libraries storage clients uh web framework and so on and also for some languages you can find automatic instrumentation agents that allow to instrument your application code without having to modify the source code itself okay uh and Depends depending on the Lang language of course but you can find that so essentially the the vision for open Telemetry is to cover everything from fully manual to fully automatic and anything in between that's the goal with the client libraries next up is the otel collector uh and The Collector is uh can collect from the open from the otel sdks I talked about just now but also from many other sources you can think about your your Kafka your redis your AWS services or or Azure or whatnot and essentially process and then export them to your back end of choice so it's built like a data processing pipeline as you'd expect you have receivers in multiple protocols you have processors that you can connect and uh and uh uh create all sorts of aggregation and processing logic and then exporters in multiple protocols so if you want to collect Telemetry from your app your uh your front end your back end you want to collect it from infrastructure components you just plug in the right receiver you have a I know kubernetes you plug in the kubernetes receiver Docker receiver uh uh MySQL reddis httpd AWS x-ray gcp pubsub what not you just plug in the right receiver then you have processes that do all sorts of uh logic processing filtering batching sampling and so on and you can you can chain them to create more elaborate logic together and it you build up into pipelines processing pipelines and then you have exporters depending on the back end you want to send it to so you want to send it to AWS you have an AWS x-ray uh exporter you have a Azure monitor exporter Google you have a a Prometheus remote right exporter if you work with Prometheus uh logs. my company also has an exporter and many other vendors so really uh and also Downstream processing like Kafka everything available for exporters um it enables Edge proc processing pattern so you can deploy multiple collectors in different Junctions to create uh to balance the load and the scalability and to increase the throughput um and you can also uh it's very performant a lot of emphasis has been put by the team on the performance the high speed low overhead uh so a lot of thought and lot of work of by the maintainers to make it uh performant and the last thing that I want to talk about is the OTP open Telemetry protocol which is the telemetry uh data uh delivery protocol it's not specifically for alel it's a generic protocol that you can relay Telemetry data based on the data specification we talked about before um uh it can be between the SDK be used between the SDK and The Collector it can be used between the collector and your back end it can be between any intermediary nodes it's really general purpose client server protocol ER request response as you can see at the bottom I I don't know if you can see the transport it supports uh uh it implemented over grpc and HTTP 1.1 on the transport uh and um it currently supports binary protuff en coding as well as Json protuff en coding payload so you have OTP over Json for example um and it's um uh I don't know if you can see at the bottom and as I mentioned it's based on the open specification of the data model now it's very important to emphasize you can use open Telemetry without uh using OTP we've seen I showed you an example just a second ago that you can use the collector with many uh receivers in different protocols you can export in multiple protocols so you're not bound to use OTP in order to migrate and start using open Telemetry and this logic actually applies also in general so you can use the SDK without the collector you can use the collector without the SDK uh it's really loose loosely coupl architecture yet a holistic unified framework for generating and collecting Telemetry data so the goal is to provide the full set but you don't have to use and you can use only parts of that that's the philosophy now that you understand the uh uh the parts and the different components I would like to now talk about the big news about what's uh new and what are the updates and the biggest update I think in open Telemetry besides applying for graduation is the following open telemetry's logging piece has reached GA very recently actually in the recent cubec con at back in Chicago whoever was there heard that news uh really exciting news that was the last signal among the three pillars that to graduate which is which is great news but what does it mean when I say logging is GA shouldn't be shouldn't I be saying open Telemetry is GA or is open Telemetry GA is it ready for production can I use it in production now what do you think who says yes with a raise of show of hands okay not many optim optimistics who says no okay who says it depends okay so the answer is it depends all the rest I guess could not decide but uh it depends and the reason it depends is that open Telemetry is actually aggregate of multiple groups multiple working groups each one uh having in charge of a different component from what I described here um uh and each one has its own uh maturity life cycle for that specific component so and the maturity life cycle in the cncf goes from draft to experimental to stable and then deprecated so as you can see this is a different scale than what I talked about before before that was the scale for open source maturity this scale is the scale of the software maturity just like you'd have with any software whether open source or not uh with experimental you could say this is sort of the beta beta something that you will start a PC or something with and stable being the ga something that is generally available and you can is mature enough for to put it in production okay so each component has its own uh maturity life cycle and that's what makes it a bit challenging to understand and hopefully for me to help you navigate through that so let's start with the maturity of the different uh signals the main signals logs matrics and traces and I'll will go through that according to the order of maturity and the first to have matured was distributed tracing traces that uh is already uh stable for quite a few years meaning that the API the SDK the protocol are stable the collecta is stable for uh tracing pipelines and there are many client libraries that are already version 1.0 or above 1.0 is when it's become stable when the tracing implementation is complete and you can see here lots of programming languages I don't know if you can see at the bottom uh the largest Suite um also stable and that's very important as I mentioned stable means uh H guarantees for long-term support for backward compatibility for dependency isolations H these are the sorts of things that You' be looking for to run it in production so that's about traces next to of matured was metrics uh metrics again the apis SDK protocol are all stable there the API and SDK specifications are already implemented as you can see here in quite a few languages Java go. net C++ JavaScript python PHP are already stable uh The Collector already supports uh metric pipelines uh and very importantly Prometheus uh is supported that's something that is being a great Focus for both communities collaboration between the Prometheus community and the AO Community uh meaning that you have the SDK can export the otel SDK can export in Prometheus format and the uh collector has both receivers and exporters in Prometheus format in open metrics format uh and also I I know it's outside the scope of otel but also on the Prometheus side a lot of work has been done uh on ER open telmary um uh compatibility uh does compatibility also by the way between the OTP specification and the Prometheus specification it's going to be a big Theme by the way for Prometheus uh prome the next major release of Prometheus that is uh planned for this year H theel compatibility so a lot of work there and the last is uh logging logging has reached General availability as I mentioned just uh less than half a year ago uh last year you have client libraries already stable in uh java.net C++ and PHP and in general in in logging there there are two paths there the path of supporting uh existing Legacy logging that you already have the unstructured flat file type of logs and there's the the longer term path for a new H uh specification for structure strongly typed structured logging so the first part that was the the the the uh introductory part was to support existing logging uh lot of work has been done to provide an SDC that can fetch the logs and transmit them over LLP alongside logs and traces um there um log appenders that have been developed in many many languages that take the logs and append the trace data the trace ID the span ID to enrich the logs with the trace related data uh lot of work has been done on on a logging framework so if you work with I don't know in Java in log 4J or log back or or in other languages. net and others you may find that you have a built-in way to append just with with configuration only using these these Frameworks uh and also lots of um in the collector lots of uh log processing has been added in many data formats thanks to uh the stanza project for for those who know this open source project has been donated H to open Telemetry so that was the Legacy logs okay alongside that uh the project has done a work in to create strongly typed machine readable specification for logging something that we've been longing for for many years in this industry uh and there's been a lot of work around that and um with the stability means that of logging means that we already have a stable data model and imple reference implementations for uh to show uh these these things the data model the OTP log protocol there all stable and the big news in this front are that elastic common schema ECS has merged into open Telemetry log specification so that means that we take the whole aggregated knowledge of a wellestablished open source Community the elastic search community and now this is an organic part of the logging uh specification in open Telemetry a great win for both communities and let's talk about some more uh more updates around open Telemetry going beyond the three pillars which is why I hate this term three pillars make think people think that this is the total sum of observability so no it's not the total sum of observability and H in open Telemetry after reaching stability with these three the next signal to have been adopted to to adopt is continuous profiling how many are familiar with continuous profiling with a show of hands okay not many so a word about continuous profiling it's very similar to the classic profilers that you would attach to your application but it's done in an ongoing manner to be able to monitor where your application spends uh its time CPU wise memory-wise sometimes also dis wise uh be able to map down functions or specific lines of code into performance uh performance and cost obviously are the the main drivers for this deep dive um so this is this is definitely uh something that is on the hotel focus and special Sig a special interest group has been formed to uh drive this it's called the Sig profiles and a lot of been done you can see here on the right hand side the from August 2022 when the profiling Vision has been shared and since then a lot has been done on the open specification the data model uh and so on and the big news hot off the press is that uh and this is just from from uh as you can see here uh from last month the data model for profiling has been merged it's an otap an open Telemetry uh extension proposal uh that has been merged uh which means that it's now an organic part of open Telemetry a great uh win for this means that now it's in a um uh it's it's it's advancing into becoming a a uh mature part of the open Telemetry it's a remarkable journey by the Sig profiles actually last uh week I had a live stream uh with members of the Sig talking about this important Milestone and talking about where we stand now and what's the road map so again if you're if you're interested check out the uh it's going to come out this after the live stream I'm going to release that as part of the podcast so check out open observability talks on your favorite app and uh lots of updates that they shared from from the activity in the Sig uh in general in a nutshell there's a data model for profiling along with prototype implementation of the client libraries and The Collector uh it's strongly influenced by prpr for those who know uh this specification uh for for existing specification by Google for for profiling so it's an extension so prpr extended and uh uh and really a big theme for open Telemetry in 2024 next up is adding Rel and monitoring report so uh let's be honest the the first focus of otel was the backend side Java net and all the backend logic you could do some some client side instrumentation with the JavaScript sdks and so on but the really the the native entities uh for representing uh session IDs and other elements and being able to ask questions uh along the lines of uh counting the number of users versus the number of new users versus returning users or visualizing how you users navigate through the application UI or identifying pages that generate High number of of errors and Def pages with high load latency all of these sorts of questions that you usually use Rel user monitoring for these were not baked into the specification you didn't have the representation in the data model so this is this is part of what we want to achieve with open Telemetry and as you can see open TM started a new Sig special interest group specifically for uh for that client side Telemetry uh a couple of years ago and since then a lot have been done I gave a few examples here as you can see uh with browser elements and uh and mobile and and others uh check it out you have the QR code here to to read more about the uh real user monitoring uh Sig and also check it out on the cncf slack at hotel clientside Telemetry really interesting stuff next up uh a lot of focus has been done on uh the operability of open Telemetry so as it matures people start putting it in production such as some of you folks and then we started putting uh more thought into how to operate it in day two scenarios and uh day two scenarios mean we started with the kubernetes operator last year uh which is and their advancement there as well support for feature Gates and and so on but also very importantly uh focus on opamp the open agent management uh uh protocol that supports remote configuration of the agent status reporting uh uh Telemetry of the actual agents themselves back to the back end and all of that so a lot of work has been done on that uh this is sort of the architecture for op pump as you can see um and and it's not really by the way just for otel it's a general purpose uh uh protocol so it can also uh you can see here I think in the example uh you can see for example for fluent bit agents and others so really uh should be able to support mixed agents even combination of Hotel collector with alongside the fluent bit and and things like that uh and as you can see some of the implementations already available uh in Java and in go and might be other languages that I missed so it's really moving fast very interesting advancements there um another aspect is ebpf for auto instrumentation uh how many have heard of B BPF here okay so I don't need to explain that's good essentially it's a way to instrument without changing the code and without not a language specific way of instrumenting so doing it in the kernel level underneath um and uh a lot of work has been done on using ebpf to Auto instrument and get Telemetry actually I had an episode with u uh Eden Federman the creator of the ebpf Go Auto instrumentation uh agent for otel who leads the Go Auto instrumentation Sig uh and does work on many more other languages not just uh go and in general uh Auto instrumentation on whether ebpf or not is a major Focus for otel so the removing the need for you to mod to modify your source code to just get started you probably will have to modify the code for custom Uh custom Telemetry but at least for for doing the the the table stake type of telemetry hopefully to do that with uh without uh changing the code Hotel demo reach GA so you have a canonical demo app uh it's the astronomy shop that you can see here on the on the right hand side uh that is as you can see on the architecture diagram it's a bit small but lots of programming languages that are implemented here net C++ erlang go and others lots of components that you can see here like RIS cash and uh and the SQL database and all and so progress and so on uh to really simulate a real life application to get you up and going uh handson with open Telemetry uh another update sunsetting open sensus for those of you whove still used open sensus or by the way also open tracing it's been uh deprecated officially uh open Telemetry is reached feature parity so it's no longer needed and uh time to move on the the repo the GitHub repos for open seses have been archived uh since July last year if I'm not mistaken uh so time to move on and very hot uh update as well otel day is coming back this year 2024 this year in Seattle in June in June 25th so uh you're based here in North America it's easier for you to get there than me uh check it out and join it's a great uh opportunity to uh meet maintainers contributors and users of open Telemetry uh check it out and there early tickets are are already available as well as open cfp so if you have experience that you want to share also a great stage to share about open Telemetry and uh for the last uh uh few minutes I just want to say a quick word about how to get started with open Telemetry because as you saw it's it could be quite confusing right lots of components each one with its own life cycle stage and and so on so how can you do that and I want to provide my two cents on that and I start by knowing your stack your own uh system and application stack and ask yourself these four questions first which programming languages do you use okay uh and this will determine help you determine the client libraries that you will need to use and perhaps also which Frameworks they used Java with spring uh net sorry node.js with happy or Express and this will help you check uh the maturity in these regards then ask sorry then ask yourselves uh which signals you want to collect logs metrics traces profiles uh to help determine the receivers that you will need to use and also the protocols especially if it's a legacy Brownfield like I don't know if you want to collect from uh Kafka you want a Kafka receiver if you want to collect from Zipkin you need a Zipkin compatible receiver so this will help you determine the receivers and then lastly which backhand Analytics tool you want to use because this will help determine the exporters that you're going to use okay and once you've mapped out your stack and mapped it to the relevant components then go ahead and check the relevant status there's a status page open Telemetry doio status that's a good place to start H lots of other resources like you see uh the docs the registry that essentially helps navigating through the massive gab Breo of open Telemetry and I'd like to invite you all of course to also check the guide to open Telemetry that I wrote that is sort of a beginner's guide to give you uh in depth on all the components that I mentioned and also uh uh the status of these and then a link to the downstream to the specific uh repos for the components that are of interest to you so it's a good way to Fan out to the official Doc docs and you have subg guides by the way for the different programming languages so a subg guide for Java net and so on so you can also have a sort of a hello world for your specific application so uh that's about uh about the guide and uh with that I'd like to thank you and uh uh maybe take some questions if if we have the time thank you D for such a great presentation uh we have of course time for questions so if you have one please raise your hand I will bring them back to you uh so it can be recorded for the replay hey thank you for your presentation uh in the slide of the demo up the you showed you mentioned postgress as that one of databases there yeah so is it possible to instrument postgress to send open Telemetry data without changing postgress or there's a I I don't remember what they us the what what is used but there's work you don't need to change Pro pogress obviously extension there's an extension so it's it's an adapter essentially it's a receiver that is able to take uh and I don't remember exactly what pogress Expos usually what what with with Frameworks it takes what it exposes naturally and then as an agent it then transforms it and and sends it to the to the relevant uh relevant collector so I don't remember the mechanism specifically because I'm not a postgress guy I know there lots of experts here in postgress but usually what it takes it takes the native metrics converts them and then sends them off to usually using OTP sure thank you so much for this great uh presentation as always um how is uh continuous profiling enabled though is he uh done through U yeah just any just a little details on that is it going to be automatically enabled uh during the uh instrumentation for the uh application can you say a bit slow so continuous profiling what did you continuous profiling yeah how is he enabled with open thetri since it's a new feature that has been added but how is he uh so it's not it's not a new feature it's really a signal from the ground up it means that the sdks will natively support the ability to fetch to generate the the profiling data from the from the source code from the application then on the collector side there will be pipelines that ingest uh the uh the profiling data so it's really from the ground up it's not uh just like you do with logs so you'll have sdks profitably also Auto instrumentation agents there I I haven't shared that because it's not final yet but for elastic suggested proposed donating an ebpf based uh profiler so that will enable instrumenting without code changes we'll have sort of Auto instrumentation agent for collectors if this goes through the the the contribution uh so whether Auto instrumented or manually instrumented this is how it work and it's based on PPR extended as I mentioned in terms of the protocol itself check out my Episode by the way I think it will answer many more of these question question so uh open obsorb talks I know that you follow but uh yeah uh who else wants to ask questions come on anyone who get ask questions including the ones we already have we have a gentleman here we'll get a sticker May the open source be with you like the shirt so uh another incentive to uh to ask some uh some interesting questions so uh what would you say to somebody who has maybe already adopted uh otel for metrics and tracing but maybe looked at fluent bid or fluent D for log and now has to kind of make a choice I'm not trying to make it a you know Cage death match only one can come out alive but just trying to understand where the the pieces fit together and if they're going to you know cooperate at some point in the future so you have already instrumented with h fluent D and fluent D or fluent bit you said fluent bit uh and you're wondering whether you should converge to open Telemetry it's it's a a very good question and obviously it's very young in open Telemetry I should say that and fluent bit has been around for longer so I would say maturity wise fluent bit is is in a in a stable place so I wouldn't be in a hurry to to move on the other hand you should understand the advantages of having all the signals run through open Telemetry in terms of the correlation so think about that that for example the data specification the the naming and the labeling of the I don't know the the Pod ID the node ID whatnot will be consistently across all the signals and then when you want to ask across cross cutting question it it it lanss itself and also in the processing you run it through the same processing Pipeline and you can convert for example their their ability to roll up logs into metrics for example how would you do that if you do that through the same processor you can do this sort of logic or just as as we do in other cases of rolling up traces span data into metrics as well so when you have it all in one collector it makes very much easier to do these sorts of cross signal correlation and also cross signal uh uh rollups and and aggregations like that so I hope I answered and there there also places where you can actually ingest like fluent bit into the collector or things like that so you can actually combine like you have the edge agent still being fluent bit and you can have a back end let's say in the in the Ingress Point having a a an oel collector that you can collect two and with the one last note with the opamp that I mentioned the protocol that control going on to the probability you can control with o pump a mixed environment with mixed agents so both oel collectors and fluent bit and fluent D would potentially be governed with a single opamp protocol hi my background is as a SRE and um I have a pet peeve where most of this data is usually just accessed through um chronologically right and we have retention through this sorry I didn't through chronologically okay yeah so um the data usually you're scanning back and forth over time but what it's hard to find something that's specific to a particular incident or particular uh time when you did a um a particular Benchmark and trying to look at the data just from that that particular event and the other problem is the data will expire over time because you can only hold up to maybe 90 days or something so it seems like there would there's um an opportunity to have a more event-based approach for like archival of like hey this incident I want to retain it for for the next five years even um or this Benchmark I need to retain for a certain period but I haven't seen any of the kind of telemetry observability people thinking about looking at um you know um basically creating a library of of of uh observability data that way uh it's a long long question so I'll try and uh and answer it shortly because I know we're running out of time uh first of all I have a talk tomorrow on the observability track about observability as a data analytics problem I think you should definitely check it out because there I talk about the observability best practices and patterns it goes well Beyond open telemetry first of all because open Telemetry is Mission as I mentioned is only on the generating the Telemetry and collecting it but then it sends it off the storage and analytics part is outside the scope of otel and much of what you you asked for is about actually how to store how to maybe compress and and rollups and then how to analyze it so it's beyond the scope much of that obviously if you do the rollups upon collector and then you don't even send it to the back end then you lost the data you lost the granularity um I can tell you for example on the discussions on the specification of U of continuous profiling for example there's been a lot of discussion going on about whether we should um uh do aggregated uh information or event based uh if the decision now is prpr and prpr is aggregated so it Trends over time but then you lose the the individual events but it is a question that came up and potentially it will evolve into event based as well so there's a lot of discussion all the time with the community especially around specifications whether you want event based or more aggregated especially for metrics and and the profiles that are things that lend themselves for this sort of data in terms of you mentioned more of the storage and analytics I will leave it to my talk tomorrow because it's a it's a bit beyond this scope but I hope that I answered the the question who else do we have time for one last question um maybe we should allow one of our sponsors to uh to be on we have another sponsor so thank you very much for listening and I'm here if you have more questions thank you very much thank you ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "nQXRhHrX4ik": {"video_title": "Gateway API: Basics, Nuance, and the Real World", "video_description": "Talk by Leigh Capili\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/gateway-api-basics-nuance-and-real-world\n\nA Kubernetes computer is supposed to house all of our apps! How do we expose them to the people who need them? We used to use the Ingress API for this, but we can do better. Gateway API is an improved approach that allows cluster operators and application owners to own the pieces and policies they need to route traffic to applications. In this session, you'll learn the basics of Gateway API. What are the resources and how are they used together? What considerations are needed for zero downtime deploys? We will cover this and more. Expect a live demo and some *performance art* :)", "transcript": "yeah I'm Lee capil as ma said I I do work with vmor tanzu right now we were acquired by broadcom recently so you know uh that's a it's it's pretty cool to be working on platform as a service uh specifically now and uh kind of our infrastructure stuff has been split out into a separate business unit that's kind of fun um I grew up here so I'm from La uh I live in Colorado now and I love to snowboard are there any Filipino ameran people in the room I'm proudly Filipino American hey I love coming back to LA hang out with my Filipino people uh and uh go and find some Sago later to to have some dessert uh so um you might have seen me around a little bit in the flux project uh and then I also used to contribute quite a lot to kubernetes Upstream especially in cluster life cycle with KUB ADM uh recently I'm back to contributing Upstream I'm working on some things that are adjacent to Gateway this thing called reference grant that we can talk about later if you like but uh yeah uh that is a real picture of my dog it's not a stock photo his name is Pepsi and he's a very good boy so and um so today's conversation is about Gateway API um I got really excited about the chance to dig in uh to Gateway because every time I've used kubernetes at significant scale it's not been with Gateway because Gateway is pretty new and I think if there's probably some pretty senior practitioners uh here there's a pretty good chance that you're not using Gateway right now um if I had to guess because uh Ingress is perfectly sufficient uh at the moment it's probably running production workloads in your environment uh there's a million examples it's a very simple API and um it's really easy to get going with Ingress you don't have to do anything special it's like already there in the cluster and so in order to get into Gateway there's an investment it's there's an investment in knowledge uh you got to go and learn the thing and then you've got to do some extra work as well to get everything working it's built out of tree uh and so beyond the basics we get into some fun kind of nuan situations uh and then um we're going to talk a little bit about a real world topic later uh that is still very much my crusade very close to my heart uh it's zero downtime deployments uh so um a bit of a mixture of intermediate and beginner content today thanks so much uh for coming so we have this fancy distributed computer uh that a lot of us community members have been working on over the past decade if you haven't heard of it it's called kubernetes and the cool thing about kubernetes is it's got this declarative programming model the declarative programming model means that that you program the distributed computer in a way that maybe you're not so used to right um and that seems like maybe kind of a basic primer since we've already been talking about kubernetes all day um but why this is exciting when we talk about Network Technology is if you think back just like maybe a decade ago there was a good chance that if you've ever been a network engineer you're probably at a terminal in some collocated uh you know data center where you've got like a bunch of loud fans around you and you're hopefully wearing earplugs and you're on that terminal logging into the switch because you got paged and it's 11:30 at night when you'd rather be you know maybe like hanging out at home and you're on that terminal and you are typing imperative commands to First build your mental model of the routing stack uh and then try to figure out how to mutate the routing stack so that you can get stuff to go the right places uh there's a lot of like fancy Hardware that you can install physically and telling that Hardware what to do has not always been easy you've usually need a specialist uh to get in there so what's exciting is we build this distributed computer and the API Machinery being so extensible in kubernetes opens us up to this Cloud native world where we can all become basic Network Engineers uh sounds fun right uh I'm I'm stoked about it yeah I can tell I can tell everyone is stoked about becoming a network engineer so the basic unit of compute in kubernetes is the Pod right and just a just a rough run through on the networking model and why all of this fancy um Machinery that we're going to be working up to is even necessary uh is that uh pods come and go as they please um we have a lot of them over the life cycle of running interesting workloads on kubernetes and they all have individual IP addresses and so we created the networking API and there's there's an immediate split here uh that's that's like an important theme uh compute is separate from networking in kubernetes so we make these things called services and services have a controller that Aggregates end points and so it looks at all of the PODS of like a particular label um using a label selector finds their IP addresses and collects them into an endpoint slice we got a bunch of these endpoint slices we also mirror it to the endpoints API so you can keep using that thing which is old and technically not supposed to be used anymore this slide is technically wrong um and now that we have these endpoint lists we can keep them dynamically updated dated right and so we're always just watching and then we do our best to keep that list up to dat and then there are a bunch of things from the ecosystem from the core kubernetes cluster uh maybe from your data center from some random bash script that you might be running in a CI job uh that might be interested in all of that networking information and traditionally it has just been nodes maybe a cloud provider load balancer uh which is like a type of service you just decorate your service to be a load balancer and now it goes and does fancy controller things in your Cloud that you pay a lot of money for and then you've got the Ingress controllers so Ingress was kind of the First Community effort um to extend kubernetes in a way where we brought kind of the first application facing protocol call abstraction into the kubernetes API so we're Beyond actually compute and we're Beyond networking and we're into the protocol layer uh which I guess is part of it's It's a higher level networking layer right and so what did the Ingress API look like well if we hop out of vs code over here and let's just pull up a rudimentary Ingress yl okay so here we have the networking API uh we named it enginex this is defaulting into the default namespace which is a terrible API decision that we all regret um and then now we have a specification uh at the top level of the spec we are going to have a list of rules those rules then can have uh these kind of protocol entries and then within those protocol entries you can go into paths and then you can decide if that path is going to match on some particular conditions right so this one is a prefix match on the root slash and that path when it matches is going to go to a back end so you know if I wanted to be all fancy and stuff right I could like make three of these you know and then start sending them different places right like maybe this one is the engine X for the store you know and then maybe this one is uh the engine X for you know the app layer right and then uh you know we can like change port numbers or whatever and then we would maybe say this is the you know API V2 you know and then this would be the store cart end points or something like that right and so what you're getting here is something that looks awfully like a reverse proxy configuration and that's exactly what the Ingress API is for it is a reverse proxy um that is the old term and now we we call it Ingress and Gateway because that's the cloud native like jargon uh this this is a reverse proxy config uh we have a single listener somewhere uh that's that's then being multiplexed based off of the content of the packets that are flowing at the protocol layer the reverse proxy is responsible for watching those packets Consulting its configuration and then deciding where to send those packets and how to modify them um based off of the config and so uh we're kind of Fanning out into multiple services and you can make a bunch of these so that's kind of fun um we start to get into a couple of problems here uh and to illustrate problem number one uh I'm going to go to the kubernetes documentation which is here so this is like the first touch point of the Ingress resource in the kubernetes docs um I'm kind of wondering this is like Upstream kubernetes docs and for some reason we've got an example here I guess it's a good example honestly um because this is how Ingress is always used in practice um you hardly ever see an Ingress resource that's not annotated with something and these annotations in the kubernetes API Machinery are how we can start decorating ating resources to have additional metadata so that something that kubernetes core doesn't control like an Ingress controller that comes from the community um can uh can get more data about the resource and what its declaration is supposed to be it's it's basically an API definition outside of the API definition um Ingress is one of the few apis where this really matters because there is an Ingress class resource which um maybe sorry I didn't pull up an example of this here it is ingress class so there's Ingress and there's Ingress class interesting right and that's because Ingress was built as a specification that was intended to be implemented by a bunch of people and if you look at the ncf landscape you can see that that intention ended up being very successful because we have a ton of Ingress controllers inside of the ecosystem it's super exciting because they get to bring all of their features and by the time you end up using an Ingress resource in the wild um this uh this simple example of a single annotation frequently ends up being like 12 annotations uh that are being operated on by two or three free controllers often it'll be like your Ingress controller and then like external DNS and then maybe also assert manager annotation in there to tell it like which issuer that it's you know supposed to be using uh and like what domain name is supposed to be like being put out into uh you know Route 53 or whatever you've configured with external DNS so this is this is kind of problem Numero Uno um sub problem of that as well is because we invited a bunch of people to extend this thing we realized oh this is a pretty decent core subset of reverse proxy uh features uh but it wasn't enough It's never enough for anybody uh and so the the path prefix matching needed to then like be broken out to support wild cards and people started doing that in annotations and then people wanted to start supporting sticky sessions and they started doing that in annotations and the annotations got so sophisticated like pull up traffic for you um traffic traffic traffic check this out this is so fun I have used this web page so many times in my life more times than I would want to um yeah this is this is an annotation but it's like structured like we've got router. entry points router. middlewares uh you know you can set the priority of what this Ingress rule is going to be using this is these are all annotations that are meant to be used on a kubernetes Ingress uh and and only traffic understands them um so then traffic rightfully got tired of this and then they made their own custom resource and Contour got tired of this and they made their own customer resource and so it was really clear that Ingress wasn't scaling for people also um there's something sort of Insidious happening here um it's we're mixing a lot of concerns into one object uh one of the nearest and dearest things to a developer's heart uh is that traffic would arrive at their application and nobody knows better what path to send things on and what port something should show up on then hopefully the developer of the app uh but a lot of devs I know when I started as a software developer I was not a network engineer I did not know that that SL SSL CS were actually supposed to be called TLS which is a different H dial right um I I did I didn't know what a network cider was um I I wasn't sure how to provision IP addresses in the public space that seems like a rare skill still um you know there's a lot of like very special skill sets in order to actually do real networking out in the real world that's why we specialize and we help each other out and um so it would actually be really nice if all of this stuff that we were putting up here could be owned by a different resource uh and that is pretty much exactly what we did with Gateway um one third the third and last problem um that I'm going to uh point out as I complain about this beautiful object that has literally revolutionize the networks of many Enterprises um is uh Ingress is lacking Precision in in one more place and that is that when you install an Ingress controller in your own cluster you often can pick where that reverse proxy lives the actual infrastructure of it you know and so you could you could put that on an edge node in your data center you can run it as a fleet of pods on your and expose them via noort um and you then hook that in that Fleet of of end points up to an Ingress class and then people can select into it and if you want more of those then you can Dey more of them you make more Ingress classes and people can keep selecting into them uh and so developers can be like I want this route on the internal Network this route on the external network this one on the CDN uh you know and and get the benefits of all of the fancy Arcane configurations as long as I keep passing the same Str set of annotations that people tell me to do so um but uh then you get into a cloud provider and they were trying to provide these awesome products like I was using a lot of gke at the time I'll be pointing to a lot of GP dos today I don't work for Google but today I guess um and the the moment that you wanted to deploy a global load balancer the gslb uh you created an Ingress resource select into their default Ingress class and now you're paying like $20 a month for that thing uh and it's routing like one service in one name space uh now you were able to then go and make another one of those in another Nam space and now you're paying $40 a month and there wasn't a really great way to divide tenants up across namespaces and reuse the same load balancer with and so especially uh with Cloud providers they they ran into this like a I have to have these onetoone mappings of expensive infrastructure to the things that I'm routing to in the cluster when you start to get into people like very multiservice arur I don't want to say microservices right like they uh it just wasn't very cost friendly and a lot of people ended up just rolling their own stacks of self-managed e controllers just so that they could get the proper service mappings to their the hierarchy of organizations but uh cool so uh thank you for listening to me rant about uh something that we probably a lot of us use um there's a there's some Growing Pains that we've all run into and that is why we are now in the beta uh after several years of Gateway API uh let's go take a look uh hopefully some of you are already familiar with Gateway if you've never touched Gateway Ines hopefully we can also present it info so just coming back to our little slides the really fun thing with Gateway uh that we realized was that that second pain point the stacks of annotations were building up people were starting to have to build their own apis there's fragmentation happening in the ecosystem and everybody had to use all of it and it got all mixed up and uh we had these single resources that were owned by multiple people right you had infrastructure people saying hey the Ingress class you know points to this thing so you need to know that and then the Ops folks were like okay we got to make sure that we configure the proper Wild Card redirects and header overrides and stuff in here and then the devs were like I'm trying to manage my pads and port numbers and like direct to my service names because I just added a new one and it's all in the same object it's that's not working well uh so in Gateway we really focused on three persons right there's the person who Provisions the infrastructure of the gateways there's the person who operates the cluster who decides how stuff gets divied up how stuff is configured uh at the networking layer and then we've got to serve the application developers so that they can own their part of the config without having to worry about all of this stuff that they frankly shouldn't even have to like be concerned about knowing about right like I'm all for people learning skill sets and stuff but if somebody's not trying to be an infrastructure Network person right now they're just trying to solve business logic problems like let's let them do that and not tell them you know here's a 70 character long gaml string that you need to indent in the proper place in your service definition right so uh Gateway is multip Persona just by Design right and you can see that if you go to Gateway api. s.k um you will see that right up front right now we get our first look at the Gateway API resource model so not that many more objects uh I mean I guess you mostly had to worry about one before now there's like seven technically eight there's eight and um so we got our classes which is similar to engress classroom before uh we have our gateways uh which this is basically the replacement for the top part of Ingress where all of those annotations went right so now we can configure TLS certificates we can do all kinds of fun hacky Network stuff to transform packets in the configuration of the Gateway and then we can hand off to our application developers who will say hey I've got a route on some protocol that I would like to expose I want that to live somewhere in a name space that I own and please aggregate that up to some Gateway um if I don't specify please try to figure it out for me uh or let me opt into some stuff so uh that's cool if you then hop into the kubernetes docs here uh we have a Concepts page on Gateway API uh where you can start to see how these things look so with Gateway classes we have uh this example class and then it ties into a controller name so the way that the spec recommends that this works because we can't Force anybody to do anything but we can say hey please be compliant with the way that it should work is uh controllers will load up all of the Gateway classes filter them out and then look for themselves in their classes so that the infrastructure provisioner of the cluster can decide what which classes you need in a cluster so if you want like a local load balancer class and then a CDN load balancer class and a public internet load balancer class the infrastructure provisioner of that cluster can decide at that time and then it can assign it to the Gateway controller to go and do stuff yeah yeah I think uh security is often owned by the cluster operator I realize now that I haven't been speaking into the recording so thank you so much for calling that out I'm going to try to remember to hold this mic properly and maybe I'll just take this off because it's like muscle memory at this point all right for now great thank you okay so Gateway class looks pretty similar to before um now I'm going to go and pick on gke here uh because they've got some really good examples oh actually it was not gke in my head it was I think on this page yeah now what's really improved about this um is we've put a lot of effort into parameterizing the gateway class so that if you would like the infrastructure provisioner uh to be able to tell the Gateway controllers how particular classes of gateways anytime you instantiate them should behave you can add some configuration in the parameters references here uh and this config can actually reference any sort of custom resource that you add to the cluster uh so this is a great place uh for traffic to add a traffic config and Contour to add a contour config um because they can read their own schema and then decide how classes behave uh so pretty uh pretty awesome API decision here for the people who maintain these Ingress and Gateway controllers uh Gateway class gives you some uh ability to variate the configs so that's kind of the first step now next I will just put that away next uh on that chart we were going from Gateway class now we're going to make gateways uh that reference some Gateway class every Gateway has to be classified uh this is the main tool that we use to help support multiple implementations of Gateway controllers running in the same cluster because now we can subdivide the API up amongst those controllers and so with Gateway if we go to the API spec uh you can see that uh sorry this was this example right here so they start out pretty simple um this is the the cluster operator now saying okay my infrastructure provisioner has provided me some classes that I can select into right um what would that look like in the real world here if I go to the gke dentation this is why I was going here um there are a lot of Gateway classes that you get from a uh kubernetes providers cluster um it's like like the muscle memory is like not working with the one hand um um I don't even know how to type anymore cool all right and I'm looking for internal Gateway classes um and then I'm looking for this dock right here Gateway class capabilities this is a wonderful table so um they've got I think eight Gateway classes listed here uh that are potentially usable inside of a GK cluster if you just spin one up uh you're going to get a similar SAR experience if you spin up a Gateway compatible cluster from some other infrastructure provider such as you know Azure or if you purchase something from one of the big vendors and uh you can see that these Gateway classes some of them are multicluster classes some of them are managed load balancers uh others are various kinds of application specific load balancers uh and they all have some very interesting feature sets that you're able to configure with different policies that you reference inside of that parameter config so all of this is done for you by by default ahead of time so I come into my GK cluster I just bought this thing I'm like okay now I um I want to opt in to using some gateways so I'm not paying for anything uh off the bat but I have all of these configurations available to me that's pretty cool um it's like I got a little catalog from the person who was provisioning my cluster so and I basically go and make my Gateway and then I select my class name uh and then there's some kind of core um Fields here it's listeners and address and um again it starts out pretty simple uh you can actually see I'm in my demo repo I'm going to be using something that basically looks exactly like this um but again um this is a great place for somebody to make some choices and so when I actually make my Gateway yeah I'm picking the ports you know I'm picking the IP addresses that it's using um and if I kind of GP through for some examples here right right this is kind Gateway and then so here's a Gateway class being uh picked here you're still seeing some patterns of using some annotations uh to like you know uh hook up to a structure that gke knows about but then like here we're using the address struct and then it's using this named address and that's actually going to use a name that means something magical to gke but look it's an official part of the API uh that's pretty cool and then and then here as well um is a static IP address example right so that that that would be the cluster operator experience of now provisioning some gateways uh that can then be used in the platform that I'm now implementing on the cluster that I just purchased very nice so lastly we've provisioned some actual reverse proxies right like I I have you know gone in I've decided hey I want to use three of these Gateway classes I want something on my VPC I want a kind of CDN capable Gateway class to serve some assets and then I also want a fancy multicluster you know uh you know XDP powered um like nearest nearest neighbor nearest path routing kind of load balancer product uh so I picked these three things it's like my Global load balancer my CDN load balancer and my VPC internal load balancer um so that I can talk to internal services and I made these three gateways and still nothing is being routed right so we've just set up all of the cluster infrastructure with the two personas the third Persona is those application developers right so now we're actually getting to to real business value almost right the only thing left for the app developer to do after they create these HTTP routes and TLS routes is actually do something valuable inside of the uh soft Ware that's running behind those routes so uh that's like always the funnest challenge right and uh everything else is kind of just a slog you know um but we get to pretend to be Network Engineers so uh the HTTP route resource is kind of the it's going to be the most common one I think that that a lot of people are going to be reaching for and then TLS route is also going to be uh quite nice if you're looking for some uh uh mtls kind of encryption in the cluster so let's look at an HTTP route I have an example of one of those uh not this this let's go to our Gateway example that we will be demoing in just a moment so you can see I have a class here uh and then I've provisioned a Gateway un fortunately I'm all three people in this example because we're a very small startup um so here's our HTTP route resource right so we're not doing encrypted HTTP uh in this case um I could have added TLS termination onto this Gateway um but we skipped that for the moment that's easier to do with a manage provider of course and um this HTTP route it looks kind of suspiciously a lot like Ingress which is nice because people know how to use Ingress there's plenty of examples uh you probably have 50 yaml files of ingresses inside of your control repositories and converting those uh is already a significant feat uh so you know some stuff has changed here um we've changed some field names and uh We've made things a little bit more capable um there are also uh a few deprecations like prefix is no longer a a proper uh word to use here you have to say path prefix um you also will notice here that you can specify parent refs uh so as a developer I if I leave this out right uh then it's kind of just up to the gateways to figure out if they should use uh my HTTP route or not uh I could come up into the meta dat section and I could start labeling uh my HTTP route and then uh gateways that are using label selectors to filter down uh what they want to Route uh can do that you can also do it at the namespace level uh you can also have multiple gateways reconciling the same sets uh or intersecting sets of HTTP routes which is actually exciting uh because you might want your service to be reachable both on the CDN and from the internal VPC uh Gateway so uh there you can see now that there's a lot of power here in separating the HTTP route and the Gateway because now we can create one toone mappings we can create many toone mappings we can create many to many mappings uh and do whatever level of sophistication uh is justified by our applications needs or more likely the complexity of our interpersonal relationships in our growing organizations so um this you'll see that this is kind of a protocol specific one uh and what's fun is if you hop over to the um gway reference do we we don't have this here we have it where was that I'm sorry I'm fumbling so much friends I'm like so much slower here so uh here is an example of a grpc route right uh I'm not a person who's ever used the Google RPC um it's it's I've only ever interacted it when like talking directly to etcd but I do know that there is a small but growing Community people who are trying to even serve grpc to public endpoints uh and and do fun stuff on it of course inter service communication uh is kind of the main um the main goal of grpc and so we support a grpc route in Gateway uh and you can still do the same sorts of things like Rules matches filters and you can send them to the particular grpc back G uh and then it is up to the Gateway controller to then support uh this optional resource that you can uh install in your cluster uh and then know how to Multiplex on grpc you know that's got it needs its own uh stream processing and load balancing algorithms cool we are doing pretty well on time here great well uh this is a a pretty highle overview of the basic objects of Gateway API uh unfortunately you can see that you already get into a lot of nuance with anything that's designed uh to be used by multiple implementers right and uh that is both the The Challenge and reward of going on a multi-year effort to ratify an API like this um some gotas are that as soon as you want to use Gateway API if if you have just spun up a core kubernetes cluster you're going to have to go install the API uh so when you first spin up kubernetes uh Ingress is just sitting there and it's waiting for you uh but Gateway is one of the first kind of very core uh intended very official uh apis that doesn't come pre-installed in kubernetes and is not some feature flag that you can just turn on in the API server uh it is actually shipped as a custom resource definition um the reality of using kubernetes for anything real these days is you probably do need to install a custom resource definition um they're they're just shipped from the gith uh GitHub releases in this case uh the custom resource definition is going to be separate from the Gateway controller that you uh that you're going to install so you're installing one thing from one place and then one thing from another place that's kind of maybe a little bit of a mental switch um but they they are often separate life cycles anyway installing a controller and installing the API the controller uh needs like has to happen before that anyway so um so let's get into a a little discussion of some things that are developing um if you go to this um API types uh you can find this little button on the side here uh that will take you to this page called reference Grant uh reference Grant is something that I'm really excited about right now uh there is a problem that you run into when you want to start configuring infrastructure that needs en encrypted or secret bits um say we install uh a TLS certificate into a secret in our cluster uh now we have the ability to configure something to use the private key to serve some secure connections uh we need to tell our Gateway controller how to get to that certificate and we need to give the Gateway controller access to read that certificate uh the sad State of Affairs is that our back is technically good enough to solve that problem uh but the where We've Ended up in the community uh is that most Ingress controllers that we install these days and subsequently we add Gateway support to these Ingress controllers so now Gateway controllers uh they it's basically become a canonical pattern that these reverse proxies that live on the very edge of your infrastructure have access to all and every type of secret thing inside of your cluster and that's not great um it's a pretty big attack surface these are the these are the first point in which bits transfer into your network from a from a source that you can't control uh an exploit in an Ingress controller uh could steadily mean an exploit on all secret things that you keep inside of your cluster and uh that's that's we we should fix that um unfortunately the path of telling everybody to do it a different way has kind of sailed and so what we are trying to do uh is build mechanisms and build build the fundamental mechanisms to start allowing apis to have granular referential access uh the source of reference Grant is actually to try and do cross Nam space stuff so here um we have somebody who owns a service uh and we want to make it possible for the person who owns that service to opt into allowing the route to point to it from a different name space now you can see that the same concept would be useful for secrets and it would be useful for um any kind of backend configuration uh that needs to point across namespaces it can also be useful for gateways uh and collecting routes whether they should have access to those routes um and this API design is very desirable not just to networking apis Sig storage is looking at using reference Grant and then flux uh a project that's pretty far outside of kubernetes core has additional needs uh around cross namespace references typically you see people making customizations uh or um hel Helm releases sorry struggling with that word releases uh that need to point to some Source right so like a customization to a git repository a Helm Helm release to a Helm oci repository and those sources could often be collected in some Central namespace but then you want to release them in namespaces that other people control and uh so this is a very fascinating and developing area and reference Grant is already an optional component that you can install with Gateway and some Gateway API controllers support it and we currently just trust the Gateway API controller to follow the references and to properly uh implement the access controls what we are working on now uh is a kep uh for referential author authorization uh extending or maybe even being built into the core API server uh and so this is really getting into nuanced real world edgy stuff that doesn't work yet we just have a barely working uh proof of concept prototype um this is uh cap number I think 3766 is the issue link so that's probably the kep number if I remember how that works properly uh we've been working hard on this is uh collaboration between multiple people uh for a long time now and we are speaking about it at coupon and are going to be trying to continue ratifying this API into something that is generally usable for multiple projects and can be uh authorized by the API server so we can fix that problem of Ingress controllers having access to literally everything that's important so um that's just a little bit of nuance bits there I'm going to take a step back here and go to a more light-hearted place which is something that I think everybody can appreciate uh a live demo so if I hop over to here and let's let's see how this go I might need this so right right now I have a kind cluster uh it's you know you can see it's just running core DNS ETD uh the normal kind cni we' got API server controller manager and the proxy uh scheduler and then I've got a traffic controller uh it says Ingress controller but I'm not running this in Ingress mode Ingress is not even enabled uh it's purely configured for Gateway uh and then not using this storage provisioner right now so then if we take a look in our configuration directory uh we can see I've got a deployment uh that's running my traffic controller maybe I could rename that to Gateway uh and then uh it's running just the latest version of traffic uh because I don't like updating the tag every time I have to do this demo uh and then yeah we can see I've disabled Ingress I've disabled their special custom resource uh and I'm enabling their experimental Gateway support and then enabling the Gateway provider and then this this container is just going to expose on the host the host Port of this single control plane node that I'm running and if we then look at the application that I'm using um I'm ironically using another uh web server reverse proxy as my app so it's just enginex a common little demo app for somebody who doesn't write too much customer facing software anymore um this is just a deployment of enginex you can see I've got a bunch of replicas here which will be useful and then um here's my rolling update strategy which is pretty much default I've got uh progress deadline seconds to 600 just labeling it with version so I can constantly change out the pods uh when I change that version number and um yeah termination grace period so nothing else happening in the container spec it's very uh just vanilla latest version of enginex so if I look at the oh I'm sorry I glossed over this um this is the crds for Gateway API I am using an older version uh because my um Gateway controller of choice in this demo uh only supports Gateway from like two years ago so but it's good enough uh to display these Concepts so we've also got the r back and then here is our Gateway configuration so we looked at this earlier uh we declare a Gateway class we have to tell traffic hey my local Gateway class um would you please control that and then as the uh so this is kind of in for provisioning now this Gateway is a available to be um created as a class and then I want to select into that class and make a Gateway uh if I wanted to I could make multiple of these and try to put them on different port numbers uh in my kind configuration I have that Port uh forwarded here and then um now we just are going to be creating a single HTTP route resource right so this is something that you could break out into your apps folder uh you can make a bunch of these uh you can do all kinds of rules matching here you can put them in different name spaces and you can select them into that traffic um Gateway right so the Gateway is named traffic here and then the parent ref here is traffic uh the traffic Gateway is of class local cool well um I've already applied uh this directory to the cluster that's just K apply uh- customize and then the folder name which is katees uh and you'll see that we can just curl Local Host and then that is the enginex uh homepage or default stall cool so what can we uh do with this uh we can point out some problems with default kubernetes Behavior Uh which is always fun because you would think that when you install a state-of-the-art distributed computer and you start using it the way that it's documented that it would would be like a good thing to do um let's show that that's not the case I'm going to start a Siege Siege is a command that just batters my web server uh with requests over and over and over again so it's basically just doing that curl uh more times per second than I could with my own hands and um if I stop The Siege you'll see that we get some statistics here right so uh you know on a state-of-the-art uh little MacBook here we we should never fail or record EST right this is a very reasonable rate of requests to be serving all right let's go and try to do a rolling update which is like the quintessential uh kubernetes demo right roll the credits and then I need to let's just get everything visible here okay so I'm updating the label here thank you m k apply dask my kubernetes oh what just happened there that's not good is it that's a that's a 502 what's happening friends that's not cool so we talked about all this stuff uh this is like the worst thing to show up in a user's browser isn't it has this have happen to you raise a hands yeah oh no that's not fun yeah uh pod shutdown so the cube API server um has a bunch of legitimate reasons to receive a delete from a controller uh like when you do a rolling update you got to delete all the pods that were there before so you can replace them makes sense the Pod gets marked as terminating asynchronous consequence of that is the service controller has to remove the end point of the Pod sounds good uh remember how like we try our best to keep the endpoints list updated um of course that takes some time but the uh KU API server is really good at getting that information to the cuets quickly right and the kets they're like oh yeah okay like I'll just run any pre-stop Hooks and then I'll go ahead and send Sig term uh to PID one and then maybe something reasonable will happen uh it's not configurable but hopefully your application does something good it it does something good maybe about half the time uh and then termination grace period seconds starts and if that termination grace period seconds ends and PID one is still in the process table then the kuet will just be like yoink I'm going to kill that and rip it out of the kernel please um and so that's uh that's kind of how pod shutdown works the consequence of that is that uh most apps don't do the right thing when they receive a Sig term uh usually you know back in in the old days when you like logged into the reverse proxy and manually edited the enginex config like you would open up the logs and you would check that the traffic drained uh before you killed the process and um and that's usually not what happens so you've got to do uh this thing called a pre-stop hook uh either that or you have to change the way your app is integrated so you can configure it um so the the issue here is we've got an endpoints list we've got a bunch of things in our Network like Gateway controllers and CP proxy and uh a bunch of you know uh maybe uh NF tables and then some bgp lists inside of your router and and then there's like maybe some other controllers like the uh cloud provider load balancers uh who are trying to distribute all of the IP information about this pod this thing right here it's got an IP address it's getting propagated to all of these places distributed everywhere around your network there's no way you can know what the state of all of those things watching for this IP address are and then we Mark the PO as terminating and kuet wants to shut it down and so it gets ripped out of the endpoints list but nobody knows yet because they have to get that information but meanwhile we've already marked the Pod is terminating the kuet has noticed and now maybe this node has found out but now the Pod has shut down it received a Sig term and then just immediately quit and the Gateway controllers and coup proxy and some router that's still like on that bgp advertisement they're still getting that information to send packets your way uh this is a really bad situation that's where those 5 twos are coming from uh another creepy thing happens here actually if I just do this one more time I usually uh oh no rainbow wheel what is happening okay here we go and then if you like just let that sit there for like 25 seconds like you notice we're hung like I can't even make any more requests even though there are pods that are running available to serve requests and it's because I'm waiting for a TCP timeout and this will happen in a user's browser and you really really don't want that um it's a it's a bad experience if you wait a few more seconds it just like it kicks up again after the TCP uh timeouts actually time out there you go right so people will be like oh I can fix this with probes um so I made I made this patch over here uh to just add probes right there's a Liv livess probe a ren probe in here you got to be really careful with liveness probes are super dangerous by the way uh make sure you just like really ramp these numbers up so they don't run that often um but uh or just comment it out but people are like oh this will fix it it's not going to fix it right Readiness probe is going to prevent your application from receiving things when it's not ready yet when it's starting up or if it needs to temporarily close its doors uh but when you're shutting down this is not even looked at people say oh you got to fail the Readiness probes in order to make it start shutting down cleanly that's not the case it's it's not needed at all um I'm not going to show the demo because we're getting a little bit short on time here for this but what I will show you is the way to fix it in kubernetes um history we've always had to do this patch you might be familiar it's the pre-stop exec command and then you hope that this binary is in the container and then you you tune your network uh by doing some very sophisticated science which is like watching logs and trying different numbers um and then that is how you you make stuff work at a production ready level you will find this in every legitimate kubernetes cluster that's running Services uh I have some I have news I I've have been complaining about this and not doing anything about it because it's hard and it requires changing the Pod API for Years Tim Hawkin opened an issue in and then a bunch of volunteers got involved and they made an 81 file change to kubernetes core and then documented it and released it in 129 as Alpha if you go to my kind config in this repo I'm on the Gateway Branch by the way there is feature Gates pod life cycle sleep action you can turn this on it's off by default and then you can go to custom probes over here and you can you you can uh oh sorry you got to go to the alpha folder you can do this it's beautiful all of that work and and years of complaining and now you do not have to you basically write the exact same thing except this number is a real number not a string it's validated validated against termination gra period and you don't have to rely on the existence of a binary this is the best thing that's ever happened to the Pod API I'm so excited about it let me show you it works it it is so good all right all right um I will let's deploy the alpha uh configuration there get all those pods rolled out we'll run a Siege and then we will go into the deployment and then we will say hello scale zero down time okay do this do this let's roll it out look at that like they're they're sleepy the pods they're so sleepy they're just terminating there for for a long time and then and then they fall out of the endpoints list and the Gateway controller finds out about it and it's just continuously sending traffic to the stuff that it should be and then eventually these things just go away beautiful I love it yeah I got a story to tell you real quick before we wrap up here you know I was so tired I could barely give my update at standup every app update had this intermittent state where the pods could barely stand up red O'Reilly and I was ready kubernetes looks so steady turns out I was over zelli cuz my merage request is sending every roll out I am checking I'm seeing 503s I'm digging every hole out till I heard this song from Lee see this wrinkle seems simple when you muster up the principle multi no cluster Network cannot be invincible and service and endpoint the root of the point is controllers need them slower cuz computers disjoint I was tired could barely give my update at stand up every app update the in aitt state where the pods could barely stand up hands up off the keyboard I shipped the policy just to hook a simple three-word pre-stop B sleep now the users are hammering I know I'm sleeping sound the pods are stammering but like CTS are sleeping now see the sprinkles seems simple muster up the principle multi no cluster Network cannot be Invincible service and end points the root of the point his controllers need him slower cuz computer is disjoint my Ingress controller it comes with a Gateway class then I made myself a Gateway now I got myself a pass so fast deploy my apps check the graph it's just as planned Deb's cramming one more ymo so their traffic is just passed to their main rout handle not needing any brandle no stress TLS DNS and no mess HTP TCP UDP to quadruple got a protocol Tuple with that RPC from Google platform is simple a Gateway repo INF for Ops Dev in Persona Trio this is Leap with a recap to the rhyme is how you know this Gateway API let's let the traffic [Applause] flow thank you for listening to me rant so ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "tPUJgq9Nb9U": {"video_title": "The state of on-prem Kubernetes", "video_description": "Talk by Justin Garrison\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/state-prem-kubernetes\n\nJust because Kubernetes is cloud native doesn\u2019t mean you can\u2019t run it on-prem, but why is it so hard in 2024? There are lots of options but all of them have trade-offs. Justin has been building on-prem clusters for 8 years and will help you figure out if you need a project or a product, CAPI or PXE, and cloud-init or config management.", "transcript": "containers in a spreadsheet that's how that's how you do it right this is this is the CIS admin database of choice was a Google doc and I put what server it was on what port was open on it who to contact and and what the app was and so I was like hey if it's down this is what we do but when I went on vacation I gave that spreadsheet to my cooworker I'm say hey just watch this okay and if if someone calls you match it up here this how it works we're probably used to that and then I was noticing that basically every week one of my one of my container hosts uh running old versions of real Kel would Kernel Panic about once a week so I'd have to go in and and restart the box or restart Docker um reload the containers and I was like this is this is boring and I don't want to do this anymore I wish there was some automated way to do that uh this kubernetes thing came out and I was kind of terrified because even early on it had this like complexity sort of notion behind it I'm like I don't I don't think I want to do that uh this complexity is gonna be too much I don't understand all these components and I definitely don't want to do it if my Colonel's panicking every week and so I found this new operating system called coros with a fancy shiny like 4.0 kernel and and we're like we're going to run it on coreos I was like okay how do I how do I automate this setup of coreos boxes and they have cloud in it and does anyone do anyone been running kubernetes long enough to know the hyper Cube hyper Cube was yeah it was like like they took all the components of kubernetes the controller manager the scheduler the API server and they put it in a single binary this huge fat binary that you could just download one thing and the second argument would be like hey hypercube API server hypercube controller manager and so I would literally in system D curled A bash down the hyper Cube and I would run it in various ways and that was that's how I bootstrapped my cluster and you know of course I said I like it'll take me a couple Sprints to figure this out and I figured out a new operating system and enough to make kubernetes run and I had an API server up and I go I was like okay well let me just try it out I'm gonna run a container engine X and the command comes back and it's just empty and I'm like well that didn't work I'm like obviously I'm missing something this thing is way too complex I'm never going to figure it out and I'm like okay well let me go list the containers and see what happened and sure enough they were running and I'm like well they're not actually running there's no way it's running that fast let me port forward to it and and it's there like engine ex dra I'm like okay let me try scaling it up to 10 cuz that's that must have been cash there must have been something else something is wrong here kubernetes is too complex and scale up to 10 and it just works I was like I don't I don't know what's going on but I show my coworker I'm like hey I think I got this kubernetes thing working I think we can move off those stalker hosts that I have and we can start having the web team deploy to this and what he told me was that's cool we can't change everyone's workflow to do that one thing we can't we can't abandon the 15,000 lines of pearl that we have to provision a box to to allow someone to do it this new way so what you have to do is you have to integrate what you just did into our standard tooling so it took me three months to do that what I thought was going to be complex with kubernetes actually was was much more simple than backporting everything into our existing Frameworks and and tools and workflows to make sure that no one had to change anything and this would just work but eventually we got to the point where that was out web team could deploy applications automatically in a load balancer and these hosts were there and that was my first four months later my first kubernetes cluster and anyone that tells you that cloud native is all about treating your servers like cattle right like that's the thing is that if you're if you're a vegetarian you treat them like crops okay it's like not pets we want cattle not house plants we want crops and like that's the cloud native way and I'm here to tell you that I literally wrote the book about Cloud native infrastructure I have written the definition for the cncf of what cloud native is and I'm telling you that that is not true you do not have to treat your servers as cattle or disposable to get the benefits of cloud native the benefits come from a lot of different things but mostly from changing behavior and that is the thing you actually want to drive and when we take kubernetes and shove it into the old way of doing on Prem uh that doesn't give you benefits one of the main reasons that I see a lot of people moving to the cloud and I worked at Amazon and eks for quite a while uh is because they want to change the behavior and the process of all of their people and they say Hey what if all we had was apis what if that was it what if you cannot SS you can s SSH but what if like like to stand up an infrastructure you have to call apis what if everything was API based and that's how we're going to change the behavior and and so my first on-prem Gus cluster was absolutely pets because I ordered the machines and I helped unbox them and rack and stack them and they were pets they were they had identities on the network that I cared about and I was responsible for and these were the six machines in my first kubernetes cluster and they were pets on multiple levels because if you know any of these characters some of them are actually pets and so that was okay because I cared about the identity of the machine that I was responsible for the operating system and the workloads were disposable to me but someone else cared about the identity of the workloads right like that was their pet the DNS entry to get to their service is not disposable right because there's DNS cach and you can't just delete that and move it to a new load balancer because people were going to fail so there is all of these lay ERS of the stack someone cares about them someone is responsible for them and I'm just saying it's okay if you're running on Prem kubernetes and someone's like you have to make it disposable I'm saying they're making you do too much work figure out what layer you care about and that's okay to make that your pet or your the thing that is you're responsible for and maybe can't go away fast forward few years uh I'm still I still like on Prem kubernetes I like metal I like doing this stuff um I I have worked building it as a as an engineer and I've also worked as a as a provider of these services to other people uh when we were building eks anywhere at Amazon remember that doesn't work um I built a home lab cluster out of an old Mac uh G4 Cube um because I needed a lab and I was like Hey I want something that looks cool I want something that actually is is is neat and it was my pet and it still exists and to my knowledge it is the most expensive uh Rick Roll that exists because my my my server my node names are never going to give you up um because I just like to troll people sometimes U but that's okay like this is this talk is to show you that range of all these things that you can do I am not going to go in depth on a lot of the tools because I have very specific things that I have used in production I have built I know very specific things I know about a lot of other things and I would love to talk to other people that have opinions about some of this stuff too because that's how we learn things but first I do want to talk about why are you running on Prem um because that is an important question uh and that is just something that sometimes it's just inertia and maybe that's fine um and and there's lots of reasons you can have all of your own reasons and and this is usually the one that is the most because you do not make the business decisions about where infrastructure runs and you were told this is where we're running and that's fine that is a valid reason that people run on Prem um this it's also maybe because you you like it a little more like maybe you don't like eks or or something else and you're just like hey actually I I want to I want to do this and that's that that is a valid option even if it's for your resume um sometimes this is resum\u00e9 driven development but remember you have a business contract with your employer that pays for your time so don't go too far down that rabbit hole um there are plenty of other reasons uh like need to understand the components um maybe you need to tweak certain things and you like bare metal um maybe you don't like uh calling your service provider and getting API limits raised every week um these are maybe you just like to deal with the physics of it and have that be the problem right these are again completely valid reasons uh maybe just maybe you're just like I I need more tickets I'm going to Jin up some reasons that I need to do this um and and and maybe you just need want to understand the broader scope of the impact of something like this that is like the core infrastructure uh in your environments and you're like hey I actually want to go talk to the network team a little bit more um so this is my gateway to go talk to them uh one that I don't think is always a valid reason but maybe you're not um vegetarian and you don't do crossfits and you need some other identity like you need this opponent that's like this is who I am now and I'm going to tell everyone about this thing and um try not to do that one but I see it I see it pretty often uh yeah cheap it it can be cheaper and and I hear a lot of complaints right now people are saying that you're running kubernetes and you're only using 40% of your CPU right if you have a sunk C like if it's a capex spend and you the only way you can make that cheaper is to use it more right you already spent the money so the thing you can do is to use more of it but people are like oh kubernetes is only using 40% of the CPU I'm like have you ever bought a server from the last 20 years and had someone run one app on it like that's not using even close to that like this is we can utilize things more and and sometimes we do that through VMS and we say oh we can pack the VMS tighter with containers and we keep getting these smaller units of things that we ship and at some point they're like hey how much is enough how much is enough utilization how much is this might be cheaper and running it on Prem in the long term absolutely can be a cheaper cheaper way to go and so like I said this is for a lot of different environments if you are in a data center uh and you are responsible for the data center itself and the infrastructure um you will have things that exist in that environment this is not a we are going to replace everything with kubernetes if you are trying to replace everything in your data center with kubernetes we should talk because that is probably not what you think is the actual outcome of it like that's that eventual goal is not going to happen there are other reasons that people built the data center and you're not going to shift everything into it so you have to work with things that exist um all of those other workloads need to know how to get to the work you know this this stuff happens right we just need load balancers we need ways to discover these things and if you say well if you just move your stuff into kubernetes that is definitely the way to start a political battle at any company trust me I know uh those company politics are one of those things that come up and and my first kubernetes cluster basically failed for these reasons um because I was very naive about what kubernetes should be to people and how it should be run and and I I did not have any of the uh I wasn't I didn't know the reasons that we ran anything else like why can't it just be kubernetes um and basically got shut down if you are in an edge environments uh you will have different constraints and by Edge I just we want to get on the same definition here I Define this as something with limited capacity you run in an environment that isn't I mean data centers also have limited capacity but that limit is usually like how much power you can get from the power company or maybe How much cooling you can do um Edge is limited in other reasons like maybe your network goes down twice a day or is on like a 4G connection and isn't fast or has high latencies this is what I'm meaning by by Edge environments maybe you don't own that closet and someone has Hardware access to your boxes something you may want to consider if you're running this on an edge um and things like Cooling and what kind of boxes you get those are all important too if you're running this anyone have a home lab that they run kubernetes on because that's always okay cool awesome um other considerations here for if you're running this at home you probably have a family they like to see you I guarantee you whether they complain about your bad jokes or not um they probably want to see you at some point uh just beware um if you have pets not the server kinds but the ones that might chew on tables or or clog your fans with hair um those are are problems that you need to consider uh how much electricity you actually want to consume um because if you don't have solar or power is expensive this is a problem uh that super micro on eBay probably looks great uh if you've ever put one in your house or in your living room um you have other things you should just just talk to someone about this first before it shows up at the door uh these are valid considerations um if your if your cluster upgrade stops your kids fortnite or YouTube um again you are going to make enemies in in your house and don't do that uh but the one that I always like come back to is my kids don't know what a pager is and when I started talking to them about the on call rotation for this they were like get out like we are not we are not going down this road and you need to make sure everyone's on the same page of of how much responsibility this is going to be all of those environments have varying ways you have what you're running kubernetes on what the OS is actually provisioned to uh and I'm breaking this up into a few options of bare metal but Hardware you just literally like the kernel is the closest to the hardware as it can be right we're not virtualizing it this is just Hardware uh and you have if you are responsible for the hardware piece you also are probably responsible for these things and in this case if you are responsible for those things I'm also saying that your servers are probably pets this is something that you care about the identity on the network the Mac if you know a MAC address of any of your machines you probably care it's just an easy way to determine if you care or not if you're virtualizing it then you say someone else cares about that Mac address and I care about the virtual Mac address and then you kind of care about these things and and that is that is maybe better for you if you have a VMware team tell them good luck on the migrations if if you don't have a VMware team um then you might also be responsible for this piece uh and then again at home like just make sure you know hopefully this battery doesn't die I don't see any batteries in here but we'll get something uh I love using Enterprise old Enterprise Hardware like I think that the micro desktops and even old laptops make great kubernetes nodes um just FYI I've I've been telling people about that for a long time and even though I have some raspberry pies almost none of them are part of any of my kubernetes clusters so now let's jump into hey you've decided that you're going to do this this is something that you're like actually no I'm all in like this sounds great still like I still want to go with it there are some options for uh provisioning which is specifically like the operating system provisioning usually some of these will tie into like hey we also do the top level bits of we'll put things on the OS um but in many cases these are like hey we're just going to get the OS on a box oh thanks what is wait oh this is actually like servers uh whether that's bare metal VMS whatever you probably need to you need to install a kernel at some level and and these are some options uh again I'm not going into all of these in depth because all of these are are classes on their own and complete talks um I will say that these are all usually tightly coupled with the operating system they you're deploying so don't give yourself more work if you are a predominantly Ra Shop do not use ju guu as much as you might like ju guu and yes you can do it and you can provision you can cross provision a lot of these things uh but the templates and the defaults are going to be much better if you say hey I like this OS so I'm going to go with that operating system and that vendor stack because that will save you a lot of time and and provisioning uh coros or now flat car uh systems with something that wasn't Matchbox um was was much more difficult uh and then at the bottom here there are some generic options which like aren't tied to an operating system um again those are usually going to be a little more work upfront you're going to have to learn a little more things you're going to have a lot more flexibility which also equals more work okay so just beware that whatever you're picking here for for your environments uh find the one that's going to be easiest for you usually or the one that already exists I gotta stop clicking on it there there are lots and lots of products that will install kubernetes in your environments this is some of them this is not all of them uh this is just the few that I know that have been around for a while we'll we'll talk about a couple of them a little more um oh by the way I'm I'm completely biased this talk um I I just want to say like this is not a completely neutral talk I have done a lot of these different things um I have used Cub spray I have I have set up open shifts uh I help build eks anywhere I now work at TS on Omni um so like this is absolutely a biased talk I am sorry I again I love to hear your opinions on things but just just FYI everyone has some biases um if you if you are looking at any of these things uh you probably know at least the basics of what some of them do um but I'm not going to go into all them I will save you a little bit of time here and just say that if you are not using one of these uh stacks on the bottom if you're not already in or gke customer you probably don't want to use their products to like provision on Prem just FYI and help help build one of them um if you have never read any news about broadcom you may not want to use tanzo um just FYI uh just if if you are looking at tenu I actually think it's was a great product when it's was coming out I thought it was a lot I have friends that work there um there was a lot of cool things they were doing and but in 2024 and you are getting started go read some news first just save yourself some trouble um and and for anthos it's like whatever the PM wants to rename it next week it's gke Enterprise now I think um but there's another name for it um so yeah um but again uh we'll talk a little bit about them nanic if you're not already using nanic to manage some of your stuff again it's probably not like you don't want to jump into that ecosystem to say I just want some kubernetes mixed in with what the other stuff I have so the other ones might be a little more of a hey I I just need a little bit less of an ecosystem um and I I just want the kubernetes pieces I mean also for like some of these like you have to be Enterprise level customers of them to even get like sign up for support so like Rancher and Omni and open shift like you can just go like say hey like I don't actually have a real contract but I would like open shift support that's that's okay you can get minimize the amount of support you're going to get from them as well is is a good option almost all of those break down into one of these Technologies um QBE ADM is ifone use QBE ADM is a command line tool that will like start a cluster for you uh it is embedded in a lot of other tools uh it is if anyone remers a cube up script Cube up. sh was the original bash script that was like 3,000 lines and it would like stand up up a cluster for you in a cloud environment and and we were running this bat literally curled a bat this thing and like hey I got a cluster this is amazing and someone's like actually let's let's make that a little more generic and a little smaller and QBE ADM was kind of the outcome of that was like hey we can take any operating system we can bootstrap your shirts your database your API server all that stuff it has these stages QBE ADM is is kind of the way a lot of people start if like it's the manual way it's that cloud in it and ignition is obviously like starting up a a system that's like I need some configuration tell me how to configure a lot of people just shove cadium in there that's like it's kind of one and the same at that point when you say uh I want to configure this thing if you're if you're really good about it or or some of these products will do this well they'll they'll tend to bundle that up into like a Packer image so say okay now we take that VM we Cube adem it and then we roll it in Packer and then we can deploy the Packer image and that's how they're going to you're going to get a fully bootstrapped OS ins environments and that's OS and kubernetes uh so that's these these are just stages of kind of the same thing so far API based deployments are are kind of they're kind of missing in a lot of the scope of these things are like I just want like this is what a cloud provider would give you if you're running somewhere else like eks give me a cluster I'm not going to do any of these other things and that's kind of the interface you're going to get and that's what cluster API was supposed to be is kind of like cluster API is kind of a unique Beast because I feel like kubernetes we have forgotten how to make an API that was not a crd and we have forgotten how to make any API that we can just call that didn't later require 18 controllers and etcd to store the data and apis are kind of missing in all of this and I can I can tell you right now that all of the cloud providers that give you a maybe good experience hopefully reasonable experience if you're going in a cloud environments uh do not use cluster API because an API that's well defined to their service is is a much better thing than trying to Cobble together cluster API to do everything and for most of those providers on the last slide cluster API is actually just Cube admv SSH it's literally um Cube spray does the same thing but where the SSH comes from is different so with Cub spray you're going to do it from your laptop and with with cluster API almost all of those providers are we are going to do this on if it's bare metal they will say you give us the IP address and an SSH search and we will cubm SSH into that box and we will suck some data out of it so just just so we like fully understand here because because again bks anywhere was built on cluster API it seemed like a good idea at the time this is what you have to do you have your laptop and you say I want a kubernetes cluster and the very first step of anything cluster API related is to run a kubernetes cluster and and this is a bootstrapping problem and some people think it as a good thing and those people are probably language developers that want to compile you know rust with rust and they're like oh it's really cool once rust can compile itself and it's like it's what really cool once kubernetes can compile itself and I'm like no it's not always the case and so you have to run kubernetes that call some apis maybe if you're VMware but most likely a lot of these are we're going to SSH to a box and run cubm we're suck some data off of it and then and and we're going to put this back in our little kubernetes and then we're going to move it over there and we're g to run all the controllers and we're going to run all the crds into that cluster we just made so it manages itself I thought this was a good idea at first no no shade to the cluster API folks they have done some great work I think it's really cool I do not want to be running it and I don't want to be responsible for it because if you lose access to those things uh cluster or um controllers are kind of hard to debug sometimes if you need to upgrade this cluster you now need to suck you need to run another cous cluster suck the crds out of it run run those controllers locally again go through and upgrade the cluster and then put it all back there it go there's one and and then throw it back your laptop is now like production critical and we have learned lessons not to do that in the past and in many cases we are doing this again and so the brightest idea we have for like well don't make your laptop part of production is we say okay what if we made another kubernetes cluster and what if we put the kubernetes cluster over there and it was responsible for the other kubernetes cluster and then that kubernetes cluster makes all of the kubernetes Clusters cluster API is not for people that have even dozens of clusters this is for providers that they're like hey I want the flexibility to deploy a cluster everywhere because they say hey I can I can take out some of the abstractions of what kubernetes is and I can put a bunch of specific details in what that kubernetes cluster is you say hey if I want to deploy this to five different service providers maybe cluster API can condense some of that but in reality how are you going to upgrade that cluster there in the middle like you like any other upgrade you've ever done to any core service in your life your DNS your DHCP do you want that in a kubernets cluster that you have to upgrade likely three times a year like the last time I upgraded core Services was not three times a year so let's talk about the other options briefly again I am not an expert in all of these things and I have biases cubes spray is great because it's a neutral product it is anible based if you've ever used anable which is SSH and Cube ADM and it just says hey I run this SSH from my box again you're putting your box in this production workflow but maybe it's also Jenkins who knows the cool thing is because it's just QBE ADM it there is no operating system like this it works better with this one it's like if you get SSH it's it's probably going to work and this is how a lot of people start and this is fine you get an OS on something Cube spray to it I have a kubernetes cluster that was cool this is not cool when you're like that cluster is now 50 nodes or a 100 nodes and you're like actually SSH is really slow and it's fine when I'm using it as a person because actually I'm slower than the computer but when you're trying to automate it and that's running forever and you get all this yaml output and I just like like I don't actually want to look at ansible Playbook runs anymore um please just make someone else do that if you're getting started and you have smallish requirements Cub spray is probably the right option for you um if you have Edge locations again with like network connectivity like SSH isn't great at like oh Network dropped like let's let's start over um anible can do some of that stuff but again you're you're dealing with State on multiple levels of how you sshed in where QB ADM was with it where anible thought The Playbook was there are lots of things to consider here while you're trying to do something like that how many people are using Rancher that's one of the most common things that I see in in a lot of places because Rancher uh has been around for a long time and has a lot of Integrations and a lot of features and it's k3s based at least AR I don't know I used Rancher a long long time ago and then I started using rc2 as like testing it and seeing how it worked I don't know anything that happened in the middle um but it does have a lot of features a lot of the features with rk2 is again it's cluster API based so it's like hey you can deploy Rancher clusters to anywhere and in this case that's what's the Tool's trying to do and it's saying hey I can abstract away some of that stuff you give me the yaml and we're going to put this anywhere right because everyone likes riding yamal um some of the like on Prem provisioning stuff was a little bit lacking last time I tried to deploy this to Bare Metal again it's like you could you can do the qb8 msh thing uh they also have elemental OS which is open Susi based which you kind of build isos and then you pixie boot those I don't have in-depth knowledge on how rk2 is working uh but if you're in that boat and you need a lot of Integrations and you want uh proxy support for like who controls things and rback it you should absolutely look at Rancher k3s based um someone else can tell me this but like it's it's a great tool it is low requirements for a kubernetes cluster you don't actually need nodes to run the actual backend which is usually a problem um but and and that's awesome because that will probably scale more than most people need but if you do want High availability and you do need five nodes uh the story there to get Upstream kubernetes I don't actually know that piece I don't know how to make k3s stream across nodes I know they have backup things for SQL light that's a question that I don't even have open shift in okd uh as as raner had lots of features open shift lots of opinions and if you are deploying open shift you're going to get everything you're going to get the cicd you're going to get uh image streams you're going to get build packs you're going to get custom CLI and red hat will tell you all day every day that it's just kubernetes and if you are using open shift for just kubernetes you are you don't need open shifts and you are paying way too much money and probably doing too much work if you're in the real stack the red hat stack of things I guess the IBM stack I don't know it's coros Bas now it's a lot of things that get mixed in there I don't know if you're in that stack you probably know what it's like to run Red Hat enterprise software and and how those upgrades work and every time that I would upgrade our rev servers which I think rebranded now uh or our directory servers or Foreman um those were big events and I don't like having pieces of software that have big events to do um but if you are in that space this is probably the right option because you already have a support contract and if you have zero opinions this is the one to look at I will talk about eks anywhere just because I'm very familiar with it and to my knowledge it's the only cluster API based provisioner that is not SSH with QB ADM because with eks anywhere they decided that we wanted to own the full provisioning of a server with bare metal there is also other providers that are Cappy based but it runs Tinkerbell and Tinkerbell is a a generic OS provisioner um that came out of uh equinex metal uh and and and so they run tiker Bell and it's a very interesting tool and it is a very complex tool and when you put cabie on top of it uh and you run it in kubernetes um again I do not think that every tool that you're running in your data center should be running in kubernetes especially the core services that are required to spin up kubernetes and EK anywhere has the same kind of model of admin clusters that create other clusters um surprisingly uh EK anyware doesn't run any of the Amazon Linux dros uh politics um so we we originally shipped it with auntu um and then we had to shift people over to build their own auntu and it does support real now I think um but it's it's it's a it's a long process to get to a cluster and and I was training people on this for over a year and when I've come to discover when the first step of I want to deploy a kubernetes cluster and I say give me a MAC address um that's probably the wrong like question it's like you get to make a CSV file with Mac addresses um is not how you necessarily should like that's not the best practice it is it is an old practice it exist for a while but when you're doing this inside of kubernetes just maybe not I do now work at a company called c a lab we make a tool called Omni um the thing I I knew about Talos Linux seven years ago when I when I was still building my on-prem kubernetes and the interesting thing about it was the fact that the operating system only has an API there is no SSH there is no the thing that we wanted people to do to move while we move to a cloud was say everything's API based like that's the thing we want and we want people to change behavior is hard to do when you still give them all the other stuff and and tals Linux only has an API and it only works with kubernetes so this isn't a generic purpose kubernetes this isn't a thing that you're like oh we could actually just use this everywhere it's like no like you only use this with kubernetes there are I I literally when I started the company I went through and I counted there are 12 binaries on the entire operating system and that's interesting to me because I could sit down and tell you all of them and I can tell you what they half of them are for formatting discs that is because it works on bare metal and because it works in environment it's like hey actually we just need a make a like a file system and so like that's how lvm and all these other things work and so we pull those things in but like like three of them are just like well here's the API and you can deploy this and that's like oh that's what I want from AWS is I just want the API to do the thing um and and Omni like controls all that the one thing that we it's it's available for home lab use um or or test environments but it it gives you an API that's not cluster API based that is just like a standard API like I can just call an API and like deploy an operating system and kubernetes uh without baking things and without scripting things and without doing all that other stuff um which I really really enjoyed the first time I did I'm literally building kuber or eks anywhere and I try out this thing and I'm like wait a minute this should be harder like all the complications we're building it into ourselves is not necessarily the thing that you want for an on-prem environment in kubernetes again it is complicated and I'm a testament that it it is it can be as complicated as you want it to be and as integrated as you want it to all those other things but the complications are probably from all the other things you have and because you're trying to fit that new thing that the behavior you want to change into that 15,000 line Pearl script and once you say like I need c to work with that then you're probably not going to get some of those benefits and and that's the thing that I think is is one of those critical important things a lot of people just miss like we just want kubernetes in the way we've always been doing servers and so that's that's my opinion on it I could give a demo I don't need to give a demo I don't always like giving demos and I actually don't even think I have it set up right now it'd be hard to do with one hand so I can ask answer questions if anyone has questions about on Prem kubernetes um and again if you're afterwards if you want to talk about some of those other options that I talked about and I don't know about if you have another like I think this one's great please tell me about it um and since my other mic is dead and I have no Proctor in the room I'm going to be running the for you uh with cluster API you mentioned you can provide so I looked at cluster in the past it seemed like to the points I think you were making earlier you have to feed in a fair amount of configuration to get it working where it kind of felt easier using terraform and it sounds like there's another option where you could use cluster API if you just already have nodes set up and it just uses QB ADM over SSH is that correct yeah most of those providers will will SSH to node you give it IP addresses and SSH he and it says okay I'm going to go SSH to them from a kubernetes pod spin that up I'm gonna sent I'm gonna run qbm steps and then we're gon to suck some data back and know the state and it some of them like Rancher will install an agent and it will report back and they say hey I'm going to keep this state going some of them will do it with a workload it says I'm going to deploy Damon set and we're going to suck that information back they do it all different ways but like the initial setup is like I need SSH to that box and if you are doing kubernetes and you like I need a full general purpose Linux uh then then like you might be doing too much work and system D I think is a fantastic init system um but it's also makes a lot of work like it just adds a lot of bloat to a system that's like actually I need to run the cuet in container D maybe I don't need everything and and that's one of the really cool things about some of the minimal operating systems I like let's just strip that down a little bit so I've seen different things pertaining to raner OS and okd and um so what what exactly would be the difference between them because I know they both have their monitoring something or other and they both have their cicd something or other so what would why would one choose one or the other what would be the difference and I would say if you are on the on the ra stack choose open shift if you are not you'll probably want to choose Rancher like that's like the main difference and I mean they're both like Rancher is going to be Susi based and you're going to be a Susi customer the hardest thing most of the time with on- Prem environments for me was I had to go talk to a lawyer to like buy support somewhere and like that's the thing that took the longest a lot of times it's like I need someone like to be able to support this because it's a production workload whatever reason it cannot just be my team that supports this and whoever you have relationships with usually is the one that I say go with that one to start and I know a lot of people that open shift was a great starting point as people started containerizing things and they have all the build packs and they have all the stuff that's like hey we can move this to open shift really quick and then they they find out they don't like some of the opinions and they say oh actually like I I want it to be different I want to use Jenkins for whatever reason um I want to use something else and once you have once you have your own opinions open shift might confine you a little too much um I I still feel like open shift is opinions and Rancher is is features and Rancher has a few more that are flexible that you're like hey I can use your defaults but also I have a little more flexibility in how I tie that in um so another one I guess on tallos in particular uh we're getting requirements from some of our customers for uh fem and I know nothing about fem but is Telos or some of these newer methods that use like flat car Linux compatible with those methods I will I will generically answer that in that most of those we have a requirement for some security thing we need validation on this thing uh the more you can push away of like it's not possible the it takes longer for the rest of whatever spec to catch up with like it's not possible one of the reasons we had a lot of people in adab use fargate was like hey you just can't do that anymore you don't have you don't have any access to the host so that problem goes away and we had a lot of people are like hey I love problems going away and and that's one of the things with something like tals that has no SSH that's like what do you need to do it's like oh we need to make sure SSH is secure literally Talos is not in the um the AWS Marketplace because AWS Marketplace requires us to verify that our SSH keys are are like secure I'm like we can't there's no like I can't do anything about it like there's nothing we can do and so like there is this moment of like hey this just doesn't it's not a thing anymore because we've thought about this differently and it does take time for that stuff to catch up and so if you have a hard requirement on an industry standard or something that's like you need this certification or you need this other thing that stuff just takes time for most of the industry to catch up on any sort of you know certification um so I don't know the specifics on on that requirements but I do know that like almost all of them it's just like it just takes a while for them to understand like oh we have to do things different every like security vendor um and and things in the past that were like hey uh we want to we need to validate this thing I remember one of our requests early on was like we had a requirement on active directory for all of our servers even the Linux boxes and they're like hey we need you to run all of your containers and join them to active directory because that was the spec that said hey this is how we validate things and this is how we have controls over things and I was like no you you will not like you we will have other problems in the space even if you're adding these pods into your generic pools of IP addresses like these are ephemeral and and we don't want them to come up good IP go down and and join night like all that stuff was goes out the window so like there's just a lot that catches up in any of these environments and as you minimize your responsibility you have to just have that conversation of like hey what are we actually doing here and in most cases All The Damons that were like hey we need kernel modules like how how are those kernel modules getting there like well we do it in the bootstrap process it goes in Packer or we ssh in it's like okay well maybe we don't and you know ebpf and some things are catching up there's still a lot that is not there most of the tools that don't work with Talos right now are because they exec into the host and they expect binaries to be there and they're like Hey where's my bash it's like your tool doesn't work because you literally shell into the host and use bash like that is just SSH like you just did it the same way thank you uh coming from a non- kubernetes guy just learning uh you I think you kind of touched on maybe cyber security Frameworks that these environments need to match up with um would pretty much a lot of this be uh figured out in development kind of seeing what works what does and then you know you have some end goals of maybe lining what the industry needs or the company needs for Frameworks this would be settled in development before re reach production I mean for sure if you have a hard requirement on some security specs or some uh benchmarking tools you do need to obviously figure those things out ahead of time um a lot of the conversations I've had at Enterprises of like we have a security spec is like okay what are you doing and like well we had a hack on this thing a long time ago and now that now we have a requirement to check the box because that thing happened even if it wasn't to us it was to someone else right and I I worked at Disney when Sony got hacked and we went through and like audited everything because we're like oh what's going on like oh actually they mostly ran windows and we didn't run any windows and it's like does that mean the security is not a you know not a problem for us no absolutely like there's still things that we wanted to make sure but we had to check some boxes to make sure that that was all right and in any of these if you're deploying this in production especially if it's going to be publicly accessible by someone um you do need to make sure that the Baseline security is there but again like I was saying like a lot of those problems might just go away because you don't have root on a box you don't have a shell on a box if someone wants to pop a shell like they can't it's just like literally like oh it's not there um so yeah figure those out in developments if you want to kick the tires especially on something like uh like you want to do multiple layers like the kubernetes layer is its own security thing and I think we have a kubernetes security talk later day um those are things you you'll want to look at as as you're going through this but figure out who's responsible for which layer because that's a very important thing and one person's not responsible for the whole thing because there's too much uh I'm gonna take one more because I actually we have the vendor shout out and I did forget um I think it's in this room I'm really sorry we had three cancellations for speakers uh so if you're looking at the schedule on the printouts that was printed last week um look at the online schedule because unfortunately we had some people that didn't make it um and and got sick and so yeah please look at the schedule but I want to give time for the vendor as well so I work for an organization that is running uh on-prem uh KS clusters uh we build them manually um mostly through ansible um they're very I just started about six months ago they're very firmly in the camp of this is cattle not pets uh but I'm the backup administrator I want to find a way to be able to adequately protect these clusters so that we don't have to try to rebuild them from scratch something they don't do very often and thus is largely unproven what's your opinion on that the first thing I always ask with backups is what are you what are you trying to like prevent what are you try like like there's data loss there's uh crypto jacket whatever like there's there's something that's happening there like I need to back up something for these reasons and there's a reason behind the backups and usually that's like Hardware failure we get a new hard drive we have to restore the hard drive uh workloads that took someone some amount of time maybe days or weeks to create this thing um now we don't want to have to make them redo that again and kubernetes again it it depends on which identity you care about and if they're redeploying clusters if they're redeploying the OS and the workloads maybe you don't care about the workloads because the state for that is in a database and you care about backing up the database and not the pods themselves the operating system if they if it's ephemeral or if they throw it away or like like some of them are readon operating systems like you don't need to back like if you're backing up a readon operating system you're probably wasting your time just because that's not meant to be uh backed up it's meant to be provisioned on the fly but if you're doing like disaster to recovery and you're saying hey actually we don't know if our NFS is going to be available when you try to redeploy and all this stuff pulls off of NFS so is is production going to be impacted because NFS is down we need to do a backup or we need to have a disaster recovery scenario for to prevent that those are the questions I usually ask and say okay which layer do you actually care about because you can go all the way down to like I'm backing up firmware on the boxes right like maybe you don't care about the firmware like there's some layer in there that like actually this is the thing that I want to prevent find that one there are kubernetes workload backup tools that'll say like hey we'll suck out your your workloads we'll find any provis um uh volumes that they're using if it's stateful we'll do all that stuff in the kubernetes layer there are obviously lots of tools especially if you're on like a VM host like I'll back up the disc the V disc or whatever it is um to get the host but you might be just storing too much stuff if you don't care about it but the the thing that I found really interesting was the fact that they're treating it like cattle and they're not redeploying frequently and that's that is that is something that maybe they're not treating like cattle maybe they do care about something and they just want to think that they're doing it the the right way by saying this is cattle and and that's the thing that I would push back on them is okay if this is your cattle I have a new box create me a new cluster and then figure out from that how many things did you not have backed up because they're going to have a get Ops repo or something a bunch of yaml and a bunch of anible and they're say okay we can deploy it anywhere anytime I'm like okay how'd the OS get there who is responsible for that how the network connections get there how the load balancer get set up and if those are the things that are the gaps maybe the backup strategy is automation on other things and not just that layer so that's what I would push back on them and say like hey if you say you're doing it this way let's talk and let's figure out what the what we're trying to prevent uh so again I'm around all weekend uh thank you very much for coming and um I want to give again vendors and sponsors are why you're here please stay for two minutes because they paid money and and they also want to talk to you and you're probably going to learn something thanks d ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}, "x2-pu42us40": {"video_title": "Pipelines: Everything Everywhere All At Once", "video_description": "Talk by Christian Hernandez\n\nhttps://www.socallinuxexpo.org/scale/21x/presentations/pipelines-everything-everywhere-all-once\n\nWhen we talk about CI/CD, we often think of it as an end to end, linear, process. With modern cloud native computing, this ceases to be the case. The reality is your pipelines are hyperdimensional with many branches that can also have it\u2019s own hyperdimension as well. This becomes a problem when dealing with GitOps workflows. Leading open-source GitOps platforms offer little to accommodate modern deployment pipelines. This talk will zero in on an alternative: patterns that address these challenges while remaining true to established GitOps principles.", "transcript": "all right everyone welcome again um if this a slight schedule change so I actually did put all this together like kind of last minute and so um hopefully you'll go on this journey uh with me so there might be some bumps might be some things and so um a little bit about uh who I am I am uh my name is Christian Hernandez I am an Argo City maintainer an open get Ops maintainer and I am head of community at Acuity um acuity is um a company that was founded by the creators of the Argo project and so um you know originally they were added to it and then they uh broke off and started a startup so um if the name is new to you um uh that's why so I'm also a guitar player so um if anyone wants to N Out about music um you can catch me after uh my website where you can catch all of my talks presentations demos or you can just follow me on the socials here um as well so um what I'm here to talk about so so um a little bit of warning uh this is an advanced giops talk so uh what I mean by that is actually let me kind of like gauge the room who is using flux or Argo CD currently just kind of quick show of hands right okay so yeah um so cool right and uh the rest of you um feel free to ask any questions you like I might lose some of you because uh this uh talk has a lot of assumptions that you are familiar at least familiar with Argo City or flux and get Ops practices in general so um so just a warning and um and again my apologies I threw this together a last minute right so but what I'm here to talk about is pipelines right uh pipelines cicd all that good stuff and when we talk about cicd and when we talk about um deploying applications at scale um this is the kind of picture we have in our head so we have you know something there's a PR right or something gets merged and something needs to get built right typically um you know an application when we when we say you know build it could be a compile of an application uh Us in the kubernetes land right especially in you know uh my life that means build a container um so something gets built and it gets deployed to Dev right gets deployed to some environment out there somewhere and uh you run some tests right it can be um you know some some code tests some um you know um user regression tests it can be user acceptance tests whatever right you run testing against that and whether or not that happens uh you know whether or not it passes or whether or not you need to make changes you know whatever let's just assume everything's a golden path you then deploying to some sort of staging environment maybe it goes through a few more environments but then you go to the next environment um and that environment uh you know it's kind of like an area where you're like okay let's see how this is going to look like in my environment and you know after I passed all the tests but how is it really going to look like and then it gets to point to PL uh to prod and everything you know this is the kind of the picture and kind of process we see in our heads a lot of the times uh that's the expectations but the reality is much more complex than that and so um you know there is uh you know someone makes a config change someone uh makes a application change it runs some test it gets into staging um you know you run a canary um you know one gets approved one one gets um you know rejected or they both get rejected for some reason um you know you throw you throw it into like the the canary release and prod do some manual analysis and all these steps have other steps right so pipelines can trigger other pipelines which trigger then other subsequent pipelines and so on and so forth which is kind of the um the idea of uh you know behind the talk right everything everywhere all at once like you know a lot of stuff is happening and a lot of stuff needs to be orchestrated right and um and so really that's the reality of cicd right we we kind of think of it as a linear uh process we kind of think of it as you know like you know go from here to here but in fact it's kind of you know it branches out um almost infinitely right um and so so um and a lot of things could happen and so on and so forth right and so in the context of get Ops CI is being overused and um CI and CD aren't aren't the same thing CI pipelines and CD pipelines are not the same thing and so especially this is especially true and I put it like in small kind of caveat letters like in the context of get Ops right if you're using something like Argo CD if you're using something like flux if you're something like a get Ops process where you're using you know something like G as a source of truth right like even if you're using infrastructure as code right that that kind of the that that the same idea applies right not just necessarily to get Ops right but um CI and CD have different end goals and so we we kind of lumped them together um and you know for for the time being right like Jenkins was like you know the big thing because it it was lump together you did everything with one platform um but a CI pipeline its goal is a given given code build me an artifact right uh the idea is that they're short-lived um you know it's predefine beginning and end done once a job is successful whatever that job is um it's smaller in scope it's rigid top down definition um you know it's it's suitable for you know short like you know I'm compiling something right or I'm building containers like you know boom boom boom um a CD pipeline is given an artifact can you roll it out safely for me and the the the the timeline is it can be indefinite we don't know you know it could be very very short it could be very very long um you know you need to do Canary manual checks you know approvals um and it's only really done until the next deployment starts because um and you know I know some canaries can last for weeks right of some you know you're you're you're you're rolling out with a feature flag and you're you're you're uh testing a subset of users right it could just take a long time they're larger in scope and requires flexibility right AB testing hot fixes roll backs continuous ver verification right notifications all that stuff um and so um so they have different processes and in a traditional workflow right in a traditional workflow where we're we where we're deploying to VMS right uh that all made sense right and now but now we are are in a um in a time where kubernetes is being adopted right it has wide adoption but it's continuous to grow and now we have kubernetes that is a declarative infrastructure that is an asynchronous process uh but cicd hasn't caught up to that because it's a a synchronous process and so um so um so what about Argo C and and this is true with respect to also flux as well um but uh Argo CD deploys um changes to a single Target environment right and if if you're familiar with application sets and and applications um you're still you're still only creating an application with application sets so you you you may think that you're using application sets to deploy multiple environments in the end of the day you're just defining other applications right but it doesn't do things like understand relationships between multiple deployments right Aro City doesn't really divorces itself from that process um it doesn't provide a pipeline to orchestrate changes right it basically says it's looking at git and it's making it true right um and for those who are using infrastructure as code like terraform or enzi or whatever uh from get same idea right I'm just looking at get and I'm applying it here it doesn't that's that's it doesn't really have a pipeline um it doesn't write changes to get Argo CD or flux does not WR back to get it just looks at your source of Truth and makes it true on your system um it doesn't verify anything it just garbage and garbage out um that that was the design of Argo City it basically says hey you know you give me garbage I'm going to deploy garbage and it doesn't really verify those changes and the very little verification does is very primitive because it's it's solely based on the kubernetes health of the environment so oops uh so what does promotion look like right what what does it actually look like in the context of get up so um for those who are using you know Argo City or flux for a while especially for those are using in production what does that actually look like and I actually had this conversation with someone um in um it was in Amsterdam right so it was in uh cucon Amsterdam they're like I have I need to orchestrate all these changes using get Ops and it's really hard right because if you think about it a promotion with getop kubernetes in general but think about it with get Ops um it can be one or many image builds and or one or many get repo commits and or 10 many uh Helm chart updates collectively everything again everywhere all at once your application deployment is actually pretty complex um and it has to do with all of these things have to be orchestrated together your image builds configuration change based on your image builds uh Helm chart updates based on the configuration changed based on the image and it can Cascade because who runs um only a one or two tiered application right in production it it's it it can get um it get worse right um cuz now when if you're doing microservices it only compounds that problem also you're trying to her cats of a different species as I as I say when in giops right um working with people who are using gups you're you're not only hurting cats but you're hurting cats and tigers and and lions right um a bear is not a cat but um you can um see where I'm going with this right and coordinating a release coordinating everyone to do a release somehow got harder um with all these new tools and all these Technologies right so um we here at Acuity um work with a lot of you know creators of the Argo project all our customers are Argo CD users um heard the heard this problem and heard it really really loud and so um so to that end um Acuity open- sourced a tool called cargo right which is why I'm wearing the shirt this is the only shirt I actually made this they didn't actually make me this shirt um it's a an open source tool completely uh free and open source tool um that does multi-stage get Ops application life uh life cycle orchestration so the idea of cargo is to take all the things that you need to do for promotion and collect those for you and then hand that off over to uh currently Argo CD there's nothing uh stopping actually adding support for flux it's very very flexible um we actually hired uh some xwe Works uh employees recently and so you know we're we're we're making this a generic get Ops orchestration tool um you know and hopefully extending it out to things like terraform as well or other infrastructure as code tools right so um what I want to do is I want to go through each component of of cargo explain you know some of the like basic concepts um if you're not familiar with Argo CD or or flux um some of the con concepts of cargo um may not completely make sense um but um I also have a demo prepared so I worked on it last night and this morning so fingers crossed to the uh demo gods that they're nice to me and um so that way you can see it in action and kind of see um the the process play out with cargo so uh but first I'm going to go uh the cargo Concepts right so one of the things that cargo does is it creates something called a project I don't like this term because it's such an oversaturated term however I lost out on that battle um but uh project is really a a collection of related cargo materials right on the back end it ends up being uh kubernetes names space but it's also like a point of arback and um how to uh do your promotion Pro uh policies right so like given a um uh given a project like you may have you may want to promote things automatically on some in some environments others you you want someone to manually approve those as well so that's kind of where the project sits kind of like at the head of of everything in cargo and so the next important concept is stage we actually went back and forth about calling it an environment or a stage and actually um we we found out and we actually we actually interviewed a lot of our customers and interviewed a lot of people is uh when you say environment that means something different to someone right you you say you you ask an infrastructure guy or an operations Guy what's an environment they're going to start saying well that's like you know my vpcs that's my firewall that's my um you know storage system lives there that's you know I have I have u u um security rules built around that um you know that that sort of thing but when you ask a developer like what's what's what's an environment he goes well that's that's where my application is running right and so we actually we we first started with environment and people were kind of getting confused it's like okay actually what we have a concept called a stage right and a stage is an application's instance purpose right not as location because people tend to think about Dev stage prod as a location or as like a a boundary of some some sort and well in this context of applica application delivery we we mean it as its purpose right what's the purpose of this application is going to be in Dev um and uh again with uh promotion policies uh progression from stage to Stage Dev test prod can be automated or automat automatically triggered and it subscribes to something called a warehouse right and I know these These are really abstract Concepts so I'll try to explain them as I go um so you subscribe to a warehouse so what is a warehouse a warehouse is where you store freight yet another concept I need to explain so what is Freight so Freight is just artifacts right so um I explained um uh an image uh a container image um some sort of configuration and some sort of Helm update that collectively together we call it a freight because we're shipping it um down the line right we promote these things together um these artifacts so we call those a freight we need to store those somewhere so we store them in in something called the warehouse um and it's basically sources from which a stage obtains a materials right obtains these artifacts get Helm image repo that sort of thing um and that is uh what I call a freight and so um promot uh promotion mechanisms right so promotion mechanisms is okay if there is new artifacts for me how do I promote this like what does promotion look like what what what is that um uh what do that promotion aspect looks like and currently there's two ways um is to write back to get so basically saying okay there's a new image tag all right I see that there's a new a version of the helm chart I can see that and there is a configuration change I see that coming you know someone made a get commit somewhere I'm going to collect all that information and WR back to get um which is uh I guess the demo that I'm going to show shortly is how to do that another is to modify Argo CD application basically look at the Argo CD application manifests and basically change those fields for you um since I do everything via get Ops that won't work for me but uh that is completely possible to those um who aren't storing applications in git um but a pro that's what a promot promotion mechanism is and so um so where does cargo fit in right and uh I kind of threw this together there's probably a better diagram that the um my Engineers are uh probably have somewhere but I threw this together last night um and it it fits kind of um I don't want to say in the middle but in in this picture it kind of does it it kind of sits um at a place where things are collected before telling Argo CD or flux to do its thing um so cargo basically kind of orchestrates all the little changes and updates that you need to make in order for a promotion to happen collect all those and then provide that uh to Argo City so um the idea is someone um builds or pushes an image to a registry cargo will detect that right um there was a a commit to Dev um you know it collects those and it commits that back to a get repo right uh devops engineer looks at those I'm like oh there's a new update let me let me update those and then um you know cargo will see that sorry Argo will see that that that you know because cargo wrote to get Argo would be like oh okay there's a new update let me roll those out for you but then cargo does something like it'll verify those changes um it'll verify those for you um and it it uses um it kind of reuses some of the code for Argo rollouts basically it uses what we call verification templ which basically says um query Prometheus and give me some of those metrics or query this web hook or you know din Trace whatever right open Telemetry um it's it's really flexible in how it figures out the health of your system um and so um you kind of have like ultimate flexibility right or visibility um with with cargo right you kind of see um these little promotion boxes I guess we call it a freight line again you know we're I think maybe we're being a little too cute with the naming but um with the freight line we have um you know each one of these uh updatable boxes right is a collection of things right like oh a get commit and um a a Docker image update right and it's like oh I can promote forwards and backwards uh from there um and you can understand where your application is U linearly so um too long did uh didn't read right tldr so when we think about CD we kind of lump together deployment delivery right cargo focuses on delivery of artifacts right it collects those artifacts and it delivers them to Argo CD which its focus or flux its focus is on the actual deployment of those artifacts so we're kind of seeing a separation we're not only separating uh CI away from this process um you know for it to focus on what it does best we're actually separating the two CDs as well where cargo is now um you know use whatever you want for CI GitHub actions uh Jenkins you know tekon whatever uh cargo will then collect that information for you and then Argo c will actually deliver those to your environment for you so um it is an open source project so um we actually do have some of our customers starting to commit to it as as well and some other members of the community um we have a uh that's the GitHub right uh you can download and use it today I'm using the open source version um today in the demo and um and we have a Discord where all this conversation is happening so city.io commmunity is where the Discord is um and where all the chatting and development is happening um as well so um before I get into the demo I I know I kind of did a fire hose type of information I want to pause maybe for a few minutes and get any questions from anyone that anyone may have or any clarification of of any information uh yes question um can we can we actually sorry I was a little slow on the uptake how is the cargo API compared to say the Argo API where you really have a lot of power to do almost everything yeah so um the cargo API um is in this early stages right so there's not a lot there uh what is there are things like web hooks right so you can like do web hooks so that way um you don't have to wait to the reconciliation time you can just wait until uh you know as soon as something happens you can do more event based things um and other features are being built out um we're still we're in our 0.4 release right now so U it it there's not a lot there right now but it's it's currently being built any other questions one back um so on one of your slides um it says that cargo will help determine what's safe to be deployed uh I mean how does it do that by like code signing or review or there's like some signatures involved for um what uh what is Deployable is that your question like how does it determined yeah like what's safe to be deployed out to your production environment how does it do that yeah so um it looks U there's there's a few ways to do it and I'll actually show uh show it in the demo there's a few ways but um uh what it does it it's it subscribes to let's say a Docker image repository and uh by default it'll just take whatever the newest tag is right you can then filter that out you can say okay you know what give me the newest that is semantically versioned right so like any and and then you can then um do some sort of like either reject or uh semantic versioning uh constraints being like okay anything that starts with 1.2 give me the newest of that um and the same for a get repo right you say Okay I want to track head because that's the default or um give me the semantically versioned uh the latest semantically versioned um give me uh look for this pattern right cuz I'm you know maybe you're not tagging it semantically you know you're not doing sver you're doing something else right some sort of and you can do a Rex as there so it there's ways to filter out um you know you can set those policies up with cargo so cargo will F filter those out for you okay any other questions so hi so for for for someone that's considering moving to giops that hasn't taken the plunge fully that's still kind of doing a CI CD setup it's kind of this is the exact thing that's giving me pause is like the environments and all the other things is so is using a tool like this good you think for like a green field CD like like rolling out cargo and something like this as on a brand new yeah um so this is very much still in uh cargo is still very much in Alpha we're we're we're looking at getting to a 1.0 release by the end of this year um cargo is something a little bit more advanced so in the beginning you're going to see that it's it's um you're like this is kind of overkill for what I'm doing um but eventually you're going to go start running into these problems so early adoption uh especially in a test system um just so you can see how how it looks uh definitely recommend um definitely not production because it's still fairly new and things are you know there's going to be breaking changes in each each release um that's why I hope the demo because you know every release something breaks on my demo so um but it is it is something to consider you're you're right because it's it's something that gives people pause in adopting get Ops because it's like oh like this is a whole new paradigm in my head and um you know and really there's not really nothing out there that kind of solves that problem problem so that that's the the cargo's aim um especially if you're into open source we love feedback again so like if you jump on the Discord say hey I wish I did this or I wish I did that're that's definitely the phase because that's we're in the phase that we are right now cool maybe one two more questions yep right here easy way to do that without manually configuring so you um you want to roll out eight eight or 80 um en two 80 or eight or 80 environments right yeah so um in in the demo I'm I'm going to release into two environments but it's it can be as easily rolled out into multiple so um U so that's the actually the idea of cargo is like you kind of collect them together and deploy them all at once versus manually doing it each with with with Argo CD for example um where each environment can be one application right and so um that's the act the actual idea of cargo so all right any other questions right ready to see me fail okay all right let's yeah all right let's let's do it I like the en enthusiasm all right let's um let's take a look I might have to make this a little bigger but this is is this is the application I don't want that what I do want is uh cargo demo uh deploy okay cool um so here I have no that's not it uh cargo so let's oh I have I usually put this in one file and customize it anyway um okay let's go to state project right so here this is a again cargo so this is a kubernetes tool so everything's going to be a lot of yamel um so this kubernetes days I assume everyone's well vested and yaml um but uh a project basically I say I'm going to create a project where I'm holding all my things um and I have a promotion policy meaning um when I deploy when as soon as you detect new Freight or new materials deploy it into my Dev stage AKA environment um and then there's a few stages between that but uh once uh once I get to um I believe I call it um stage is the next environment if that is successful just automatically deploy it to uat because I'm staging it and I just want to run some tests right after that so that's kind of uh that from a high level I have um a warehouse so this is kind of um what I was explaining here so a warehouse um You is kind of where Freight is stored and where I'm defining hey where do how I'm going to retrieve my materials right so here very very simple just for the purposes of this uh this demo I'm subscribing to a git repo right where all my configurations live and then I'm subscribing to an image repository where all um all my images for my application live right and that's kind of where I want to collect them all together um again I can even add more more images and more um uh Helm charts if I want right this is kind of like Warehouse is kind of where everything lands um and as you can see I commented out line 13 you know I can allow certain tags I can do different constraints I'm going to ignore the latest tags right because I don't never in production never want to deploy latest uh tag um my selection strategy just give me the newest build you know there's there's different strategies there you can look at the a cargo documentation but just very simply my warehouse I'm looking at two things and then from there uh let's look at my Dev stage uhuh there we go so uh Dev stage right so what I'm saying is I'm subscribing so where do I uh line nine and 10 where am I retrieving my um uh these artifacts for this specific stage right I'm subscribing to this Warehouse what I just showed you verification right so I'm using a verification analysis template meaning that um what am I going to use to verify that things in this stage are healthy and I'll show you the analysis template it's basically create an analysis template for those who are familiar with um Argo rollouts that's exactly what you think it is um I'll show you in a minute if you don't know Argo rollouts but I'm sub I'm saying okay once something is rolled out please run this analysis to determine whether or not is healthy and then next again when I was saying about promot promotion mechanisms all right once I have the these things how do I promote them and from a um from a getop standpoint that's basically writing it back to get so what I'm doing is all right I want you to um write all these materials I want you to render them out into this specific Branch um and you know Argo CD is in tracking that Branch so anytime there's something new Argo c will deploy from there um and then uh this last thing is like okay and then once Things are Written to get I want you to um do basically hit a web hook and say or Aro CD sync sync for me um and the one before where's analysis template right so analysis template if you are familiar this is an Argo rollouts type of thing um uh for the purpose of this demo I'm just doing a sleep right but just to kind of illustrate the fact that um uh if you're if you're not familiar this can be uh right here I'm basically launching a container that that just sleeps but you can do things like uh query Prometheus you can do things um like query din Trace open Telemetry run another pod that actually does something right if you want to do something specific uh that that sort of thing but that's what the analysis does and then I want to show you my uh uh actually let me go back to test environment so I have different stages but I'm not going to go through each one of them um but here as as you if you notice here um I'm actually so in the dev environment I'm subscribing to the warehouse but in my test uh stage I'm subscribing to Dev meaning that uh I I want the Providence of like whatever is deployed in this environment I don't want you to make me uh make things available for me to promote unless they are okay in you know where I'm subscribing to in this case another uh stage so uh what you can do this is kind of like the CD pipelining that I was kind of mentioning in the beginning where uh you do um you know Dev subscribes to the warehouse collects those materials deploys them okay now the next step will then look at that and be like if if those are okay I'm subscribed to those that means it's okay to deploy to me um I'm not doing it automatically um but it'll be available to me right so um and then the same kind of um idea you can run analysis templates um promotion mechanism is basically the same except I'm writing to a different branch but as you can see you can run different um analysis template and have different kinds of promotion mechanisms per stage depending on your environment and so that's here and then yeah right Branch so basically um what what I'm doing so it's it's it's a little um it's a little interesting to put this put this together so I'll kind of walk through it a little bit so here I am looking at changes made into main right so someone merges or commits directly into main right um but someone does something to Main and then in this uh let me look at the depth stage in this instance okay anytime someone writes something to main what I'm going to do is I'm going to uh take those and then that's what this line 18 means render I'm going to render those files right so Helm or customize and I'm going to write those to that to this Branch this readon branch and Argo CD looks at that branch and rolls things out it's basically a payload branch in your gate repo and it writes back so um so that's kind of like high level here I have a two prod instances um here and then I um you know uh Us West one US West 2 right or something or Us East one what whatever you want to call it right I have two instances um I have uh so that's that's kind of the demo um deployment configuration right and I also have uh this is just a sample code application that builds an application pushes pushes it up to Docker or I think I'm doing a Quay key you know wherever depends where you're from how you pronounce it um I have Argo CD uh deploying an application um here and then I have cargo actually I think I'm going to make uh I need to make a quick change but um here uh cargo uh here Argo CD the um applications are missing because there's nothing in those branches right and cargo's not doing anything I've um I've promoted you know this there's something running in Dev if I look here this is the the dev environment here so um and actually looking at this looks like this won't let me again this is live yeah it won't let me promote it because it's not healthy yet one second how do I do this okay I'm G to make a massive change where is my uh lordy okay uh what did I do here uh let's go to app base what I'm going to do is let's see if this makes it work uh no uh Ingress the problem isass isn't um isn't running actually uh I could install the Ingress it doesn't really anyways live debugging lordy okay Dev if I had a said my kingdom for a said okay uh that was what was that test and then uat and I think it should be okay after this this is a good uh a good way to test this actually oops so let's do get ad get commit we do it live make sure to sign your commits as always okay that might be a little better okay um and then I need to refresh this okay um so you see the new Freight came down and actually automatically promoted to Dev um and that's because I have uh I have Auto promotion set so anytime there's a new a new Freight coming in it'll Auto promote and then it'll automatically um I need to stop this here let me do this okay all right cool all right so pretend you didn't see any of that all right so so I have this Freight right it's uh deployed into here I might need to uh okay cool um so this is my Dev app uh there's nothing in prod or test or anything like that uh so new Freight came down and that automatically got promoted to Dev um I can then uh promote this to um push this to test and I can say Okay since since this succeeded on a deployment in this environment I'm going to click yes what ends up happening is that collects those artifacts builds a new um uh new manifest uh commits it directly back to uh to get and if I go here uh you can see that it actually uh did that um um cargo did that for me um and then it synced uh this application what I can do here um since it synced uh was successful in test I had an automatic promotion to uh uat which that's going here as you can see here it say progressing um and uh because it passed uh test right and if I go here if I do um okay get pods car cargo demo um you can see the analysis ran right because it was just asleep um and so now I have this someone asked like okay right like I want to promote it to you know 800 environments or whatever so this is how I do it um you know I you know there's like a little bus or whatever I can ship this off to um prod and both all my production environments get updated um with one um and if I go here you'll see that they'll start progressing and sinking right and so that was kind of like pretty pretty straightforward of like how I promote there was a configuration change and how I promote those and I just realized I'm running out of time so let's see if I can do this real real quick um so so so in actuality what actually happens right okay so let's go through the simple use case to see if I can go through it really quick in the next 10 minutes or so um I want to make an application change this application change requires a configuration change um and it requires a configuration change because I'm adding some sort of support for something right and so um so let's do this so then let's um here I I wrote a little cheat sheet but like if I go to here and I um uh I uncomment this right so I'm going to add something in the uh navigation bar of my application here um I'm going to add a navigation bar so that's what I did there boom all right um I need to go to my app.goo.gl [Music] save okay um and then I'll go to the next step while it's thinking about doing something um and then my handlers right I'm actually um you know lightning code right it's who needs AI um right so then I uncommented the code I I pre-wrote for this uh just to show that I am um uh writing an actual new application so um that application requires a configuration change and so um I'm going to go to my configuration and then uh again since this is like you said a cooking show um uh app overlays uh what I'm going to do is remove Dev cp-a uh and that would be hack Dev here uh okay so then what I did is I added um basically a deployment patch saying oh hey by the way there's a new configuration file I need to do um and then I actually need to delete uh these patches for the networking there we go um all right and so then here again uh I need to do a get ad oops get ad get commit uh doing it live get push origin main boom configuration change is done uh let's build the application get add get commit Sam uh live coding get a push origin [Music] Main um cool so that will um in my app that should fire off a GitHub action live coding right uh which it does which Builds an application actually if I go down to Cargo uh there should be some new Freight Let me refresh this there should be some new Freight because I made a configuration change that should come down the freight line pretty soon yeah so you see that um it came down I made a configuration change but my app change um uh yeah I need to my port forward anytime you do a roll out your port forward doesn't port forward okay um so they um the configuration change was there but not the application change um and then this this kind of goes however it goes if you want to watch paint dry um I'll do that but as you see here uh cargo um detected there's new Freight it's like but you can see the image version is the same right because I only made a configuration change so I collected that Freight and put it in Dev for me cuz I have that auto automatic promotion policy and then um and we can wait for this oh no what did I do is qu down again where is podman exited with code 125 okay let's do this then uh uh where is my app we still have some time so let's Salvage this uh Docker login Quay awesome uh okay uh Docker build whoa I don't want that Docker build DT uh let's call this um scale we'll we'll just do it manually instead of I'll debug that GitHub action for what oh no interesting I think my base image no longer exists oh yeah yeah yeah but I wonder why it failed the GitHub action yeah cuz I'm I'm this is a an M1 right um the architecture I would think it would uh what's the way to build it's uh Docker build there's a there's a flag you can run right to try to buildform platform is it yes you're right uh okay build X okay gotcha build X build or is it probably the be fa uh plat come on dash dash platform what is it uh amd64 or X well I'm building it for X Linux is it thank you oh I have to do again build no interesting no manifest okay okay yeah so the Arch Linux doesn't have support 4 a. yeah wow okay let's try to build this again what was the error here let's see here can't talk to V1 container registry what this literally like worked the other day uh taqu yeah I am well I gave it the credentials no presid and local pman uh can we just rebuild it again let's rerun yeah that's exactly what I'm doing I'm like let's just rerun it let's see what happens Funny Story red Hatter right um working at Acuity they told me they were the ones that brought down Quay that one time that we needed to make the MySQL adjustments it was them anyway I'll tell you the story you know of uh I'm like that was you guys like that took us down for a while anyway um let's see yeah it builds fine it's just pushing it to Quay looks like I need this win yesterday my workshop blew up too so right it does build it's pushing Okay so successful all right was just tempor temporary glitch maybe rate limit who knows oh maybe that maybe that was it yeah maybe it had to okay so then it build and pushed an image let's look at cargo that should bring down a freight it takes a little bit um and I think I may have the sver on it as well but we'll see oh come on uh what was my configuration actually uh Warehouse uh newest build ignore tag latest okay that's fine where is Christian h14 cargo okay okay it's there so let's go back to Cargo and see might be an error on my end configuring oh there it is so uh new Freight came down and now it's a newer version come on which means it's rolling it out on dev so third one okay and then uh let's promote that just manually while we're waiting named a Lion Okay Okay cool so then that means that my port forward needs to be reset I think yeah so here now now I have the new greet here um right and so then I can then promote uh these artifacts uh collectively right I have an image I have the configuration that is associated with that image that release right and then I can then promote those from environment to environment by uh by basically cargo orchestrating it delivering those to Argo CD so once it collects what it needs it gives it to Argo CD and Argo CD just does kind of what it does naturally doesn't actually even know that uh cargo is um is involved right in any other way so um looks like we have about fiveish maybe a little bit more is minutes uh for any questions um that you may have right so demo kind of worked uh that was kind of cool um so uh worked a lot better than I expected because I just did it last night this morning so um so yeah again cargo open open source project created by the creators of the Argo project um to basically help orchestrate get Ops promotions so um yeah so for any questions that anyone anyone has here question over here all right let me get to you so this looks to be like a single image if you had a ton of images would they all be different Freight Lines or do they get aggregated how does that work yeah so um it depends how you set up your Warehouse so everything will land in a warehouse um so um you can and that is basically an array right it's a list uh so you can have multiple images um and uh you do you do kind of have to get creative in how you promote them right the promotion mechanism because um you know you may want to write an imaged CU if you have multiple images you're writing them to different um you know customized overlays or you know you know what have you um but uh you'll aggregate them together kind of similarly um with a you know with a G commit an image or image image Helm chart you know any combination of those um cargo will work with that any other questions no oh question right here would you be able to like pin different versions to different environments I guess with the config files right um not currently but it is in the next release so uh yes with the asterisk um we uh we're introducing a feature uh we don't know what we're going to call it yet but basically to freeze um yeah exactly it's like okay hey this environment I want you to freeze it on this version of these um you know from this specific Freight like I want you to even if someone clicks upgrade or anything like no we're going to big red button sort sort of thing um that is not there currently but it will be there cool oh question here like your shirt by the way it's funny thank you uh how easy would it be to lift and shift this to disconnected networks um oh so um there there is so the only thing currently there is a bug that you can't um provide credentials for internal repos um but the idea there is yeah you don't you can subscribe to um currently only GitHub Enterprise like if you have GitHub you know behind your firewall um or an internal registry but the internal registry it needs to be open um we're fixing that bug um but the idea is you if disconnected environment you basically install this on Prim and instead of pointing it to public repos you point it to your local repos all right any oh question if someone was interested in working on the open source aspect of it uh what would be your entry point like just join the Discord and yeah so um the uh two places obviously join the Discord right to uh get get some more if you um want to contribute uh obviously it' be a GitHub uh pulling the issues and um you know we have uh we're tracking like features and issues in GitHub and so um but yeah the entry point would be the uh the community Discord so yeah for sure all right all right I think we're just about of time yeah we're just out of time all right thank you very much thank you so much for an awesome talk yes h ", "play_list": {"title": "KCD Los Angeles 2024", "description": "KCD Los Angeles was co-located to the Southern California Linux Expo (SCALE), a 21-year-old nonprofit software conference held in sunny Pasadena, CA every March.  This event was co-hosted directly next to DevOpsDays LA.\n\nKCD-LA consisted of two days and two tracks of presentations and workshops, which took place from March 14 to March 15, 2024. We also shared part of KCD-LA with the Data On Kubernetes community.\n\nLearn more about our KCD and other at cncf.io/kcds.", "playlists_id": "PLj6h78yzYM2PqisqddRLzGSvSQ3tbxd_l"}}}